


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>functorch._src.vmap &mdash; functorch preview documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->

  

  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/functorch" aria-label="functorch"></a>

      <div class="main-menu">
        <ul>

          <li>
            <a href="tutorials.html">Tutorials</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/functorch/tree/main/examples">Examples</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/functorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href='https://pytorch.org/functorch/versions.html'>main (0.1.0) &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch API Reference</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>functorch._src.vmap</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for functorch._src.vmap</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">,</span> <span class="n">_broadcast_to_and_flatten</span><span class="p">,</span> <span class="n">TreeSpec</span><span class="p">,</span> <span class="n">_register_pytree_node</span>
<span class="kn">from</span> <span class="nn">.pytree_hacks</span> <span class="kn">import</span> <span class="n">tree_map_</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">inspect</span>

<span class="kn">from</span> <span class="nn">functorch._C</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_batch_dim</span><span class="p">,</span>
    <span class="n">_remove_batch_dim</span><span class="p">,</span>
    <span class="n">_vmap_decrement_nesting</span><span class="p">,</span>
    <span class="n">_vmap_increment_nesting</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">in_dims_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]</span>
<span class="n">out_dims_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">register_torch_return_types</span><span class="p">():</span>
    <span class="c1"># Register torch.return_types as pytree node.</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">return_types</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">attr</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">return_types</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">attr</span><span class="p">):</span>
            <span class="n">return_type_class</span> <span class="o">=</span> <span class="n">attr</span>
            <span class="c1"># Note: We capture the current `return_type_class` with default argument `constructor`</span>
            <span class="c1"># in the lambda otherwise we will point to the last value of `return_type_class` for all lambdas</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span><span class="n">return_type_class</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">constructor</span><span class="o">=</span><span class="n">return_type_class</span><span class="p">:</span> <span class="n">constructor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="n">register_torch_return_types</span><span class="p">()</span>


<span class="c1"># Temporary OrderedDict registration as pytree</span>
<span class="k">def</span> <span class="nf">_odict_flatten</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">_odict_unflatten</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">((</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>


<span class="n">_register_pytree_node</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">,</span> <span class="n">_odict_flatten</span><span class="p">,</span> <span class="n">_odict_unflatten</span><span class="p">)</span>


<span class="c1"># Checks that all args-to-be-batched have the same batch dim size</span>

<span class="k">def</span> <span class="nf">_validate_and_get_batch_size</span><span class="p">(</span>
        <span class="n">flat_in_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">arg</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">in_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>
                   <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">batch_sizes</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">([</span><span class="n">size</span> <span class="o">!=</span> <span class="n">batch_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">]):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap: Expected all tensors to have the same size in the mapped &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;dimension, got sizes </span><span class="si">{</span><span class="n">batch_sizes</span><span class="si">}</span><span class="s1"> for the mapped dimension&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_num_outputs</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span>

<span class="c1"># If value is a tuple, check it has length `num_elements`.</span>
<span class="c1"># If value is not a tuple, make a tuple with `value` repeated `num_elements` times</span>


<span class="k">def</span> <span class="nf">_as_tuple</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">num_elements</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">error_message_lambda</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">value</span><span class="p">,)</span> <span class="o">*</span> <span class="n">num_elements</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_elements</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message_lambda</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">value</span>


<span class="k">def</span> <span class="nf">_process_batched_inputs</span><span class="p">(</span>
    <span class="n">in_dims</span><span class="p">:</span> <span class="n">in_dims_t</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">TreeSpec</span><span class="p">]:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;expected `in_dims` to be int or a (potentially nested) tuple &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;matching the structure of inputs, got: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">in_dims</span><span class="p">)</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">)(&lt;inputs&gt;): got no inputs. Maybe you forgot to add &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;inputs, or you are trying to vmap over a function with no inputs. &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;The latter is unsupported.&#39;</span><span class="p">)</span>

    <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">flat_in_dims</span> <span class="o">=</span> <span class="n">_broadcast_to_and_flatten</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">flat_in_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;in_dims is not compatible with the structure of `inputs`. &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;in_dims has structure </span><span class="si">{</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">in_dims</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1"> but inputs &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;has structure </span><span class="si">{</span><span class="n">args_spec</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;Got in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s1"> for an input but in_dim must be either &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;an integer dimension or None.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;Got in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s1"> for an input but the input is of type &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s1">. We cannot vmap over non-Tensor arguments, &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;please use None as the respective in_dim&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">in_dim</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">or</span> <span class="n">in_dim</span> <span class="o">&gt;=</span> <span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;Got in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s1"> for some input, but that input is a Tensor &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;of dimensionality </span><span class="si">{</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s1"> so expected in_dim to satisfy &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;-</span><span class="si">{</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s1"> &lt;= in_dim &lt; </span><span class="si">{</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">in_dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">flat_in_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">in_dim</span> <span class="o">%</span> <span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">_validate_and_get_batch_size</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">),</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span>

<span class="c1"># Creates BatchedTensors for every Tensor in arg that should be batched.</span>
<span class="c1"># Returns the (potentially) batched arguments and the batch_size.</span>


<span class="k">def</span> <span class="nf">_create_batched_inputs</span><span class="p">(</span>
        <span class="n">flat_in_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">vmap_level</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="c1"># See NOTE [Ignored _remove_batch_dim, _add_batch_dim]</span>
    <span class="n">batched_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">arg</span> <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
                      <span class="n">_add_batch_dim</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                      <span class="k">for</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span>

<span class="c1"># Undos the batching (and any batch dimensions) associated with the `vmap_level`.</span>


<span class="k">def</span> <span class="nf">_unwrap_batched</span><span class="p">(</span>
        <span class="n">batched_outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
        <span class="n">out_dims</span><span class="p">:</span> <span class="n">out_dims_t</span><span class="p">,</span>
        <span class="n">vmap_level</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="n">flat_batched_outputs</span><span class="p">,</span> <span class="n">output_spec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">flat_batched_outputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, ...): `</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">` must only return &#39;</span>
                         <span class="sa">f</span><span class="s1">&#39;Tensors, got type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="si">}</span><span class="s1"> as a return.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">incompatible_error</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, ..., out_dims=</span><span class="si">{</span><span class="n">out_dims</span><span class="si">}</span><span class="s1">)(&lt;inputs&gt;): &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;out_dims is not compatible with the structure of `outputs`. &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;out_dims has structure </span><span class="si">{</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">out_dims</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1"> but outputs &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;has structure </span><span class="si">{</span><span class="n">output_spec</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># Some weird edge case requires us to spell out the following</span>
        <span class="c1"># see test_out_dims_edge_case</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">flat_out_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_dims</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_dims</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">flat_out_dims</span> <span class="o">=</span> <span class="n">out_dims</span>
            <span class="n">out_dims</span> <span class="o">=</span> <span class="n">out_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">incompatible_error</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flat_out_dims</span> <span class="o">=</span> <span class="n">_broadcast_to_and_flatten</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">output_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">flat_out_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">incompatible_error</span><span class="p">()</span>

    <span class="n">flat_outputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">_remove_batch_dim</span><span class="p">(</span><span class="n">batched_output</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">batched_output</span><span class="p">,</span> <span class="n">out_dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_batched_outputs</span><span class="p">,</span> <span class="n">flat_out_dims</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">,</span> <span class="n">output_spec</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_int</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, ..., out_dims=</span><span class="si">{</span><span class="n">out_dims</span><span class="si">}</span><span class="s1">): `out_dims` must be &#39;</span>
        <span class="sa">f</span><span class="s1">&#39;an int or a python collection of ints representing where in the outputs the &#39;</span>
        <span class="sa">f</span><span class="s1">&#39;vmapped dimension should appear.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_out_dims_is_int_or_int_pytree</span><span class="p">(</span><span class="n">out_dims</span><span class="p">:</span> <span class="n">out_dims_t</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="n">tree_map_</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_int</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">out_dims</span><span class="o">=</span><span class="n">out_dims</span><span class="p">),</span> <span class="n">out_dims</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="c1"># Not all callables have __name__, in fact, only static functions/methods do.</span>
    <span class="c1"># A callable created via functools.partial or an nn.Module, to name some</span>
    <span class="c1"># examples, don&#39;t have a __name__.</span>
    <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

<span class="c1"># vmap(func)(inputs) wraps all Tensor inputs to be batched in BatchedTensors,</span>
<span class="c1"># sends those into func, and then unwraps the output BatchedTensors. Operations</span>
<span class="c1"># on BatchedTensors perform the batched operations that the user is asking for.</span>
<span class="c1">#</span>
<span class="c1"># vmap&#39;s randomness behavior differs from JAX&#39;s, which would require a PRNG key</span>
<span class="c1"># to be passed everywhere.</span>


<div class="viewcode-block" id="vmap"><a class="viewcode-back" href="../../../generated/functorch.vmap.html#functorch.vmap">[docs]</a><span class="k">def</span> <span class="nf">vmap</span><span class="p">(</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">in_dims</span><span class="p">:</span> <span class="n">in_dims_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">out_dims</span><span class="p">:</span> <span class="n">out_dims_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">randomness</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;error&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    vmap is the vectorizing map; ``vmap(func)`` returns a new function that</span>
<span class="sd">    maps :attr:`func` over some dimension of the inputs. Semantically, vmap</span>
<span class="sd">    pushes the map into PyTorch operations called by :attr:`func`, effectively</span>
<span class="sd">    vectorizing those operations.</span>

<span class="sd">    vmap is useful for handling batch dimensions: one can write a function</span>
<span class="sd">    :attr:`func` that runs on examples and then lift it to a function that can</span>
<span class="sd">    take batches of examples with ``vmap(func)``. vmap can also be used to</span>
<span class="sd">    compute batched gradients when composed with autograd.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): A Python function that takes one or more arguments.</span>
<span class="sd">            Must return one or more Tensors.</span>
<span class="sd">        in_dims (int or nested structure): Specifies which dimension of the</span>
<span class="sd">            inputs should be mapped over. :attr:`in_dims` should have a</span>
<span class="sd">            structure like the inputs. If the :attr:`in_dim` for a particular</span>
<span class="sd">            input is None, then that indicates there is no map dimension.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        out_dims (int or Tuple[int]): Specifies where the mapped dimension</span>
<span class="sd">            should appear in the outputs. If :attr:`out_dims` is a Tuple, then</span>
<span class="sd">            it should have one element per output. Default: 0.</span>
<span class="sd">        randomness (str): Specifies whether the randomness in this</span>
<span class="sd">            vmap should be the same or different across batches. If &#39;different&#39;,</span>
<span class="sd">            the randomness for each batch will be different. If &#39;same&#39;, the</span>
<span class="sd">            randomness will be the same across batches. If &#39;error&#39;, any calls to</span>
<span class="sd">            random functions will error. Default: &#39;error&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a new &quot;batched&quot; function. It takes the same inputs as</span>
<span class="sd">        :attr:`func`, except each input has an extra dimension at the index</span>
<span class="sd">        specified by :attr:`in_dims`. It takes returns the same outputs as</span>
<span class="sd">        :attr:`func`, except each output has an extra dimension at the index</span>
<span class="sd">        specified by :attr:`out_dims`.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        :func:`vmap` works best with functional-style code. Please do not</span>
<span class="sd">        perform any side-effects in :attr:`func`, with the exception of</span>
<span class="sd">        in-place PyTorch operations. Examples of side-effects include mutating</span>
<span class="sd">        Python data structures and assigning values to variables not captured</span>
<span class="sd">        in :attr:`func`.</span>

<span class="sd">    One example of using :func:`vmap` is to compute batched dot products. PyTorch</span>
<span class="sd">    doesn&#39;t provide a batched ``torch.dot`` API; instead of unsuccessfully</span>
<span class="sd">    rummaging through docs, use :func:`vmap` to construct a new function.</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [D], [D] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(torch.dot)  # [N, D], [N, D] -&gt; [N]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(2, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y)</span>

<span class="sd">    :func:`vmap` can be helpful in hiding batch dimensions, leading to a simpler</span>
<span class="sd">    model authoring experience.</span>

<span class="sd">        &gt;&gt;&gt; batch_size, feature_size = 3, 5</span>
<span class="sd">        &gt;&gt;&gt; weights = torch.randn(feature_size, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def model(feature_vec):</span>
<span class="sd">        &gt;&gt;&gt;     # Very simple linear model with activation</span>
<span class="sd">        &gt;&gt;&gt;     return feature_vec.dot(weights).relu()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; examples = torch.randn(batch_size, feature_size)</span>
<span class="sd">        &gt;&gt;&gt; result = functorch.vmap(model)(examples)</span>

<span class="sd">    :func:`vmap` can also help vectorize computations that were previously difficult</span>
<span class="sd">    or impossible to batch. One example is higher-order gradient computation.</span>
<span class="sd">    The PyTorch autograd engine computes vjps (vector-Jacobian products).</span>
<span class="sd">    Computing a full Jacobian matrix for some function f: R^N -&gt; R^N usually</span>
<span class="sd">    requires N calls to ``autograd.grad``, one per Jacobian row. Using :func:`vmap`,</span>
<span class="sd">    we can vectorize the whole computation, computing the Jacobian in a single</span>
<span class="sd">    call to ``autograd.grad``.</span>

<span class="sd">        &gt;&gt;&gt; # Setup</span>
<span class="sd">        &gt;&gt;&gt; N = 5</span>
<span class="sd">        &gt;&gt;&gt; f = lambda x: x ** 2</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(N, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; y = f(x)</span>
<span class="sd">        &gt;&gt;&gt; I_N = torch.eye(N)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Sequential approach</span>
<span class="sd">        &gt;&gt;&gt; jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]</span>
<span class="sd">        &gt;&gt;&gt;                  for v in I_N.unbind()]</span>
<span class="sd">        &gt;&gt;&gt; jacobian = torch.stack(jacobian_rows)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # vectorized gradient computation</span>
<span class="sd">        &gt;&gt;&gt; def get_vjp(v):</span>
<span class="sd">        &gt;&gt;&gt;     return torch.autograd.grad(y, x, v)</span>
<span class="sd">        &gt;&gt;&gt; jacobian = functorch.vmap(get_vjp)(I_N)</span>

<span class="sd">    :func:`vmap` can also be nested, producing an output with multiple batched dimensions</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [D], [D] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(functorch.vmap(torch.dot))  # [N1, N0, D], [N1, N0, D] -&gt; [N1, N0]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y) # tensor of size [2, 3]</span>

<span class="sd">    If the inputs are not batched along the first dimension, :attr:`in_dims` specifies</span>
<span class="sd">    the dimension that each inputs are batched along as</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [N], [N] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(torch.dot, in_dims=1)  # [N, D], [N, D] -&gt; [D]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(2, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y)   # output is [5] instead of [2] if batched along the 0th dimension</span>

<span class="sd">    If there are multiple inputs each of which is batched along different dimensions,</span>
<span class="sd">    :attr:`in_dims` must be a tuple with the batch dimension for each input as</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [D], [D] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(torch.dot, in_dims=(0, None))  # [N, D], [D] -&gt; [N]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y) # second arg doesn&#39;t have a batch dim because in_dim[1] was None</span>

<span class="sd">    If the input is a Python struct, :attr:`in_dims` must be a tuple containing a struct</span>
<span class="sd">    matching the shape of the input:</span>

<span class="sd">        &gt;&gt;&gt; f = lambda dict: torch.dot(dict[&#39;x&#39;], dict[&#39;y&#39;])</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(5)</span>
<span class="sd">        &gt;&gt;&gt; input = {&#39;x&#39;: x, &#39;y&#39;: y}</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(f, in_dims=({&#39;x&#39;: 0, &#39;y&#39;: None},))</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(input)</span>

<span class="sd">    By default, the output is batched along the first dimension. However, it can be batched</span>
<span class="sd">    along any dimension by using :attr:`out_dims`</span>

<span class="sd">        &gt;&gt;&gt; f = lambda x: x ** 2</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(2, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_pow = functorch.vmap(f, out_dims=1)</span>
<span class="sd">        &gt;&gt;&gt; batched_pow(x) # [5, 2]</span>

<span class="sd">    For any function that uses kwargs, the returned function will not batch the kwargs but will</span>
<span class="sd">    accept kwargs</span>

<span class="sd">        &gt;&gt;&gt; x = torch.randn([2, 5])</span>
<span class="sd">        &gt;&gt;&gt; def f(x, scale=4.):</span>
<span class="sd">        &gt;&gt;&gt;   return x * scale</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; batched_pow = functorch.vmap(f)</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(batched_pow(x), x * 4)</span>
<span class="sd">        &gt;&gt;&gt; batched_pow(x, scale=x) # scale is not batched, output has shape [2, 2, 5]</span>

<span class="sd">    .. note::</span>
<span class="sd">        vmap does not provide general autobatching or handle variable-length</span>
<span class="sd">        sequences out of the box.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">randomness</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;error&#39;</span><span class="p">,</span> <span class="s1">&#39;different&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only allowed values for randomness are &#39;error&#39;, &#39;different&#39;, or &#39;same&#39;. Got </span><span class="si">{</span><span class="n">randomness</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_check_out_dims_is_int_or_int_pytree</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span> <span class="o">=</span> <span class="n">_process_batched_inputs</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="n">vmap_level</span> <span class="o">=</span> <span class="n">_vmap_increment_nesting</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">randomness</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">batched_inputs</span> <span class="o">=</span> <span class="n">_create_batched_inputs</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span>
            <span class="n">batched_outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">batched_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_unwrap_batched</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">_vmap_decrement_nesting</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">wrapped</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright functorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script >let toggleHintShow = 'Click to show';</script>
         <script >let toggleHintHide = 'Click to hide';</script>
         <script >let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  <script script type="text/javascript">
    var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
  </script>

  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

  

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>

<link rel="canonical" href="_modules/functorch/_src/vmap.html" />