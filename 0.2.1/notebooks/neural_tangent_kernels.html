


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Neural Tangent Kernels &mdash; functorch 0.2.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="AOT Autograd - How to use and optimize?" href="aot_autograd_optimizations.html" />
    <link rel="prev" title="Per-sample-gradients" href="per_sample_grads.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->

  

  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/functorch" aria-label="functorch"></a>

      <div class="main-menu">
        <ul>

          <li>
            <a href="https://pytorch.org/functorch">Tutorials</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/functorch/tree/main/examples">Examples</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/functorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href='https://pytorch.org/functorch/versions.html'>0.2.1 (works with PyTorch 1.12.1) &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Neural Tangent Kernels</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/neural_tangent_kernels.ipynb.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="tex2jax_ignore mathjax_ignore section" id="neural-tangent-kernels">
<h1>Neural Tangent Kernels<a class="headerlink" href="#neural-tangent-kernels" title="Permalink to this headline">¶</a></h1>
<p>The neural tangent kernel (NTK) is a kernel that describes <a class="reference external" href="https://en.wikipedia.org/wiki/Neural_tangent_kernel">how a neural network evolves during training</a>. There has been a lot of research around it <a class="reference external" href="https://arxiv.org/abs/1806.07572">in recent years</a>. This tutorial, inspired by the implementation of <a class="reference external" href="https://github.com/google/neural-tangents">NTKs in JAX</a> (see <a class="reference external" href="https://arxiv.org/abs/2206.08720">Fast Finite Width Neural Tangent Kernel</a> for details), demonstrates how to easily compute this quantity using functorch.</p>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<p>First, some setup. Let’s define a simple CNN that we wish to compute the NTK of.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">functorch</span> <span class="kn">import</span> <span class="n">make_functional</span><span class="p">,</span> <span class="n">vmap</span><span class="p">,</span> <span class="n">vjp</span><span class="p">,</span> <span class="n">jvp</span><span class="p">,</span> <span class="n">jacrev</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>

<span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">21632</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>And let’s generate some random data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-a-function-version-of-the-model">
<h2>Create a function version of the model<a class="headerlink" href="#create-a-function-version-of-the-model" title="Permalink to this headline">¶</a></h2>
<p>functorch transforms operate on functions. In particular, to compute the NTK, we will need a function that accepts the parameters of the model and a single input (as opposed to a batch of inputs!) and returns a single output.</p>
<p>We’ll use functorch’s <code class="docutils literal notranslate"><span class="pre">make_functional</span></code> to accomplish the first step. If your module has buffers, you’ll want to use <code class="docutils literal notranslate"><span class="pre">make_functional_with_buffers</span></code> instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">fnet</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">make_functional</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Keep in mind that the model was originally written to accept a batch of input data points. In our CNN example, there are no inter-batch operations. That is, each data point in the batch is independent of other data points. With this assumption in mind, we can easily generate a function that evaluates the model on a single data point:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fnet_single</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">fnet</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="compute-the-ntk-method-1-jacobian-contraction">
<h2>Compute the NTK: method 1 (Jacobian contraction)<a class="headerlink" href="#compute-the-ntk-method-1-jacobian-contraction" title="Permalink to this headline">¶</a></h2>
<p>We’re ready to compute the empirical NTK. The empirical NTK for two data points <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> is defined as the matrix product between the Jacobian of the model evaluated at <span class="math notranslate nohighlight">\(x_1\)</span> and the Jacobian of the model evaluated at <span class="math notranslate nohighlight">\(x_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[J_{net}(x_1) J_{net}^T(x_2)\]</div>
<p>In the batched case where <span class="math notranslate nohighlight">\(x_1\)</span> is a batch of data points and <span class="math notranslate nohighlight">\(x_2\)</span> is a batch of data points, then we want the matrix product between the Jacobians of all combinations of data points from <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
<p>The first method consists of doing just that - computing the two Jacobians, and contracting them. Here’s how to compute the NTK in the batched case:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">empirical_ntk_jacobian_contraction</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="c1"># Compute J(x1)</span>
    <span class="n">jac1</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))(</span><span class="n">params</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    <span class="n">jac1</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">jac1</span><span class="p">]</span>
    
    <span class="c1"># Compute J(x2)</span>
    <span class="n">jac2</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))(</span><span class="n">params</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">jac2</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">jac2</span><span class="p">]</span>
    
    <span class="c1"># Compute J(x1) @ J(x2).T</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;Naf,Mbf-&gt;NMab&#39;</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">j2</span><span class="p">)</span> <span class="k">for</span> <span class="n">j1</span><span class="p">,</span> <span class="n">j2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jac1</span><span class="p">,</span> <span class="n">jac2</span><span class="p">)])</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">empirical_ntk_jacobian_contraction</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([20, 5, 10, 10])
</pre></div>
</div>
</div>
</div>
<p>In some cases, you may only want the diagonal or the trace of this quantity, especially if you know beforehand that the network architecture results in an NTK where the non-diagonal elements can be approximated by zero. It’s easy to adjust the above function to do that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">empirical_ntk_jacobian_contraction</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">compute</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">):</span>
    <span class="c1"># Compute J(x1)</span>
    <span class="n">jac1</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))(</span><span class="n">params</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    <span class="n">jac1</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">jac1</span><span class="p">]</span>
    
    <span class="c1"># Compute J(x2)</span>
    <span class="n">jac2</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))(</span><span class="n">params</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">jac2</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">jac2</span><span class="p">]</span>
    
    <span class="c1"># Compute J(x1) @ J(x2).T</span>
    <span class="n">einsum_expr</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">compute</span> <span class="o">==</span> <span class="s1">&#39;full&#39;</span><span class="p">:</span>
        <span class="n">einsum_expr</span> <span class="o">=</span> <span class="s1">&#39;Naf,Mbf-&gt;NMab&#39;</span>
    <span class="k">elif</span> <span class="n">compute</span> <span class="o">==</span> <span class="s1">&#39;trace&#39;</span><span class="p">:</span>
        <span class="n">einsum_expr</span> <span class="o">=</span> <span class="s1">&#39;Naf,Maf-&gt;NM&#39;</span>
    <span class="k">elif</span> <span class="n">compute</span> <span class="o">==</span> <span class="s1">&#39;diagonal&#39;</span><span class="p">:</span>
        <span class="n">einsum_expr</span> <span class="o">=</span> <span class="s1">&#39;Naf,Maf-&gt;NMa&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="kc">False</span>
        
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="n">einsum_expr</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">j2</span><span class="p">)</span> <span class="k">for</span> <span class="n">j1</span><span class="p">,</span> <span class="n">j2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jac1</span><span class="p">,</span> <span class="n">jac2</span><span class="p">)])</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">empirical_ntk_jacobian_contraction</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="s1">&#39;trace&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([20, 5])
</pre></div>
</div>
</div>
</div>
<p>The asymptotic time complexity of this method is <span class="math notranslate nohighlight">\(N O [FP]\)</span> (time to compute the Jacobians) <span class="math notranslate nohighlight">\( + N^2 O^2 P\)</span> (time to contract the Jacobians), where <span class="math notranslate nohighlight">\(N\)</span> is the batch size of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, <span class="math notranslate nohighlight">\(O\)</span> is the model’s output size, <span class="math notranslate nohighlight">\(P\)</span> is the total number of parameters, and <span class="math notranslate nohighlight">\([FP]\)</span> is the cost of a single forward pass through the model. See section section 3.2 in <a class="reference external" href="https://arxiv.org/abs/2206.08720">Fast Finite Width Neural Tangent Kernel</a> for details.</p>
</div>
<div class="section" id="compute-the-ntk-method-2-ntk-vector-products">
<h2>Compute the NTK: method 2 (NTK-vector products)<a class="headerlink" href="#compute-the-ntk-method-2-ntk-vector-products" title="Permalink to this headline">¶</a></h2>
<p>The next method we will discuss is a way to compute the NTK using NTK-vector products.</p>
<p>This method reformulates NTK as a stack of NTK-vector products applied to columns of an identity matrix <span class="math notranslate nohighlight">\(I_O\)</span> of size <span class="math notranslate nohighlight">\(O\times O\)</span> (where <span class="math notranslate nohighlight">\(O\)</span> is the output size of the model):</p>
<div class="math notranslate nohighlight">
\[J_{net}(x_1) J_{net}^T(x_2) = J_{net}(x_1) J_{net}^T(x_2) I_{O} = \left[J_{net}(x_1) \left[J_{net}^T(x_2) e_o\right]\right]_{o=1}^{O},\]</div>
<p>where <span class="math notranslate nohighlight">\(e_o\in \mathbb{R}^O\)</span> are column vectors of the identity matrix <span class="math notranslate nohighlight">\(I_O\)</span>.</p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(\textrm{vjp}_o = J_{net}^T(x_2) e_o\)</span>. We can use a vector-Jacobian product to compute this.</p></li>
<li><p>Now, consider <span class="math notranslate nohighlight">\(J_{net}(x_1) \textrm{vjp}_o\)</span>. This is a Jacobian-vector product!</p></li>
<li><p>Finally, we can run the above computation in parallel over all columns <span class="math notranslate nohighlight">\(e_o\)</span> of <span class="math notranslate nohighlight">\(I_O\)</span> using <code class="docutils literal notranslate"><span class="pre">vmap</span></code>.</p></li>
</ul>
<p>This suggests that we can use a combination of reverse-mode AD (to compute the vector-Jacobian product) and forward-mode AD (to compute the Jacobian-vector product) to compute the NTK.</p>
<p>Let’s code that up:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">empirical_ntk_ntk_vps</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">compute</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">get_ntk</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">func_x1</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">func_x2</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

        <span class="n">output</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">func_x1</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">get_ntk_slice</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
            <span class="c1"># This computes vec @ J(x2).T</span>
            <span class="c1"># `vec` is some unit vector (a single slice of the Identity matrix)</span>
            <span class="n">vjps</span> <span class="o">=</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
            <span class="c1"># This computes J(X1) @ vjps</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">jvps</span> <span class="o">=</span> <span class="n">jvp</span><span class="p">(</span><span class="n">func_x2</span><span class="p">,</span> <span class="p">(</span><span class="n">params</span><span class="p">,),</span> <span class="n">vjps</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">jvps</span>

        <span class="c1"># Here&#39;s our identity matrix</span>
        <span class="n">basis</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">vmap</span><span class="p">(</span><span class="n">get_ntk_slice</span><span class="p">)(</span><span class="n">basis</span><span class="p">)</span>
        
    <span class="c1"># get_ntk(x1, x2) computes the NTK for a single data point x1, x2</span>
    <span class="c1"># Since the x1, x2 inputs to empirical_ntk_ntk_vps are batched,</span>
    <span class="c1"># we actually wish to compute the NTK between every pair of data points</span>
    <span class="c1"># between {x1} and {x2}. That&#39;s what the vmaps here do.</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vmap</span><span class="p">(</span><span class="n">get_ntk</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">compute</span> <span class="o">==</span> <span class="s1">&#39;full&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">result</span>
    <span class="k">if</span> <span class="n">compute</span> <span class="o">==</span> <span class="s1">&#39;trace&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NMKK-&gt;NM&#39;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">compute</span> <span class="o">==</span> <span class="s1">&#39;diagonal&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NMKK-&gt;NMK&#39;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result_from_jacobian_contraction</span> <span class="o">=</span> <span class="n">empirical_ntk_jacobian_contraction</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span>
<span class="n">result_from_ntk_vps</span> <span class="o">=</span> <span class="n">empirical_ntk_ntk_vps</span><span class="p">(</span><span class="n">fnet_single</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">result_from_jacobian_contraction</span><span class="p">,</span> <span class="n">result_from_ntk_vps</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our code for <code class="docutils literal notranslate"><span class="pre">empirical_ntk_ntk_vps</span></code> looks like a direct translation from the math above! This showcases the power of function transforms: good luck trying to write an efficient version of the above using stock PyTorch.</p>
<p>The asymptotic time complexity of this method is <span class="math notranslate nohighlight">\(N^2 O [FP]\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the batch size of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, <span class="math notranslate nohighlight">\(O\)</span> is the model’s output size, and <span class="math notranslate nohighlight">\([FP]\)</span> is the cost of a single forward pass through the model. Hence this method performs more forward passes through the network than method 1, Jacobian contraction (<span class="math notranslate nohighlight">\(N^2 O\)</span> instead of <span class="math notranslate nohighlight">\(N O\)</span>), but avoids the contraction cost altogether (no <span class="math notranslate nohighlight">\(N^2 O^2 P\)</span> term, where <span class="math notranslate nohighlight">\(P\)</span> is the total number of model’s parameters). Therefore, this method is preferable when <span class="math notranslate nohighlight">\(O P\)</span> is large relative to <span class="math notranslate nohighlight">\([FP]\)</span>, such as fully-connected (not convolutional) models with many outputs <span class="math notranslate nohighlight">\(O\)</span>. Memory-wise, both methods should be comparable. See section 3.3 in <a class="reference external" href="https://arxiv.org/abs/2206.08720">Fast Finite Width Neural Tangent Kernel</a> for details.</p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="aot_autograd_optimizations.html" class="btn btn-neutral float-right" title="AOT Autograd - How to use and optimize?" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="per_sample_grads.html" class="btn btn-neutral" title="Per-sample-gradients" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright functorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Neural Tangent Kernels</a><ul>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#create-a-function-version-of-the-model">Create a function version of the model</a></li>
<li><a class="reference internal" href="#compute-the-ntk-method-1-jacobian-contraction">Compute the NTK: method 1 (Jacobian contraction)</a></li>
<li><a class="reference internal" href="#compute-the-ntk-method-2-ntk-vector-products">Compute the NTK: method 2 (NTK-vector products)</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script >let toggleHintShow = 'Click to show';</script>
         <script >let toggleHintHide = 'Click to hide';</script>
         <script >let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
         <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  <script script type="text/javascript">
    var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
  </script>

  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

  

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>

<link rel="canonical" href="notebooks/neural_tangent_kernels.html" />