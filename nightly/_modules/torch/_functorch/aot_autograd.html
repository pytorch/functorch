


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._functorch.aot_autograd &mdash; functorch nightly documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>nightly (2.6.0a0+git406db6a) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torch._functorch.aot_autograd</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._functorch.aot_autograd</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: ignore-errors</span>

<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">NewType</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch._dynamo.logging</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">import</span> <span class="nn">torch.utils.dlpack</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._decomp.decompositions_for_rng</span> <span class="kn">import</span> <span class="n">PhiloxStateTracker</span><span class="p">,</span> <span class="n">rng_decompositions</span>
<span class="kn">from</span> <span class="nn">torch._dispatch.python</span> <span class="kn">import</span> <span class="n">enable_python_dispatcher</span>
<span class="kn">from</span> <span class="nn">torch._dynamo</span> <span class="kn">import</span> <span class="n">compiled_autograd</span>
<span class="kn">from</span> <span class="nn">torch._dynamo.utils</span> <span class="kn">import</span> <span class="n">dynamo_timed</span><span class="p">,</span> <span class="n">preserve_rng_state</span>
<span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">detect_fake_mode</span>
<span class="kn">from</span> <span class="nn">torch._inductor.utils</span> <span class="kn">import</span> <span class="n">BoxedBool</span>
<span class="kn">from</span> <span class="nn">torch._subclasses</span> <span class="kn">import</span> <span class="n">FakeTensor</span><span class="p">,</span> <span class="n">FakeTensorMode</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.proxy_tensor</span> <span class="kn">import</span> <span class="n">make_fx</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">ShapeEnv</span>
<span class="kn">from</span> <span class="nn">torch.utils._python_dispatch</span> <span class="kn">import</span> <span class="n">is_traceable_wrapper_subclass</span>


<span class="n">static_inputs_log</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">getArtifactLogger</span><span class="p">(</span>
    <span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;cudagraph_static_inputs&quot;</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.autograd_cache</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">AOTAutogradCache</span><span class="p">,</span>
    <span class="n">autograd_cache_key</span><span class="p">,</span>
    <span class="n">should_use_local_autograd_cache</span><span class="p">,</span>
    <span class="n">should_use_remote_autograd_cache</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.collect_metadata_analysis</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.functional_utils</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">_check_if_mutation_can_be_in_graph</span><span class="p">,</span>
    <span class="n">are_all_mutations_hidden_from_autograd</span><span class="p">,</span>
    <span class="n">are_all_mutations_under_no_grad_or_inference_mode</span><span class="p">,</span>
    <span class="n">assert_functional_graph</span><span class="p">,</span>
    <span class="n">from_fun</span><span class="p">,</span>
    <span class="n">gen_alias_from_base</span><span class="p">,</span>
    <span class="n">has_data_mutation</span><span class="p">,</span>
    <span class="n">has_metadata_mutation</span><span class="p">,</span>
    <span class="n">is_fun</span><span class="p">,</span>
    <span class="n">sync_functional_tensor</span><span class="p">,</span>
    <span class="n">to_fun</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.input_output_analysis</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">_tensors_definitely_do_not_overlap</span><span class="p">,</span>
    <span class="n">compute_overlapping_inputs</span><span class="p">,</span>
    <span class="n">create_graph_signature</span><span class="p">,</span>
    <span class="n">create_synthetic_base_metadata</span><span class="p">,</span>
    <span class="n">remove_dupe_metadata</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.jit_compile_runtime_wrappers</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">aot_dispatch_autograd</span><span class="p">,</span>
    <span class="n">aot_dispatch_base</span><span class="p">,</span>
    <span class="n">aot_dispatch_export</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.logging_utils</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">callback_set</span><span class="p">,</span>
    <span class="n">describe_input</span><span class="p">,</span>
    <span class="n">format_guard_bug_msg</span><span class="p">,</span>
    <span class="n">get_aot_compilation_context</span><span class="p">,</span>
    <span class="n">get_aot_graph_name</span><span class="p">,</span>
    <span class="n">get_graph_being_compiled</span><span class="p">,</span>
    <span class="n">graph_being_compiled</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">nth_graph</span><span class="p">,</span>
    <span class="n">set_model_name</span><span class="p">,</span>
    <span class="n">setup_stacktrace_preservation_hooks</span><span class="p">,</span>
    <span class="n">track_graph_compiling</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.runtime_wrappers</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">AOTDedupeWrapper</span><span class="p">,</span>
    <span class="n">AOTSyntheticBaseWrapper</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.schemas</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="n">BackwardSignature</span><span class="p">,</span>
    <span class="n">FQN</span><span class="p">,</span>
    <span class="n">GraphInputName</span><span class="p">,</span>
    <span class="n">GraphOutputName</span><span class="p">,</span>
    <span class="n">GraphSignature</span><span class="p">,</span>
    <span class="n">InputAliasInfo</span><span class="p">,</span>
    <span class="n">MutationType</span><span class="p">,</span>
    <span class="n">OutputAliasInfo</span><span class="p">,</span>
    <span class="n">OutputType</span><span class="p">,</span>
    <span class="n">SubclassCreationMeta</span><span class="p">,</span>
    <span class="n">SubclassMeta</span><span class="p">,</span>
    <span class="n">TensorAlias</span><span class="p">,</span>
    <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.subclass_utils</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">create_metadata_for_subclass</span><span class="p">,</span>
    <span class="n">requires_subclass_dispatch</span><span class="p">,</span>
    <span class="n">unwrap_tensor_subclasses</span><span class="p">,</span>
    <span class="n">unwrap_tensor_subclasses_with_indices_to_original</span><span class="p">,</span>
    <span class="n">wrap_tensor_subclasses</span><span class="p">,</span>
    <span class="n">wrap_tensor_subclasses_maybe_joint</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.traced_function_transforms</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">aot_dispatch_subclass</span><span class="p">,</span>
    <span class="n">create_functional_call</span><span class="p">,</span>
    <span class="n">create_functionalized_fn</span><span class="p">,</span>
    <span class="n">create_functionalized_rng_ops_wrapper</span><span class="p">,</span>
    <span class="n">create_joint</span><span class="p">,</span>
    <span class="n">fn_input_mutations_to_outputs</span><span class="p">,</span>
    <span class="n">fn_prepped_for_autograd</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._aot_autograd.utils</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">_get_autocast_states</span><span class="p">,</span>
    <span class="n">_get_symint_hints</span><span class="p">,</span>
    <span class="n">call_func_at_runtime_with_args</span><span class="p">,</span>
    <span class="n">create_tree_flattened_fn</span><span class="p">,</span>
    <span class="n">KNOWN_TYPES</span><span class="p">,</span>
    <span class="n">make_boxed_compiler</span><span class="p">,</span>
    <span class="n">make_boxed_func</span><span class="p">,</span>
    <span class="n">maybe_to_fresh_input</span><span class="p">,</span>
    <span class="n">normalize_as_list</span><span class="p">,</span>
    <span class="n">partial_flatten_asdict</span><span class="p">,</span>
    <span class="n">root_module_when_exporting_non_strict</span><span class="p">,</span>
    <span class="n">strict_zip</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.partitioners</span> <span class="kn">import</span> <span class="n">default_partition</span>


<span class="nb">zip</span> <span class="o">=</span> <span class="n">strict_zip</span>

<span class="c1"># This global counter increments every time we compile a graph with</span>
<span class="c1"># AOTAutograd.  You can use this to correlate runtime error messages</span>
<span class="c1"># with compile time (e.g., if you get an error at runtime saying</span>
<span class="c1"># compiled graph 3 failed, you can set a breakpoint at compile time</span>
<span class="c1"># for this graph number to investigate further at compile time.)</span>
<span class="c1">#</span>
<span class="c1"># NB: this is different from get_aot_compilation_context, which tracks</span>
<span class="c1"># each underlying graph that is compiled.  In contrast, AOT_COUNTER</span>
<span class="c1"># corresponds to top-level invocations of aot_module/aot_function;</span>
<span class="c1"># one counter is allocated per entire compiled block (but this block</span>
<span class="c1"># may involve compiling multiple subgraphs; e.g., for forwards/backwards)</span>
<span class="n">AOT_COUNTER</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd contains a pretty non-trivial amount of logic to handle edge cases around aliasing and mutation</span>
<span class="c1"># that are external to the graph (they show up as side effects in some way when you run the graph).</span>
<span class="c1">#</span>
<span class="c1"># Take a look at `test_aotdispatch.py TestAOTAutograd.test_input_mutation*` tests for some examples functions</span>
<span class="c1"># and what they&#39;re compiled graphs looks like.</span>
<span class="c1"># Below is a very long comment detailing several edge cases, and showing how AOT Autograd handles them.</span>
<span class="c1">#</span>
<span class="c1"># Note [AOT Autograd: input data mutations]</span>
<span class="c1">#</span>
<span class="c1"># If we compile a function that mutates inputs, then those input mutations are real side effects</span>
<span class="c1"># that a user expects to see after running the compiled graph.</span>
<span class="c1"># However, the graph that we want to send to a backend needs to be *entirely* functional.</span>
<span class="c1"># The way we reconcile this difference is that we remove the mutations completely from the graph that we compile</span>
<span class="c1"># but we update the graph to return (updated_inputs, user_outputs).</span>
<span class="c1"># In the epilogue that runs after the compiled graph is executed, we copy the updated inputs back to the originals.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># After AOT Autograd compiles, we end up with a:</span>
<span class="c1"># (a) compiled graph</span>
<span class="c1"># (b) autograd.Function.forward() method, that executes the compiled graph</span>
<span class="c1"># (c) wrapper function, that calls the autograd.Function.forward() and performs the epilogue</span>
<span class="c1">#</span>
<span class="c1"># The output of (a, b, c) are all written below.</span>
<span class="c1">#</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated gets a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_x_updated, grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is that updated inputs (due to data mutations) *do* participate</span>
<span class="c1"># in the compiled backward graph! Since the compiled forward graph gets N extra outputs</span>
<span class="c1"># (due to updated inputs showing up as graph outputs),</span>
<span class="c1"># The compiled backward gets an additional N inputs.</span>
<span class="c1"># That way, during the x.copy_(x_updated) bit in the epilogue, gradients will flow from the updated input</span>
<span class="c1"># back to the original input.</span>


<span class="c1"># Note [AOT Autograd: input metadata mutations]</span>
<span class="c1">#</span>
<span class="c1"># For the same reason as input mutations, we also don&#39;t put input metadata mutations in the graph.</span>
<span class="c1"># Instead, we return the updated version of the input (a view), and mutate the input&#39;s metadata outside of the graph</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.t_()</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.t()</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated does *not* get a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.as_strided_(x_updated)</span>
<span class="c1">#     return out</span>


<span class="c1"># Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd needs special handling for outputs that alias graph inputs or intermediates!</span>
<span class="c1"># Why?</span>
<span class="c1"># (1) autograd.Function.forward() has a limitation, where views that returned in the forward cannot later be mutated.</span>
<span class="c1"># (2) views don&#39;t need to be compiled in the graph anyway - it&#39;s cheap to generate them outside of the compiled graph,</span>
<span class="c1">#     in an epilogue.</span>
<span class="c1"># For outputs that alias inputs, we do the following:</span>
<span class="c1"># (a) *still* return the aliased output as a graph output</span>
<span class="c1"># (b) In the AOT Autograd wrapper/epilogue, we don&#39;t return that aliased output. Instead, we use it to regenerate the output.</span>
<span class="c1">#</span>
<span class="c1"># For outputs that alias *intermediates*, we do the following:</span>
<span class="c1"># (a) Return the output in the compiled forward, **and** return it&#39;s ._base (a graph intermediates) as an output in the forward</span>
<span class="c1"># (b) Use (output, graph_intermediate) to regenerate the alias, and return that to the user (instead of the compiled fw output).</span>
<span class="c1"># You might wonder why we return the aliased output directly in the graph (and making the graph compute it),</span>
<span class="c1"># only to not return it and instead generate a fresh alias off of the intermediate,</span>
<span class="c1"># instead of (say) just storing metadata about the size/stride of the output somewhere to generate the alias. There are two reasons:</span>
<span class="c1"># (1) Getting the actual alias tensor allows us to use view-replay to generate the alias, instead of an as_strided() call</span>
<span class="c1"># (2) Inductor (and other backends) are free to change the memory format of graph outputs, if it results in better performance.</span>
<span class="c1">#     This can result in problems if a user later tries to .view() that output expecting it to have one set of strides,</span>
<span class="c1">#     when it has a different set of strides.</span>
<span class="c1">#     By including the view op directly in the graph, inductor takes that into account when deciding what memory format</span>
<span class="c1">#     the graph intermediate should be.</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is how our traced backward() graph handles aliases.</span>
<span class="c1"># (this applies to outputs aliasing inputs, outputs aliasing intermediates,</span>
<span class="c1">#  *and* updated inputs returned in the compiled forward due to metadata-only mutations).</span>
<span class="c1"># Any outputs that alias (either inputs or intermediates) do NOT participate in the compiled backward graph</span>
<span class="c1"># It would be wasteful to include them in the compiled backward(), because we regenerate them eagerly</span>
<span class="c1"># at the end of the forward.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     return out1, out2</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     # the compiled graph also returns the intermediate</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># # intermediate gets a gradient in the compiled backward.</span>
<span class="c1"># # both output aliases (out1 and out2) do not.</span>
<span class="c1"># def compiled_backward_graph(grad_intermediate):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     out1, out2, intermediate = compiled_forward_graph(x)</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     out1, out2, intermediate = autograd.Function.apply(x)</span>
<span class="c1">#     # regenerate out1 from the input</span>
<span class="c1">#     out1_regenerated = out1._view_func(x)</span>
<span class="c1">#     # regenerate out1 from the intermediate</span>
<span class="c1">#     out2_regenerated = out2._view_func(intermediate)</span>
<span class="c1">#     return out1_regenerated, out2_regenerated</span>


<span class="c1"># Note [AOT Autograd: mutations to inputs that alias other inputs]</span>
<span class="c1">#</span>
<span class="c1"># Another edge case that is (only partially) handled today is when an input is mutated, but itself aliases another input.</span>
<span class="c1"># AOT Autograd needs to **ensure** that functionalization knows that the two inputs are aliased to each other.</span>
<span class="c1"># That way, when the aliased input is accessed later in the graph, functionalization knows to &quot;update&quot; the alias</span>
<span class="c1"># given the mutation that occurred.</span>
<span class="c1">#</span>
<span class="c1"># This is handled by updating the calling convention: we create a &quot;synthetic base&quot; that becomes a new input</span>
<span class="c1"># in the compiled function, and we regenerate the original (aliased) inputs directly off of the base</span>
<span class="c1"># inside of the compiled function.</span>
<span class="c1">#</span>
<span class="c1"># This logic is fully encapsulated in aot_wrapper_synthetic_base()</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x, x_view):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x * x_view</span>
<span class="c1">#     return out</span>
<span class="c1"># f(x, x.view(-1))</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(base)</span>
<span class="c1">#     x = generate_x(base)</span>
<span class="c1">#     x_view = generate_x_view(base)</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     x_view_updated = x_updated.view(-1)</span>
<span class="c1">#     out = x_updated * x_view_updated</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The calling convention change from (aliases) -&gt; (base) happens</span>
<span class="c1"># # *outside* of the autograd.Function.forward().</span>
<span class="c1"># # That means the forward() only has 1 input (base),</span>
<span class="c1"># # and the backward() only has 1 output (grad_base)</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_base = ...</span>
<span class="c1">#     return grad_base</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(base):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(base)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The compiled wrapper is where we create synthetic bases.</span>
<span class="c1"># # The info on which inputs are mutated is also tracked *before* synthetic base creation.</span>
<span class="c1"># def compiled_wrapper(x, x_view):</span>
<span class="c1">#     base = merge_view_inputs(x, x_view)</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(base)</span>
<span class="c1">#     # x and x_view are aliased in eager mode, so this mutation to x will automatically affect x_view.</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>


<span class="c1"># Note [AOT Autograd: Views to avoid tangents aliasing inputs]</span>
<span class="c1">#</span>
<span class="c1"># We view every forward output when creating out tangent tensors to handle the problematic</span>
<span class="c1"># case in which a subclass does extra aliasing between graph outputs/inputs in a way that</span>
<span class="c1"># is not visible above the sublass.</span>
<span class="c1">#</span>
<span class="c1"># Ordinarily, when constructing the joint function that we want to trace in AOTAutograd,</span>
<span class="c1"># we&#39;re guaranteed that the tangent tensors that we pass</span>
<span class="c1"># into the joint are distinct tensors from the primals. This is because when</span>
<span class="c1"># decide which forward outputs to create tangents for, we only create tangents</span>
<span class="c1"># for forward outputs that are not aliases of inputs (See Note</span>
<span class="c1"># [AOT Autograd: outputs aliasing inputs or intermediates!]).</span>
<span class="c1">#</span>
<span class="c1"># However, when wrapper tensor subclasses enter the picture, it is possible</span>
<span class="c1"># to have an output of the forward that is a subclass that is not an</span>
<span class="c1"># input / alias of an input, but one of its inner tensors is an alias!</span>
<span class="c1"># NestedTensor is an example: Performing an out-of-place pointwise op on a</span>
<span class="c1"># NestedTensor constructs a fresh NestedTensor that holds onto the input&#39;s</span>
<span class="c1"># offsets tensor directly.</span>
<span class="c1">#</span>
<span class="c1"># Having tangent tensors that are the same as the (primal) forward inputs,</span>
<span class="c1"># can cause problems during tracing as make_fx() will specialize on our</span>
<span class="c1"># duplicate inputs: If we passed in the same tensor for primals_1 and</span>
<span class="c1"># tangents_1 during tracing, make_fx() will happily sub out all usages of</span>
<span class="c1"># tangents_1 with primals_1 in the graph, which is not what we want.</span>
<span class="c1">#</span>
<span class="c1"># To work around this, we view every forward output when creating out tangent</span>
<span class="c1"># tensors so that tangents can never be the same as forward inputs even if</span>
<span class="c1"># forward inputs alias forward outputs.</span>

<span class="c1"># Note [Side-Effectful Tokens in AOTAutograd]</span>
<span class="c1">#</span>
<span class="c1"># We allow some some side-effectful operators in</span>
<span class="c1"># the post-AOTAutograd (functional) graph, such as prints and torchbind operations.</span>
<span class="c1"># To ensure that these side-effects are compatible to future graph passes that</span>
<span class="c1"># assume that the graph is functional, we will thread &quot;effect tokens&quot; to show</span>
<span class="c1"># data dependence between these side-effectful operators. Practically speaking,</span>
<span class="c1"># effect tokens are just dummy values (torch.tensor([])). The graph would look</span>
<span class="c1"># like the following:</span>
<span class="c1">#</span>
<span class="c1"># def gm(self, token0, reader):</span>
<span class="c1">#    token1, frame = with_token(ordered_effect_op, (reader,), token0)</span>
<span class="c1">#    frame = frame * 2</span>
<span class="c1">#    token2, frame2 = with_token(ordered_effect_op, (reader,), token1)</span>
<span class="c1">#    frame2 = frame2 * 2</span>
<span class="c1">#    return token2, frame, frame2</span>
<span class="c1">#</span>
<span class="c1"># We will pass the token as an input to the graph, thread it through</span>
<span class="c1"># side-effectful operators using the `with_effects` high order operator, and then</span>
<span class="c1"># return the updated token as an output.</span>
<span class="c1"># So the signature of the graph input would look something like</span>
<span class="c1"># (*tokens, *params_buffers, *user_inputs), and the signature of the graph</span>
<span class="c1"># output would look something like (*tokens, *outputs).</span>
<span class="c1">#</span>
<span class="c1"># However, Inductor does not want the concept of tokens in the final generated</span>
<span class="c1"># code&#39;s input and output. Since changing the graph signature inside of inductor</span>
<span class="c1"># is difficult, after generating the forward graph, we will run a pass to</span>
<span class="c1"># remove the tokens from the inputgenerate the following graph for Inductor, where</span>
<span class="c1"># the tokens are created and sunk within the graph, rather than as inputs and</span>
<span class="c1"># outputs:</span>
<span class="c1">#</span>
<span class="c1"># def gm(self, reader):</span>
<span class="c1">#    token0 = torch.ops.prims._make_token()</span>
<span class="c1">#    token1, frame = with_token(ordered_effect_op, (reader,), token0)</span>
<span class="c1">#    frame = frame * 2</span>
<span class="c1">#    token2, frame2 = with_token(ordered_effect_op, (reader,), token1)</span>
<span class="c1">#    frame2 = frame2 * 2</span>
<span class="c1">#    sink_token = torch.ops.prims._sink_tokens([token2])</span>
<span class="c1">#    return frame, frame2</span>

<span class="c1">#</span>
<span class="c1">#</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>


<span class="n">aot_autograd_decompositions</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">FakifiedFlatArgs</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s2">&quot;FakifiedFlatArgs&quot;</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">process_inputs</span><span class="p">(</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="n">fake_mode</span><span class="p">:</span> <span class="n">FakeTensorMode</span><span class="p">,</span>
    <span class="n">shape_env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShapeEnv</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FakifiedFlatArgs</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">fake_mode</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">torch._dynamo.source</span> <span class="kn">import</span> <span class="n">ConstantSource</span>

                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                    <span class="c1"># We always specialize on scalar values in export.</span>
                    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">x</span>
                    <span class="n">source</span> <span class="o">=</span> <span class="n">ConstantSource</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sym_</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">shape_env</span><span class="o">.</span><span class="n">create_symintnode</span><span class="p">(</span>
                        <span class="n">shape_env</span><span class="o">.</span><span class="n">create_symbol</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="p">),</span> <span class="n">hint</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ScriptObject</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_library</span><span class="o">.</span><span class="n">fake_class_registry</span><span class="o">.</span><span class="n">maybe_to_fake_obj</span><span class="p">(</span>
                    <span class="n">fake_mode</span><span class="p">,</span> <span class="n">x</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">x</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span> <span class="ow">is</span> <span class="n">fake_mode</span>
                <span class="k">return</span> <span class="n">x</span>
            <span class="k">if</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="n">attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="n">FakeTensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                        <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span><span class="o">.</span><span class="n">fake_mode</span> <span class="ow">is</span> <span class="n">fake_mode</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attrs</span>
                    <span class="p">)</span>
                    <span class="k">return</span> <span class="n">x</span>

            <span class="c1"># see note [Tensor Fakification and Symbol Caching]</span>
            <span class="n">symbolic_context</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">source</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">trace</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">tracing_context</span> <span class="o">:=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">try_get</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tracing_context</span><span class="o">.</span><span class="n">tensor_to_context</span><span class="p">:</span>
                    <span class="n">symbolic_context</span> <span class="o">=</span> <span class="n">tracing_context</span><span class="o">.</span><span class="n">tensor_to_context</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
                    <span class="n">source</span> <span class="o">=</span> <span class="n">symbolic_context</span><span class="o">.</span><span class="n">tensor_source</span>
                    <span class="c1"># We already fakeified this tensor in Dynamo, don&#39;t</span>
                    <span class="c1"># dump the trace for it again</span>
                    <span class="n">trace</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span>
                <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">static_weight_shapes</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">symbolic_context</span>
            <span class="p">):</span>
                <span class="c1"># TODO: Ensure that this codepath is never exercised from</span>
                <span class="c1"># Dynamo</span>
                <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">static_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">symbolic_context</span><span class="o">=</span><span class="n">symbolic_context</span><span class="p">,</span>
                <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">,</span>
                <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">FakifiedFlatArgs</span><span class="p">([</span><span class="n">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">construct_fake_mode</span><span class="p">(</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FakeTensorMode</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShapeEnv</span><span class="p">]]:</span>
    <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">ShapeEnv</span><span class="p">()</span> <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">dynamic_shapes</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">shape_env</span><span class="o">=</span><span class="n">shape_env</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">create_aot_dispatcher_function</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">fake_flat_args</span><span class="p">:</span> <span class="n">FakifiedFlatArgs</span><span class="p">,</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="n">fake_mode</span><span class="p">:</span> <span class="n">FakeTensorMode</span><span class="p">,</span>
    <span class="n">shape_env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShapeEnv</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">ViewAndMutationMeta</span><span class="p">]:</span>
    <span class="k">with</span> <span class="n">dynamo_timed</span><span class="p">(</span><span class="s2">&quot;create_aot_dispatcher_function&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_create_aot_dispatcher_function</span><span class="p">(</span>
            <span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_aot_dispatcher_function</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">fake_flat_args</span><span class="p">:</span> <span class="n">FakifiedFlatArgs</span><span class="p">,</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="n">fake_mode</span><span class="p">:</span> <span class="n">FakeTensorMode</span><span class="p">,</span>
    <span class="n">shape_env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShapeEnv</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">ViewAndMutationMeta</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graphs of the attr:`flat_fn` to generate a</span>
<span class="sd">    joint graph. The joint graph is an Fx graph with Aten ops. Please refer to</span>
<span class="sd">    the tracing mechanism to understand the graph capturing details.</span>

<span class="sd">    The joint graph is then passed through attr:`partition_fn` to isolate the</span>
<span class="sd">    forward and backward portions, which are then respectively compiled via the</span>
<span class="sd">    provided attr:`fw_compiler` and attr:`bw_compiler`.</span>

<span class="sd">    The resulting compiled forward and backward graphs are then wrapped up in a</span>
<span class="sd">    ``torch.autograd.Function`` object.</span>

<span class="sd">    The calling convention here is that the first aot_config.num_params_buffers</span>
<span class="sd">    inputs in flat_args are parameters and buffers, and the rest are inputs.</span>

<span class="sd">    We use this to assume that parameters/buffer&#39;s shapes don&#39;t change.</span>

<span class="sd">    Note: this function is used both by aot_function and aot_export (controlled by aot_config.is_export)</span>
<span class="sd">        When aot_config.is_export is True, we return an FX graph + metadata</span>
<span class="sd">        When aot_config.is_export is False, we return an ordinary runtime function</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This is the main entry point.</span>
    <span class="c1"># TODO: Chillee argues that dynamo itself should pass in fake tensors to</span>
    <span class="c1"># the list of arguments when compiling; at the moment we do not do this</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">aot_autograd_decompositions</span><span class="p">,</span>
        <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
        <span class="c1"># Update the decompositions with functionalized random decompositions</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="o">**</span><span class="n">rng_decompositions</span><span class="p">,</span>
            <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="c1"># Check flat_args to see if they&#39;re already fake.  If so, use that fake</span>
    <span class="c1"># mode instead.</span>

    <span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">enable_python_dispatcher</span><span class="p">()</span> <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="c1"># See NOTE: [Deferring tensor pack/unpack hooks until runtime]</span>
    <span class="c1"># If any saved tensor hooks are active, we **don&#39;t** want to trace them.</span>
    <span class="c1"># Instead, we&#39;ll let them run at runtime, around the custom autograd.Function</span>
    <span class="c1"># that we generate in torch.compile.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_multithreading_enabled</span><span class="p">(</span>
        <span class="kc">False</span>
    <span class="p">),</span> <span class="n">preserve_rng_state</span><span class="p">(),</span> <span class="p">(</span>
        <span class="n">fake_mode</span>
    <span class="p">),</span> <span class="p">(</span>
        <span class="n">python_dispatcher_mode</span>
    <span class="p">),</span> <span class="n">PhiloxStateTracker</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_disable_saved_tensors_hooks_during_tracing</span><span class="p">():</span>
        <span class="kn">from</span> <span class="nn">torch._library.fake_class_registry</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">FakeScriptObject</span><span class="p">,</span>
            <span class="n">maybe_to_fake_obj</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Tracing may mutate the states the fake script object,</span>
        <span class="c1"># so we need to duplicate the fake script objects so that subsequent tracing</span>
        <span class="c1"># won&#39;t be affected.</span>
        <span class="k">def</span> <span class="nf">_dup_fake_script_obj</span><span class="p">(</span><span class="n">fake_flat_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">maybe_to_fake_obj</span><span class="p">(</span><span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">fake_flat_args</span><span class="p">),</span> <span class="n">arg</span><span class="o">.</span><span class="n">real_obj</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">FakeScriptObject</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">arg</span>
                <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">fake_flat_args</span>
            <span class="p">]</span>

        <span class="n">needs_autograd</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fake_flat_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="c1"># Patch set_rng_state as set_rng_state with fake tensors is</span>
            <span class="c1"># nonsensical. This does not affect the collection of metadata.</span>
            <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.set_rng_state&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="kc">None</span><span class="p">):</span>
                <span class="n">mod</span> <span class="o">=</span> <span class="n">root_module_when_exporting_non_strict</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">mod</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_detect_attribute_assignment</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>
                <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
                    <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
                        <span class="n">flat_fn</span><span class="p">,</span>
                        <span class="n">static_input_indices</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">static_input_indices</span><span class="p">,</span>
                        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
                        <span class="n">is_train</span><span class="o">=</span><span class="n">needs_autograd</span><span class="p">,</span>
                        <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">,</span>
                    <span class="p">)(</span><span class="o">*</span><span class="n">_dup_fake_script_obj</span><span class="p">(</span><span class="n">fake_flat_args</span><span class="p">))</span>

                <span class="n">req_subclass_dispatch</span> <span class="o">=</span> <span class="n">requires_subclass_dispatch</span><span class="p">(</span>
                    <span class="n">fake_flat_args</span><span class="p">,</span> <span class="n">fw_metadata</span>
                <span class="p">)</span>

                <span class="n">output_and_mutation_safe</span> <span class="o">=</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="c1"># view-type operations preserve requires_grad even in no_grad.</span>
                    <span class="c1"># Do not count aliases of inputs with requires_grad as reason to make a training graph,</span>
                    <span class="c1"># as AOTAutograd will perform view-replay to regenerate the view outputs at runtime,</span>
                    <span class="c1"># setting their grad_fn properly.</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span>
                        <span class="n">x</span><span class="o">.</span><span class="n">output_type</span>
                        <span class="ow">in</span> <span class="p">(</span><span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span>
                <span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">mutations_under_no_grad_or_inference_mode</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">mutations_hidden_from_autograd</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">needs_autograd</span> <span class="ow">and</span> <span class="n">output_and_mutation_safe</span><span class="p">:</span>
                    <span class="c1"># We realized that none of the outputs require grad,</span>
                    <span class="c1"># and none of the inputs that require grad are mutated.</span>
                    <span class="c1"># so we actually have an inference graph.</span>
                    <span class="n">needs_autograd</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="c1"># A bit silly: right now in the subclass codepath, our ViewAndMutationMeta</span>
                    <span class="c1"># changes depending on whether we pass in is_train / keep_input_mutations,</span>
                    <span class="c1"># so we&#39;re forced to recompute the metadata.</span>
                    <span class="c1"># TODO: refactor the subclass path of run_functionalized_fw_and_collect_metadata</span>
                    <span class="c1"># so that this is unnecessary.</span>
                    <span class="k">if</span> <span class="n">req_subclass_dispatch</span><span class="p">:</span>
                        <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
                            <span class="n">flat_fn</span><span class="p">,</span>
                            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
                            <span class="n">is_train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">,</span>
                            <span class="n">static_input_indices</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">static_input_indices</span><span class="p">,</span>
                        <span class="p">)(</span><span class="o">*</span><span class="n">fake_flat_args</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
                            <span class="n">input_info</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span>
                            <span class="n">output_info</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">,</span>
                            <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">,</span>
                            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
                            <span class="n">traced_tangents</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">,</span>
                            <span class="n">traced_tangent_memory_formats</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">traced_tangent_memory_formats</span><span class="p">,</span>
                            <span class="n">subclass_inp_meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_inp_meta</span><span class="p">,</span>
                            <span class="n">subclass_fw_graph_out_meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_fw_graph_out_meta</span><span class="p">,</span>
                            <span class="n">subclass_tangent_meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">subclass_tangent_meta</span><span class="p">,</span>
                            <span class="n">is_train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">tokens</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">tokens</span><span class="p">,</span>
                            <span class="n">static_input_indices</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">static_input_indices</span><span class="p">,</span>
                        <span class="p">)</span>

        <span class="k">if</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">req_subclass_dispatch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">torch.compile is currently being used with tensor subclass inputs:</span>
<span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">fake_flat_args</span><span class="p">])</span><span class="si">}</span><span class="s2">. We are attempting to a compile a graph with two graph outputs</span>
<span class="s2">that alias one another, which is currently unsupported in the subclass use case. If you run into this,</span>
<span class="s2">please file a github issue&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
            <span class="c1"># aot_export: ban input metadata mutations for now to keep shared code paths simpler.</span>
            <span class="c1"># Keeping .resize_() in the graph will require some work</span>
            <span class="c1"># Allowing it but keeping the graph functional will require some calling convention changes.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found an input that received a metadata mutation, through e.g. a call to `.resize_()` or `.transpose_()`.</span>
<span class="s2">This is currently banned in the aot_export workflow. If you need this functionality, please file a github issue.</span>

<span class="s2">fw_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="c1"># In export, banning data mutations on inputs that require grad for now.</span>
            <span class="c1"># This should be rare, and is tricky to get right. When we trace the backward,</span>
            <span class="c1"># we currently trace with autograd.grad instead of .backward(), which makes it difficult</span>
            <span class="c1"># to ensure that we run autograd all the way through the input **before** it saw the mutation.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">x</span>
                        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span>
                        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="o">!=</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found a graph input that requires gradients, and received a mutation.</span>
<span class="s2">This is currently banned in the aot_export workflow. If you need this functionality, please file a github issue.</span>

<span class="s2">fw_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">req_subclass_dispatch</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;\</span>
<span class="sd">aot_export is not currently supported with traceable tensor subclass.</span>
<span class="sd">If you need this feature, please comment on &lt;CREATE_ISSUE_LINK&gt;&quot;&quot;&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Need to decide on a strategy for functionalized RNG: toggling via global config seems bad,</span>
            <span class="c1"># and turning it on will require a non-trivial calling convention change for any export runtime.</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;\</span>
<span class="sd">Functionalized RNG is not currently supported in the aot_export workflow. Please file a github issue,</span>
<span class="sd">or otherwise set torch._functorch.config.functionalize_rng_ops = False.&quot;&quot;&quot;</span>
                <span class="p">)</span>

        <span class="k">def</span> <span class="nf">choose_dispatcher</span><span class="p">(</span><span class="n">needs_autograd</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Pick a dispatcher based on the config rules.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">is_export</span><span class="p">:</span>
                <span class="c1"># export uses just the &quot;graph bits&quot;, whereas the other</span>
                <span class="c1"># two dispatchers include some extra work around handling a runtime epilogue</span>
                <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">aot_dispatch_export</span><span class="p">,</span> <span class="n">needs_autograd</span><span class="o">=</span><span class="n">needs_autograd</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">needs_autograd</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">aot_dispatch_autograd</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">aot_dispatch_base</span>

        <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">choose_dispatcher</span><span class="p">(</span><span class="n">needs_autograd</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>

        <span class="n">compiled_fn</span><span class="p">,</span> <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span>
            <span class="n">flat_fn</span><span class="p">,</span>
            <span class="n">_dup_fake_script_obj</span><span class="p">(</span><span class="n">fake_flat_args</span><span class="p">),</span>
            <span class="n">aot_config</span><span class="p">,</span>
            <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">,</span> <span class="n">fw_metadata</span>


<div class="viewcode-block" id="aot_function"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_function.html#functorch.compile.aot_function">[docs]</a><span class="k">def</span> <span class="nf">aot_function</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">keep_inference_input_mutations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Whether or not to trace with dynamic shapes</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">enable_log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`fn` using torch dispatch</span>
<span class="sd">    mechanism, and then compiles the generated forward and backward graphs</span>
<span class="sd">    through :attr:`fw_compiler` and :attr:`bw_compiler`.</span>

<span class="sd">    :func:`aot_function` traces the forward and backward graph ahead of time,</span>
<span class="sd">    and generates a joint forward and backward graph.  :attr:`partition_fn` is</span>
<span class="sd">    then used to separate out forward and backward graphs. The partitioner</span>
<span class="sd">    function can be used to perform optimizations such as recomputation. One can</span>
<span class="sd">    set `decompositions` dictionary to decompose the operators into a sequence</span>
<span class="sd">    of core or simpler operators supported by the backend compilers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Callable): A Python function that takes one ore more arguments. Must</span>
<span class="sd">            return one or more Tensors.</span>
<span class="sd">        fw_compiler (Callable): A Python function that accepts an Fx graph with</span>
<span class="sd">            Aten ops and input args, and returns a Callable that semantically is</span>
<span class="sd">            equivalent to the input Fx graph.</span>
<span class="sd">        bw_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph.  Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">        partition_fn (Callable): A Python function that takes a joint forward</span>
<span class="sd">            and backward graph, and partitions it into separate forward and</span>
<span class="sd">            backward graphs.</span>
<span class="sd">        decompositions (Dict): A dictionary to define the decomposition of</span>
<span class="sd">            larger Aten ops into simpler or core Aten ops.</span>
<span class="sd">        inference_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph. inference_compiler is invoked</span>
<span class="sd">            if no autograd is needed. Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``Callable`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`fn`, but with forward and backward graph compiled via</span>
<span class="sd">        :attr:`fw_compile` and :attr:`bw_compile`.</span>

<span class="sd">    A simple example usage of :func:`aot_function` is as follows. This example</span>
<span class="sd">    will print the forward and backward graphs of the function ``fn``</span>

<span class="sd">        &gt;&gt;&gt; fn = lambda x : x.sin().cos()</span>
<span class="sd">        &gt;&gt;&gt; def print_compile_fn(fx_module, args):</span>
<span class="sd">        &gt;&gt;&gt;     print(fx_module)</span>
<span class="sd">        &gt;&gt;&gt;     return fx_module</span>
<span class="sd">        &gt;&gt;&gt; aot_fn = aot_function(fn, print_compile_fn)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(4, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; aot_fn(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="k">if</span> <span class="n">inference_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inference_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="n">inference_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_export</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">no_tangents</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">enable_log</span><span class="o">=</span><span class="n">enable_log</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">cached_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">returned_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">cached_res</span>
        <span class="c1"># Now flatten the tensor args</span>
        <span class="n">flat_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">arg_tree_leaves</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Compile the function and save it in the cache</span>
        <span class="k">if</span> <span class="n">cached_res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">flat_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">create_tree_flattened_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
            <span class="p">(</span><span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span><span class="p">)</span> <span class="o">=</span> <span class="n">construct_fake_mode</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>
            <span class="n">fake_flat_args</span><span class="p">:</span> <span class="n">FakifiedFlatArgs</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span>
                <span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span>
            <span class="p">)</span>
            <span class="n">compiled_fn</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                <span class="n">flat_fn</span><span class="p">,</span>
                <span class="n">fake_flat_args</span><span class="p">,</span>
                <span class="n">aot_config</span><span class="p">,</span>
                <span class="n">fake_mode</span><span class="p">,</span>
                <span class="n">shape_env</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">cached_res</span> <span class="o">=</span> <span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="n">out_spec</span><span class="p">)</span>

        <span class="n">cached_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">cached_res</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">cached_fn</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">returned_function</span></div>


<div class="viewcode-block" id="aot_module"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_module.html#functorch.compile.aot_module">[docs]</a><span class="k">def</span> <span class="nf">aot_module</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`mod` using torch dispatch</span>
<span class="sd">    tracing mechanism. It is wrapper function, that underneath uses</span>
<span class="sd">    :func:`aot_function` to perform tracing and compilation.</span>

<span class="sd">    :func:`aot_module` lifts the parameters and buffers of ``nn.Module`` as inputs</span>
<span class="sd">    to a new callable which is then compiled through :func:`aot_function`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod (Callable): A ``nn.Module`` module.</span>
<span class="sd">        args : args to be passed to :func:`aot_function`</span>
<span class="sd">        kwargs : kwargs to be passed to :func:`aot_function`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``nn.Module`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`mod`, but with forward and backward graph compiled.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="n">named_params</span><span class="p">,</span> <span class="n">named_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">named_params</span><span class="p">,</span> <span class="o">**</span><span class="n">named_buffers</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_and_buffers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">named_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">num_params_buffers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_params</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">)</span>
    <span class="n">compiled_f</span> <span class="o">=</span> <span class="n">aot_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="k">class</span> <span class="nc">AOTModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">orig_module</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="n">named_params</span><span class="p">,</span>
                <span class="n">named_buffers</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">AOTModule</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">aot_module_simplified</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cudagraphs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BoxedBool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the simplified or low overhead version of aot_module. For frontends</span>
<span class="sd">    like TorchDynamo, the input functions/modules to AOT are static and have</span>
<span class="sd">    unpacked inputs/outputs. This gives us an opportunity to remove the</span>
<span class="sd">        (1) pytree overhead to parse inputs/outputs,</span>
<span class="sd">        (2) AOT Autograd cache,</span>
<span class="sd">        (3) Reading of params/buffers in every forward call</span>

<span class="sd">    :func:`aot_module_simplified` removes these overheads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="n">params_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params_flat</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cudagraphs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cudagraphs</span> <span class="o">=</span> <span class="n">BoxedBool</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">triton</span><span class="o">.</span><span class="n">cudagraphs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="k">if</span> <span class="n">inference_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inference_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>

    <span class="n">seen_sources</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># First, the params</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">tracing_context</span> <span class="o">:=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_guards</span><span class="o">.</span><span class="n">TracingContext</span><span class="o">.</span><span class="n">try_get</span><span class="p">():</span>
        <span class="n">tracing_context</span><span class="o">.</span><span class="n">params_flat</span> <span class="o">=</span> <span class="n">params_flat</span>
        <span class="p">(</span>
            <span class="n">tracing_context</span><span class="o">.</span><span class="n">params_flat_unwrap_subclasses</span><span class="p">,</span>
            <span class="n">tracing_context</span><span class="o">.</span><span class="n">params_unwrapped_to_flat_index</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">unwrap_tensor_subclasses_with_indices_to_original</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Then, the params 1:1 mapped sources, if relevant.</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;_param_name_to_source&quot;</span><span class="p">):</span>
        <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># We now know this came from dynamo, and (1) we care about guards,</span>
        <span class="c1"># so setting up aot_autograd_arg_pos_to_source for downstream dedup guards</span>
        <span class="c1"># can now be done safely. (2) Dynamo logic protects the 1:1 sizing below.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_param_name_to_source</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> not found.&quot;</span>
            <span class="n">source</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">_param_name_to_source</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">source</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_sources</span><span class="p">,</span> <span class="n">source</span>
            <span class="n">seen_sources</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
            <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

    <span class="c1"># Next, the input args</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">static_input_indices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">):</span>
        <span class="c1"># Non dynamo entrypoints can get to here...</span>
        <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">find_nodes</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="s2">&quot;placeholder&quot;</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="s2">&quot;_dynamo_source&quot;</span><span class="p">):</span>
                <span class="c1"># ... but not here!</span>
                <span class="k">if</span> <span class="n">aot_autograd_arg_pos_to_source</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">source</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_dynamo_source</span>
                <span class="k">assert</span> <span class="n">source</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_sources</span><span class="p">,</span> <span class="n">source</span>
                <span class="n">seen_sources</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
                <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
                <span class="n">source_name</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">name</span><span class="p">()</span> <span class="k">if</span> <span class="n">source</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

                <span class="k">if</span> <span class="s2">&quot;tensor_dict&quot;</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;tensor_dict&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                    <span class="s2">&quot;_dynamo_static_input_type&quot;</span><span class="p">,</span> <span class="kc">None</span>
                <span class="p">):</span>
                    <span class="n">static_inputs_log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                        <span class="s2">&quot;Adding static input pos </span><span class="si">%s</span><span class="s2"> for source </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">source_name</span>
                    <span class="p">)</span>
                    <span class="n">static_input_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">static_inputs_log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                        <span class="s2">&quot;Non-static input pos </span><span class="si">%s</span><span class="s2"> for source </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">source_name</span>
                    <span class="p">)</span>

    <span class="k">if</span> <span class="n">aot_autograd_arg_pos_to_source</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">)</span>

    <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">full_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
            <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">break</span>

    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="n">inference_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">params_len</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">,</span>
        <span class="n">static_input_indices</span><span class="o">=</span><span class="n">static_input_indices</span><span class="p">,</span>
        <span class="n">is_export</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">no_tangents</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">cache_info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span> <span class="o">=</span> <span class="n">construct_fake_mode</span><span class="p">(</span><span class="n">full_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>
    <span class="n">fake_flat_args</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span><span class="n">full_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">dispatch_and_compile</span><span class="p">():</span>
        <span class="n">functional_call</span> <span class="o">=</span> <span class="n">create_functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_spec</span><span class="p">,</span> <span class="n">params_len</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">compiled_autograd</span><span class="o">.</span><span class="n">disable</span><span class="p">():</span>
            <span class="n">compiled_fn</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                <span class="n">functional_call</span><span class="p">,</span>
                <span class="n">fake_flat_args</span><span class="p">,</span>
                <span class="n">aot_config</span><span class="p">,</span>
                <span class="n">fake_mode</span><span class="p">,</span>
                <span class="n">shape_env</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">compiled_fn</span>

    <span class="c1"># Autograd cache stuff</span>
    <span class="n">remote</span> <span class="o">=</span> <span class="n">should_use_remote_autograd_cache</span><span class="p">()</span>
    <span class="n">local</span> <span class="o">=</span> <span class="n">should_use_local_autograd_cache</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">local</span> <span class="ow">or</span> <span class="n">remote</span><span class="p">:</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">AOTAutogradCache</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
            <span class="n">dispatch_and_compile</span><span class="p">,</span>
            <span class="n">mod</span><span class="p">,</span>
            <span class="n">fake_flat_args</span><span class="p">,</span>
            <span class="n">aot_config</span><span class="p">,</span>
            <span class="n">cudagraphs</span><span class="p">,</span>
            <span class="n">local</span><span class="p">,</span>
            <span class="n">remote</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">dispatch_and_compile</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">GmWrapper</span><span class="p">):</span>
        <span class="c1"># This function is called by the flatten_graph_inputs wrapper, which boxes</span>
        <span class="c1"># the inputs so that they can be freed before the end of this scope.</span>
        <span class="c1"># For overhead reasons, this is not the default wrapper, see comment:</span>
        <span class="c1"># https://github.com/pytorch/pytorch/pull/122535/files#r1560096481</span>
        <span class="k">def</span> <span class="nf">boxed_forward</span><span class="p">(</span><span class="n">runtime_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]):</span>
            <span class="n">flat_args</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">flat_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
            <span class="n">flat_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">runtime_args</span><span class="p">)</span>
            <span class="n">runtime_args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

        <span class="c1"># Just for convenience</span>
        <span class="n">boxed_forward</span><span class="o">.</span><span class="n">zero_grad</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">zero_grad</span>
        <span class="n">boxed_forward</span><span class="o">.</span><span class="n">named_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span>
        <span class="n">boxed_forward</span><span class="o">.</span><span class="n">named_buffers</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span>
        <span class="k">return</span> <span class="n">boxed_forward</span>

    <span class="c1"># TODO: There is something deeply wrong here; compiled_fn running with</span>
    <span class="c1"># the boxed calling convention, but aot_module_simplified somehow</span>
    <span class="c1"># historically returned a function that was not the boxed calling</span>
    <span class="c1"># convention.  This should get fixed...</span>
    <span class="c1"># NB: GraphModule/nn.Module rely on the non-boxed calling convention here</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">runtime_args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">]):</span>
        <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">runtime_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span>

    <span class="c1"># Just for convenience</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">zero_grad</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">zero_grad</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_buffers</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span>

    <span class="k">return</span> <span class="n">forward</span>


<span class="k">def</span> <span class="nf">aot_export_module</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># If true, we&#39;ll return a joint forward-backward graph,</span>
    <span class="c1"># As well as metadata on the loss + gradients in the backward.</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="c1"># If trace_joint is True, we expect your module to return a scalar loss.</span>
    <span class="c1"># Your module can return multiple outputs, so you must specify which output the loss is.</span>
    <span class="n">output_loss_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="c1"># If None, will be infered from inputs and mod.graph.nodes if mod is a graph module, but the inferred result might be wrong.</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">GraphSignature</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function takes in a module, and returns:</span>
<span class="sd">    (1) an FX graph that can be exported</span>
<span class="sd">    (2) some metadata about the graph</span>

<span class="sd">    If `trace_joint=True` we will return a joint graph of the forward + backward.</span>

<span class="sd">    The traced FX graph will have the following properties compared to the original module:</span>
<span class="sd">    (1) Inputs and outputs to the module will be pytree-flattened</span>
<span class="sd">    (2) Parameters and buffers on the module will be lifted into graph inputs,</span>
<span class="sd">        graph_inputs = (*parameters, *buffers, *user_inputs)</span>
<span class="sd">    (3) The graph will be fully functionalized</span>
<span class="sd">    (4) Any input mutations will be converted into additional outputs in the graph,</span>
<span class="sd">        meaning whoever calls this graph is responsible for applying the mutations</span>
<span class="sd">        back to the original inputs.</span>
<span class="sd">    (5) If is_joint is provided the graph will return parameter gradients in addition to user outputs.</span>
<span class="sd">        The graph output will look like:</span>
<span class="sd">        graph_outputs = (*updated_inputs, *user_outputs, *param_gradients)</span>

<span class="sd">    There are also several restrictions on what modules can use this API. In particular:</span>
<span class="sd">    (1) If trace_joint is specified, we expect the loss function to be **fused**</span>
<span class="sd">        into the module forward. One of the outputs to the forward must be a scalar loss,</span>
<span class="sd">        which is specified with `output_loss_index`.</span>
<span class="sd">        All other outputs to the forward are presumed to not require gradients.</span>
<span class="sd">    (2) This API cannot capture optimizers (although in theory we could build an API for this).</span>
<span class="sd">    (3) Metadata mutations on params/buffers/inputs are banned.</span>
<span class="sd">    (4) Data mutations on anything that requires gradients are banned (parameters)</span>
<span class="sd">    (5) If an input is mutated, it is not allowed to alias any other inputs.</span>
<span class="sd">    (6) Parameters must not be duplicated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pre_dispatch</span> <span class="ow">and</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;pre_dispatch is not supported when trace_joint is True.&quot;</span><span class="p">)</span>
    <span class="n">named_parameters</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="n">params_and_buffers_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params_and_buffers</span><span class="p">)</span>
    <span class="n">params_and_buffers_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="p">{}</span>

    <span class="n">functional_call</span> <span class="o">=</span> <span class="n">create_functional_call</span><span class="p">(</span>
        <span class="n">mod</span><span class="p">,</span> <span class="n">params_spec</span><span class="p">,</span> <span class="n">params_len</span><span class="p">,</span> <span class="n">store_orig_mod</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">num_fw_outs</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="c1"># This helper effectively just adds some extra asserts about what the backward will look like:</span>
        <span class="c1"># Outputs must include a scalar loss, that we compute gradients w.r.t.</span>
        <span class="c1"># We don&#39;t compute gradients w.r.t. anything else: so just in case we detach()</span>
        <span class="c1"># and other output tensors.</span>
        <span class="k">def</span> <span class="nf">fn_to_trace</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">nonlocal</span> <span class="n">num_fw_outs</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">functional_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">output_loss_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;\</span>
<span class="sd">If trace_joint=Trueit is required that one of your forward outputs must be a scalar loss.</span>
<span class="sd">You must specify the which (index) output is the loss with output_loss_index.&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
                <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">out</span><span class="p">,)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected forward output to be either a tensor or a list/tuple of tensors. found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">out</span><span class="p">):</span>
                <span class="c1"># We only want to create a backward graph w.r.t. the loss that the user passed in.</span>
                <span class="c1"># This implies that every other output should not require gradients.</span>
                <span class="c1"># Instead of making this an error (and forcing the user to detach all other outputs</span>
                <span class="c1"># of their forward),</span>
                <span class="c1"># we&#39;ll automatically detach them here.</span>
                <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">output_loss_index</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found an output of the forward that requires gradients, that was not the scalar loss.</span>
<span class="s2">We require all outputs to the forward that are not the scalar loss to not require gradient,</span>
<span class="s2">because we will only compute a backward graph against the scalar loss.</span>
<span class="s2">You can fix this by calling .detach() on each of your forward outputs that is not the loss.</span>
<span class="s2">You specified that output index </span><span class="si">{</span><span class="n">output_loss_index</span><span class="si">}</span><span class="s2"> is the loss, but we found that</span>
<span class="s2">the output at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> requires gradients.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
            <span class="n">out_loss</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="n">output_loss_index</span><span class="p">]</span>
            <span class="n">num_fw_outs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">out_loss</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">The output at index </span><span class="si">{</span><span class="n">output_loss_index</span><span class="si">}</span><span class="s2"> was marked as the loss, but it does not require gradients&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">out_loss</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">We require the output marked as the loss (at index </span><span class="si">{</span><span class="n">output_loss_index</span><span class="si">}</span><span class="s2">) to be a scalar, but it has shape </span><span class="si">{</span><span class="n">out_loss</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>

        <span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Run under no_grad, so our tracing machinery only traces an inference graph.</span>
        <span class="c1"># However if pre_dispatch=True, we want to correctly trace set_grad_enabled calls for training.</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span> <span class="k">if</span> <span class="n">pre_dispatch</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span>
        <span class="n">fn_to_trace</span> <span class="o">=</span> <span class="n">functional_call</span>

    <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># First, the params</span>
    <span class="c1"># NB: It is REQUIRED that parameters come first, Inductor infers &quot;fixed&quot;</span>
    <span class="c1"># parameters by looking at the difference in parameter count outside</span>
    <span class="c1"># and inside AOTAutograd, and assumes the prefix of arguments are fixed</span>
    <span class="c1"># arguments</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_and_buffers_flat</span><span class="p">)</span>
    <span class="c1"># Next, the input args</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">ctx</span><span class="p">():</span>
        <span class="n">fx_g</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">in_spec</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">_aot_export_function</span><span class="p">(</span>
            <span class="n">fn_to_trace</span><span class="p">,</span>
            <span class="n">full_args</span><span class="p">,</span>
            <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
            <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">params_len</span><span class="p">,</span>
            <span class="n">no_tangents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">,</span>
            <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>

        <span class="nd">@wraps</span><span class="p">(</span><span class="n">functional_call</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">flattened_joint</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="c1"># The idea here is that the joint graph that AOTAutograd creates has some strict properties:</span>
            <span class="c1"># (1) It accepts two arguments (primals, tangents), and pytree_flattens them</span>
            <span class="c1"># (2) It returns a tuple of (fw_outs, gradients)</span>
            <span class="c1"># This is a very useful convention for anyone who wants to partition the joint graph</span>
            <span class="c1"># into a separate forward and backward graph.</span>
            <span class="c1"># However,</span>
            <span class="c1"># (1) for people exporting a single joint graph, it would be preferable not to have</span>
            <span class="c1">#     any pytrees in the graph.</span>
            <span class="c1"># (2) We are guaranteed in the aot_export_module case that the forward outputs a loss,</span>
            <span class="c1">#     and there are therefore no tangents that are needed to run the joint graph.</span>
            <span class="c1"># (3) AOTAutograd creates a grad_input for every input in the forward,</span>
            <span class="c1">#     including None&#39;s for inputs that are not grad-requiring tensors.</span>
            <span class="c1">#     we don&#39;t want these in our export graph.</span>
            <span class="c1">#     and there are therefore no tangents that are needed to run the joint graph.</span>
            <span class="c1"># This function &quot;fixes&quot; both of the above by removing any tangent inputs,</span>
            <span class="c1"># and removing pytrees from the original FX graph.</span>
            <span class="n">fake_tangents</span> <span class="o">=</span> <span class="p">[</span>
                <span class="kc">None</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
                    <span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inp_runtime_indices</span>
                <span class="p">)</span>
            <span class="p">]</span>
            <span class="n">fw_outs</span><span class="p">,</span> <span class="n">gradients</span> <span class="o">=</span> <span class="n">fx_g</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">fake_tangents</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="n">output_gradients</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">gradients</span><span class="p">)):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">),</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">Found a parameter that did not receive a gradient.</span>
<span class="s2">&quot;This is most likely a bug, but if this needs to be supported please comment on this Github issue:</span>
<span class="s2">https://github.com/pytorch/pytorch/issues/101192</span>
<span class="s2">&quot;&quot;&quot;</span>
                    <span class="n">output_gradients</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="o">*</span><span class="n">fw_outs</span><span class="p">,</span> <span class="o">*</span><span class="n">output_gradients</span>

        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">flattened_joint</span><span class="p">,</span> <span class="n">record_module_stack</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="o">*</span><span class="n">full_args</span><span class="p">)</span>

    <span class="n">user_args_flat</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">arg_tree_leaves</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span><span class="p">,</span> <span class="n">create_graph_signature</span><span class="p">(</span>
        <span class="n">fx_g</span><span class="p">,</span>
        <span class="n">metadata</span><span class="p">,</span>
        <span class="n">in_spec</span><span class="p">,</span>
        <span class="n">out_spec</span><span class="p">,</span>
        <span class="n">user_args_flat</span><span class="o">=</span><span class="n">user_args_flat</span><span class="p">,</span>
        <span class="n">params_and_buffers_flat</span><span class="o">=</span><span class="n">params_and_buffers_flat</span><span class="p">,</span>
        <span class="n">param_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">named_parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
        <span class="n">buffer_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">named_buffers</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="n">trace_joint</span><span class="p">,</span>
        <span class="n">num_user_fw_outs</span><span class="o">=</span><span class="n">num_fw_outs</span><span class="p">,</span>
        <span class="n">loss_index</span><span class="o">=</span><span class="n">output_loss_index</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">aot_export_joint_simple</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="c1"># It looks like the main consequence of this API is that for dynamic shapes,</span>
    <span class="c1"># it will assume that parms/buffers are static.</span>
    <span class="c1"># With the new inferred dynamic shapes API, maybe this doesn&#39;t matter?</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simplified version of export. Used by higher order operators.</span>

<span class="sd">    This function makes a high-level &quot;no calling convention changes&quot; guarantee:</span>
<span class="sd">    - If no inputs require grad (so we export an inference graph),</span>
<span class="sd">      there are *no* calling convention change between the exported graph, and &quot;func&quot;.</span>
<span class="sd">    - If at least one input requires grad (so we trace out and export a joint fw-bw graph),</span>
<span class="sd">      Then if you were partition the graph into a separate forward and backward graph,</span>
<span class="sd">      The forward graph will have no calling convention changes compared to &quot;func&quot;.</span>

<span class="sd">    The above also relies on some strong restrictions around which functions this API accepts:</span>
<span class="sd">    (1) `args` cannot contain any pytrees (they must have been pytree_flattened already)</span>
<span class="sd">    (2) `func` cannot mutate any inputs</span>
<span class="sd">    (3) The outputs of `func` cannot alias any inputs.</span>

<span class="sd">    Note: this function is only lightly tested today. It will probably be tested more heavily by higher order ops.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">nullcontext</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Run under no_grad, so our tracing machinery only traces an inference graph.</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span>

    <span class="k">with</span> <span class="n">ctx</span><span class="p">():</span>
        <span class="n">fx_g</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">in_spec</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">_aot_export_function</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span>
            <span class="n">args</span><span class="p">,</span>
            <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">in_spec</span><span class="p">,</span> <span class="n">_kw_in_spec</span> <span class="o">=</span> <span class="n">in_spec</span><span class="o">.</span><span class="n">children_specs</span>
    <span class="c1"># At this point, we can just directly return the (joint or inference graph) that we traced.</span>
    <span class="c1"># First though: a bunch of assertions to make sure that our graph doesn&#39;t require</span>
    <span class="c1"># any calling convention changes compared to the original function.</span>
    <span class="c1"># These restrictions are *in addition to* the general restrictions on export.</span>

    <span class="c1"># No input mutations</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">])</span>
        <span class="o">!=</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple does not support input mutations. </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="c1"># No output aliasing</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metadata</span><span class="o">.</span><span class="n">output_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="o">!=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">])</span>
        <span class="o">!=</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple does not support outputs that alias inputs. </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="c1"># No pytrees</span>
    <span class="k">if</span> <span class="n">in_spec</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires inputs to be a single list/tuple. in_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">in_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">()</span> <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">in_spec</span><span class="o">.</span><span class="n">children_specs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires individual inputs not to be pytrees. in_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">in_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires outputs to be a single list/tuple. out_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">out_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">()</span> <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">children_specs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aot_export_joint_simple requires individual outputs not to be pytrees. out_spec=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">out_spec</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="c1"># TODO: we might have to temporarily patch config.functionalize_rng</span>
    <span class="c1"># so that it doesn&#39;t run when we&#39;re exporting a higher order op.</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="c1"># Smoke test that after partitioning, we can run the forward without any calling convention changes.</span>
        <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">default_partition</span><span class="p">(</span>  <span class="c1"># noqa: F821</span>
            <span class="n">fx_g</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">num_fwd_outputs</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">output_infos</span><span class="p">)</span>  <span class="c1"># noqa: F821</span>
        <span class="p">)</span>
        <span class="c1"># Attempt to run the fw_module with the original user inputs</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">fake_mode</span><span class="p">:</span>
            <span class="n">fw_module</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span>


<span class="c1"># Private for now because we aren&#39;t providing a contract on what to return</span>
<span class="c1"># for joint graphs (we could when there&#39;s a clearer use case)</span>
<span class="c1"># In the future, we may need to add more export API&#39;s that provide their own strong guarantees.</span>
<span class="c1"># This is meant as a general helper function for handling various export-y use cases.</span>
<span class="k">def</span> <span class="nf">_aot_export_function</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># If we&#39;re exporting a joint graph and we don&#39;t want any tangent inputs in the graph</span>
    <span class="c1"># (because we are backpropping through a scalar 1 loss),</span>
    <span class="c1"># we need to explicitly specify not to include tangents in the graph.</span>
    <span class="c1"># It&#39;s not enough just to check that our tangent is a scalar, since we also</span>
    <span class="c1"># need to know if it is a 1 (no need to make it a graph input), or something else</span>
    <span class="c1"># (requiring it to be a graph input).</span>
    <span class="c1"># We don&#39;t know this info at trace time though, so we need to make it an explicit config.</span>
    <span class="n">no_tangents</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">pre_dispatch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="c1"># If None, `dynamic_shapes` will be infered from inputs, but the inferred result might be wrong.</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span><span class="p">]:</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="p">{}</span>

    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">create_tree_flattened_fn</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">flat_args</span><span class="p">,</span> <span class="n">in_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">dynamic_shapes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Try to infer `dynamic_shapes from inputs and graph nodes</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="s2">&quot;_orig_mod&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">_orig_mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">func</span><span class="o">.</span><span class="n">_orig_mod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span>
                <span class="k">if</span> <span class="s2">&quot;val&quot;</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span>
            <span class="p">]</span>
            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
        <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="c1"># The export use case doesn&#39;t care about several bits of AOTConfig</span>
    <span class="c1"># (1) compilers (we just export the graph)</span>
    <span class="c1"># (2) partitioners (export is only full graph, user can partition themselves)</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="c1"># For now there&#39;s no use case involving keeping input mutations in the graph</span>
        <span class="c1"># (which we can only do in the inference case anyway).</span>
        <span class="c1"># We can add this later if we need to.</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_export</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">no_tangents</span><span class="o">=</span><span class="n">no_tangents</span><span class="p">,</span>
        <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span> <span class="o">=</span> <span class="n">construct_fake_mode</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>
    <span class="n">fake_flat_args</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">shape_env</span><span class="p">)</span>

    <span class="n">fx_g</span><span class="p">,</span> <span class="n">meta</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">fake_flat_args</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="p">,</span>
        <span class="n">fake_mode</span><span class="p">,</span>
        <span class="n">shape_env</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">fx_g</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">in_spec</span><span class="p">,</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">spec</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">_detect_attribute_assignment</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># Do not allow assignment of tensor attributes during export unless</span>
    <span class="c1"># the attribute is registered as a buffer.</span>

    <span class="n">NN_MODULE_STD_ATTRS</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;_backward_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_backward_pre_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_buffers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_forward_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_forward_hooks_always_called&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_forward_hooks_with_kwargs&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_forward_pre_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_forward_pre_hooks_with_kwargs&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_is_full_backward_hook&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_load_state_dict_post_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_load_state_dict_pre_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_modules&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_non_persistent_buffers_set&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_parameters&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_state_dict_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_state_dict_pre_hooks&quot;</span><span class="p">,</span>
        <span class="s2">&quot;training&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">NN_MODULE_LAZY_STD_ATTRS</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;_initialize_hook&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_load_hook&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">STD_ATTRS</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">*</span><span class="n">NN_MODULE_STD_ATTRS</span><span class="p">,</span>
        <span class="o">*</span><span class="n">NN_MODULE_LAZY_STD_ATTRS</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_get_attributes</span><span class="p">(</span><span class="n">mod</span><span class="p">):</span>
        <span class="c1"># return any attributes of a module that are not standard attributes</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">STD_ATTRS</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">is_leaf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># Ideally is_leaf should not be needed when mapping, but it seems that</span>
        <span class="c1"># subclasses of a standard container X may sometimes map to X, which</span>
        <span class="c1"># destroys information and can cause future mapping to fail.</span>
        <span class="n">known_subclasses_that_lose_info</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
            <span class="c1"># add more here if needed</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">known_subclasses_that_lose_info</span><span class="p">)</span>

    <span class="c1"># save state of attributes before enter</span>
    <span class="n">snapshot</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">_get_attributes</span><span class="p">(</span><span class="n">mod</span><span class="p">),</span> <span class="n">is_leaf</span><span class="o">=</span><span class="n">is_leaf</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="c1"># after exit, compare state of attributes with snapshot</span>
        <span class="c1"># to detect which tensor attributes were assigned</span>
        <span class="n">assigned_tensor_attributes</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span> <span class="nf">_collect_assigned_tensor_attributes</span><span class="p">(</span><span class="n">kp</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">_v</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">v</span><span class="p">:</span>
                <span class="n">attr</span><span class="p">,</span> <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">kp</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">assigned_tensor_attributes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;self.</span><span class="si">{</span><span class="n">attr</span><span class="o">.</span><span class="n">key</span><span class="si">}{</span><span class="n">pytree</span><span class="o">.</span><span class="n">keystr</span><span class="p">(</span><span class="n">rest</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># TODO(avik): Assigning all other types are allowed right now.</span>
                <span class="c1"># Maybe in the future we want to limit this to primitive types?</span>

        <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map_with_path</span><span class="p">(</span>
            <span class="n">_collect_assigned_tensor_attributes</span><span class="p">,</span> <span class="n">snapshot</span><span class="p">,</span> <span class="n">_get_attributes</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># restore state of all attributes (including, e.g., of primitive types)</span>
        <span class="n">mod</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">snapshot</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">assigned_tensor_attributes</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">assigned_tensor_attributes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">noun</span><span class="p">,</span> <span class="n">verb</span> <span class="o">=</span> <span class="s2">&quot;attributes&quot;</span><span class="p">,</span> <span class="s2">&quot;were&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">noun</span><span class="p">,</span> <span class="n">verb</span> <span class="o">=</span> <span class="s2">&quot;attribute&quot;</span><span class="p">,</span> <span class="s2">&quot;was&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The tensor </span><span class="si">{</span><span class="n">noun</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">assigned_tensor_attributes</span><span class="p">)</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">verb</span><span class="si">}</span><span class="s2"> assigned during export. &quot;</span>
                <span class="s2">&quot;Such attributes must be registered as buffers using the `register_buffer` API &quot;</span>
                <span class="s2">&quot;(https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer).&quot;</span>
            <span class="p">)</span>


<span class="n">compiled_function</span> <span class="o">=</span> <span class="n">aot_function</span>
<span class="n">compiled_module</span> <span class="o">=</span> <span class="n">aot_module</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>