


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._functorch.aot_autograd &mdash; functorch nightly documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>nightly (2.1.0a0+gitbb7d988) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torch._functorch.aot_autograd</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._functorch.aot_autograd</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">NewType</span>
<span class="kn">from</span> <span class="nn">unittest.mock</span> <span class="kn">import</span> <span class="n">patch</span>

<span class="kn">from</span> <span class="nn">functorch</span> <span class="kn">import</span> <span class="n">make_fx</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx.traceback</span> <span class="k">as</span> <span class="nn">fx_traceback</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">import</span> <span class="nn">torch.utils.dlpack</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._dispatch.python</span> <span class="kn">import</span> <span class="n">enable_python_dispatcher</span>
<span class="kn">from</span> <span class="nn">torch._dynamo.utils</span> <span class="kn">import</span> <span class="n">dynamo_timed</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span>
<span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">detect_fake_mode</span><span class="p">,</span> <span class="n">tracing</span>
<span class="kn">from</span> <span class="nn">torch._prims_common</span> <span class="kn">import</span> <span class="n">CUDARngStateHelper</span>
<span class="kn">from</span> <span class="nn">torch._logging</span> <span class="kn">import</span> <span class="n">getArtifactLogger</span>
<span class="kn">from</span> <span class="nn">torch._subclasses</span> <span class="kn">import</span> <span class="n">FakeTensor</span><span class="p">,</span> <span class="n">FakeTensorMode</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">immutable_collections</span><span class="p">,</span> <span class="n">Interpreter</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.proxy_tensor</span> <span class="kn">import</span> <span class="n">is_sym_node</span><span class="p">,</span> <span class="n">py_sym_types</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">ShapeEnv</span><span class="p">,</span> <span class="n">is_concrete_int</span><span class="p">,</span> <span class="n">fx_placeholder_vals</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing.reductions</span> <span class="kn">import</span> <span class="n">StorageWeakRef</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">stateless</span>
<span class="kn">from</span> <span class="nn">torch._decomp.decompositions_for_rng</span> <span class="kn">import</span> <span class="n">PhiloxStateTracker</span><span class="p">,</span> <span class="n">rng_decompositions</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">.partitioners</span> <span class="kn">import</span> <span class="n">default_partition</span>
<span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">TracingContext</span><span class="p">,</span> <span class="n">DuplicateInputs</span><span class="p">,</span> <span class="n">Source</span>

<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">aot_joint_log</span> <span class="o">=</span> <span class="n">getArtifactLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;aot_joint_graph&quot;</span><span class="p">)</span>
<span class="n">aot_graphs_log</span> <span class="o">=</span> <span class="n">getArtifactLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;aot_graphs&quot;</span><span class="p">)</span>

<span class="n">MutationType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span>
    <span class="s2">&quot;MutationType&quot;</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;metadata_only&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;data_and_metadata&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">OutputType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span>
    <span class="s2">&quot;OutputType&quot;</span><span class="p">,</span> <span class="p">(</span>
        <span class="c1"># output is not an alias</span>
        <span class="s2">&quot;non_alias&quot;</span><span class="p">,</span>
        <span class="c1"># output aliases an input</span>
        <span class="s2">&quot;alias_of_input&quot;</span><span class="p">,</span>
        <span class="c1"># output **is** an input tensor</span>
        <span class="s2">&quot;is_input&quot;</span><span class="p">,</span>
        <span class="c1"># output has a ._base tensor, which is a graph intermediate.</span>
        <span class="c1"># We need to return its ._base as a graph output,</span>
        <span class="c1"># so its requires_grad info is populated correctly.</span>
        <span class="c1"># Instructs the runtime code to regenerate the current output</span>
        <span class="c1"># from a base tensor, graph_intermediates[base_idx]</span>
        <span class="s2">&quot;alias_of_intermediate_save_as_output&quot;</span><span class="p">,</span>
        <span class="c1"># Same as above; but we don&#39;t need to explicitly add its ._base</span>
        <span class="c1"># as a graph output, because it already **is** a graph output.</span>
        <span class="s2">&quot;alias_of_intermediate&quot;</span><span class="p">,</span>
        <span class="c1"># Same as above; but the output&#39;s ._base is **already** a user output.</span>
        <span class="c1"># Instructs the runtime code to regenerate the current output from</span>
        <span class="c1"># a base tensor, user_outputs[base_idx]</span>
        <span class="s2">&quot;alias_of_intermediate_base_is_user_output&quot;</span><span class="p">,</span>
        <span class="c1"># See Note [Intermediate Bases Optimization]</span>
        <span class="s2">&quot;unsafe_view_alias&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">keys</span><span class="p">())),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">(</span>
        <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">partial_asdict</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">is_dataclass</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">fields</span><span class="p">(</span><span class="n">obj</span><span class="p">)}</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="vm">__class__</span><span class="p">([</span><span class="n">partial_asdict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">partial_asdict</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">obj</span>

<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="c1"># This global counter increments every time we compile a graph with</span>
<span class="c1"># AOTAutograd.  You can use this to correlate runtime error messages</span>
<span class="c1"># with compile time (e.g., if you get an error at runtime saying</span>
<span class="c1"># compiled graph 3 failed, you can set a breakpoint at compile time</span>
<span class="c1"># for this graph number to investigate further at compile time.)</span>
<span class="c1">#</span>
<span class="c1"># NB: this is different from get_aot_compilation_context, which tracks</span>
<span class="c1"># each underlying graph that is compiled.  In contrast, AOT_COUNTER</span>
<span class="c1"># corresponds to top-level invocations of aot_module/aot_function;</span>
<span class="c1"># one counter is allocated per entire compiled block (but this block</span>
<span class="c1"># may involve compiling multiple subgraphs; e.g., for forwards/backwards)</span>
<span class="n">AOT_COUNTER</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="n">KNOWN_TYPES</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
    <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">py_sym_types</span><span class="p">)</span>
<span class="p">)</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">preserve_rng_state</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_python_dispatch</span><span class="o">.</span><span class="n">_disable_current_modes</span><span class="p">():</span>
        <span class="n">rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">cuda_rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_python_dispatch</span><span class="o">.</span><span class="n">_disable_current_modes</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">cuda_rng_state</span><span class="p">)</span>



<span class="c1"># Set up hooks so that during backward the fx&#39;s stack_trace is properly set</span>
<span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">setup_stacktrace_preservation_hooks</span><span class="p">(</span><span class="n">roots</span><span class="p">:</span> <span class="n">List</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">q</span><span class="p">:</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">fn</span><span class="p">,</span> <span class="n">_idx</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">next_functions</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">seen</span> <span class="ow">or</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">node</span>

    <span class="k">def</span> <span class="nf">get_callback</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">callback</span><span class="p">():</span>
            <span class="k">global</span> <span class="n">callback_set</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">)</span>
            <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">callback</span>

    <span class="k">def</span> <span class="nf">get_prehook</span><span class="p">(</span><span class="n">stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">prehook</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
            <span class="k">global</span> <span class="n">callback_set</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">callback_set</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">variable</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span>
                    <span class="n">get_callback</span><span class="p">(</span><span class="n">fx_traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())</span>
                <span class="p">)</span>
                <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prehook</span>

    <span class="k">def</span> <span class="nf">get_posthook</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">posthook</span><span class="p">(</span><span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">posthook</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="n">forward_node_stack</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;traceback_&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_prehook</span><span class="p">(</span><span class="n">get_prehook</span><span class="p">(</span><span class="n">forward_node_stack</span><span class="p">))</span>

        <span class="n">special_stack</span> <span class="o">=</span> <span class="n">forward_node_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">special_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;Gradient addition node due to multiple use of tensor around:&quot;</span>
        <span class="p">)</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">get_posthook</span><span class="p">(</span><span class="n">special_stack</span><span class="p">))</span>


<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd contains a pretty non-trivial amount of logic to handle edge cases around aliasing and mutation</span>
<span class="c1"># that are external to the graph (they show up as side effects in some way when you run the graph).</span>
<span class="c1">#</span>
<span class="c1"># Take a look at `test_aotdispatch.py TestAOTAutograd.test_input_mutation*` tests for some examples functions</span>
<span class="c1"># and what they&#39;re compiled graphs looks like.</span>
<span class="c1"># Below is a very long comment detailing several edge cases, and showing how AOT Autograd handles them.</span>
<span class="c1">#</span>
<span class="c1"># Note [AOT Autograd: input data mutations]</span>
<span class="c1">#</span>
<span class="c1"># If we compile a function that mutates inputs, then those input mutations are real side effects</span>
<span class="c1"># that a user expects to see after running the compiled graph.</span>
<span class="c1"># However, the graph that we want to send to a backend needs to be *entirely* functional.</span>
<span class="c1"># The way we reconcile this difference is that we remove the mutations completely from the graph that we compile</span>
<span class="c1"># but we update the graph to return (updated_inputs, user_outputs).</span>
<span class="c1"># In the epilogue that runs after the compiled graph is executed, we copy the updated inputs back to the originals.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># After AOT Autograd compiles, we end up with a:</span>
<span class="c1"># (a) compiled graph</span>
<span class="c1"># (b) autograd.Function.forward() method, that executes the compiled graph</span>
<span class="c1"># (c) wrapper function, that calls the autograd.Function.forward() and performs the epilogue</span>
<span class="c1">#</span>
<span class="c1"># The output of (a, b, c) are all written below.</span>
<span class="c1">#</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated gets a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_x_updated, grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is that updated inputs (due to data mutations) *do* participate</span>
<span class="c1"># in the compiled backward graph! Since the compiled forward graph gets N extra outputs</span>
<span class="c1"># (due to updated inputs showing up as graph outputs),</span>
<span class="c1"># The compiled backward gets an additional N inputs.</span>
<span class="c1"># That way, during the x.copy_(x_updated) bit in the epilogue, gradients will flow from the updated input</span>
<span class="c1"># back to the original input.</span>


<span class="c1"># Note [AOT Autograd: input metadata mutations]</span>
<span class="c1">#</span>
<span class="c1"># For the same reason as input mutations, we also don&#39;t put input metadata mutations in the graph.</span>
<span class="c1"># Instead, we return the updated version of the input (a view), and mutate the input&#39;s metadata outside of the graph</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     x.t_()</span>
<span class="c1">#     out = x.mul(3)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     x_updated = x.t()</span>
<span class="c1">#     out = x_updated.mul(3)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # x_updated does *not* get a gradient in the compiled backward</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(x)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(x)</span>
<span class="c1">#     x.as_strided_(x_updated)</span>
<span class="c1">#     return out</span>


<span class="c1"># Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd needs special handling for outputs that alias graph inputs or intermediates!</span>
<span class="c1"># Why?</span>
<span class="c1"># (1) autograd.Function.forward() has a limitation, where views that returned in the forward cannot later be mutated.</span>
<span class="c1"># (2) views don&#39;t need to be compiled in the graph anyway - it&#39;s cheap to generate them outside of the compiled graph,</span>
<span class="c1">#     in an epilogue.</span>
<span class="c1"># For outputs that alias inputs, we do the following:</span>
<span class="c1"># (a) *still* return the aliased output as a graph output</span>
<span class="c1"># (b) In the AOT Autograd wrapper/epilogue, we don&#39;t return that aliased output. Instead, we use it to regenerate the output.</span>
<span class="c1">#</span>
<span class="c1"># For outputs that alias *intermediates*, we do the following:</span>
<span class="c1"># (a) Return the output in the compiled forward, **and** return it&#39;s ._base (a graph intermediates) as an output in the forward</span>
<span class="c1"># (b) Use (output, graph_intermediate) to regenerate the alias, and return that to the user (instead of the compiled fw output).</span>
<span class="c1"># You might wonder why we return the aliased output directly in the graph (and making the graph compute it),</span>
<span class="c1"># only to not return it and instead generate a fresh alias off of the intermediate,</span>
<span class="c1"># instead of (say) just storing metadata about the size/stride of the output somewhere to generate the alias. There are two reasons:</span>
<span class="c1"># (1) Getting the actual alias tensor allows us to use view-replay to generate the alias, instead of an as_strided() call</span>
<span class="c1"># (2) Inductor (and other backends) are free to change the memory format of graph outputs, if it results in better performance.</span>
<span class="c1">#     This can result in problems if a user later tries to .view() that output expecting it to have one set of strides,</span>
<span class="c1">#     when it has a different set of strides.</span>
<span class="c1">#     By including the view op directly in the graph, inductor takes that into account when deciding what memory format</span>
<span class="c1">#     the graph intermediate should be.</span>
<span class="c1">#</span>
<span class="c1"># Another important thing to note is how our traced backward() graph handles aliases.</span>
<span class="c1"># (this applies to outputs aliasing inputs, outputs aliasing intermediates,</span>
<span class="c1">#  *and* updated inputs returned in the compiled forward due to metadata-only mutations).</span>
<span class="c1"># Any outputs that alias (either inputs or intermediates) do NOT participate in the compiled backward graph</span>
<span class="c1"># It would be wasteful to include them in the compiled backward(), because we regenerate them eagerly</span>
<span class="c1"># at the end of the forward.</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     return out1, out2</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(x):</span>
<span class="c1">#     out1 = x.t()</span>
<span class="c1">#     intermediate = x.mul(2)</span>
<span class="c1">#     out2 = intermediate.view(-1)</span>
<span class="c1">#     # the compiled graph also returns the intermediate</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># # intermediate gets a gradient in the compiled backward.</span>
<span class="c1"># # both output aliases (out1 and out2) do not.</span>
<span class="c1"># def compiled_backward_graph(grad_intermediate):</span>
<span class="c1">#     grad_x = ...</span>
<span class="c1">#     return grad_x</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(x):</span>
<span class="c1">#     out1, out2, intermediate = compiled_forward_graph(x)</span>
<span class="c1">#     return out1, out2, intermediate</span>
<span class="c1">#</span>
<span class="c1"># def compiled_wrapper(x):</span>
<span class="c1">#     out1, out2, intermediate = autograd.Function.apply(x)</span>
<span class="c1">#     # regenerate out1 from the input</span>
<span class="c1">#     out1_regenerated = out1._view_func(x)</span>
<span class="c1">#     # regenerate out1 from the intermediate</span>
<span class="c1">#     out2_regenerated = out2._view_func(intermediate)</span>
<span class="c1">#     return out1_regenerated, out2_regenerated</span>


<span class="c1"># Note [AOT Autograd: mutations to inputs that alias other inputs]</span>
<span class="c1">#</span>
<span class="c1"># Another edge case that is (only partially) handled today is when an input is mutated, but itself aliases another input.</span>
<span class="c1"># AOT Autograd needs to **ensure** that functionalization knows that the two inputs are aliased to each other.</span>
<span class="c1"># That way, when the aliased input is accessed later in the graph, functionalization knows to &quot;update&quot; the alias</span>
<span class="c1"># given the mutation that occurred.</span>
<span class="c1">#</span>
<span class="c1"># This is handled by updating the calling convention: we create a &quot;synthetic base&quot; that becomes a new input</span>
<span class="c1"># in the compiled function, and we regenerate the original (aliased) inputs directly off of the base</span>
<span class="c1"># inside of the compiled function.</span>
<span class="c1">#</span>
<span class="c1"># This logic is fully encapsulated in aot_wrapper_synthetic_base()</span>
<span class="c1">#</span>
<span class="c1"># Example: original user code:</span>
<span class="c1"># def f(x, x_view):</span>
<span class="c1">#     x.mul_(2)</span>
<span class="c1">#     out = x * x_view</span>
<span class="c1">#     return out</span>
<span class="c1"># f(x, x.view(-1))</span>
<span class="c1">#</span>
<span class="c1"># AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):</span>
<span class="c1"># def compiled_forward_graph(base)</span>
<span class="c1">#     x = generate_x(base)</span>
<span class="c1">#     x_view = generate_x_view(base)</span>
<span class="c1">#     x_updated = x.mul(2)</span>
<span class="c1">#     x_view_updated = x_updated.view(-1)</span>
<span class="c1">#     out = x_updated * x_view_udpated</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The calling convention change from (aliases) -&gt; (base) happens</span>
<span class="c1"># # *outside* of the autograd.Function.forward().</span>
<span class="c1"># # That means the forward() only has 1 input (base),</span>
<span class="c1"># # and the backward() only has 1 output (grad_base)</span>
<span class="c1"># def compiled_backward_graph(grad_out):</span>
<span class="c1">#     grad_base = ...</span>
<span class="c1">#     return grad_base</span>
<span class="c1">#</span>
<span class="c1"># def autograd.Function.forward(base):</span>
<span class="c1">#     x_updated, out = compiled_forward_graph(base)</span>
<span class="c1">#     return x_updated, out</span>
<span class="c1">#</span>
<span class="c1"># # The compiled wrapper is where we create synthetic bases.</span>
<span class="c1"># # The info on which inputs are mutated is also tracked *before* synthetic base creation.</span>
<span class="c1"># def compiled_wrapper(x, x_view):</span>
<span class="c1">#     base = merge_view_inputs(x, x_view)</span>
<span class="c1">#     x_updated, out = autograd.Function.apply(base)</span>
<span class="c1">#     # x and x_view are aliased in eager mode, so this mutation to x will automatically affect x_view.</span>
<span class="c1">#     x.copy_(x_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>


<span class="c1"># This class stores info about every user output.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">OutputAliasInfo</span><span class="p">:</span>
    <span class="c1"># Tells us if this output is:</span>
    <span class="c1"># (1) a regular (non-aliased) output</span>
    <span class="c1"># (2) an alias of a forward input</span>
    <span class="c1"># (3) **is** a forward input (special case of &quot;alias_of_input&quot;)</span>
    <span class="c1"># (4) an alias of an intermediate (aka an alias of an output of the inner traced forward)</span>
    <span class="c1"># (5) an alias of an intermediate, that explicitly requires returning the intermediate</span>
    <span class="c1">#     as a graph output</span>
    <span class="c1"># (6) an alias of an intermediate, where that intermediate is also a user output</span>
    <span class="n">output_type</span><span class="p">:</span> <span class="n">OutputType</span>
    <span class="c1"># The raw type of the output (torch.Tensor, SymInt, etc)</span>
    <span class="n">raw_type</span><span class="p">:</span> <span class="nb">type</span>
    <span class="c1"># If (1) above, then</span>
    <span class="c1"># - base_idx is None</span>
    <span class="c1"># If (2) or (3) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is user_fwd_input[base_idx]</span>
    <span class="c1">#   (This is an index into the inputs *before* we make synthetic bases)</span>
    <span class="c1"># If (4) or (5) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is output_graph_intermediates[base_idx]</span>
    <span class="c1">#   here, this refers to the index of the *direct* traced</span>
    <span class="c1"># If (6) above, then:</span>
    <span class="c1"># - Tells us that the base of this alias is output_user_fwds[base_idx]</span>
    <span class="c1">#   here, this refers to the index of the *direct* traced</span>
    <span class="n">base_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="c1"># If it is a Tensor, what the dynamic dims are (otherwise is None)</span>
    <span class="n">dynamic_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>


<span class="c1"># This class tells us info about user inputs.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">InputAliasInfo</span><span class="p">:</span>
    <span class="n">is_leaf</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">mutates_data</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">mutates_metadata</span><span class="p">:</span> <span class="nb">bool</span>


<span class="c1"># This class encapsulates all aliasing + mutation info we need about the forward graph</span>
<span class="c1"># See a more detailed overview of the edge case handling at</span>
<span class="c1"># https://docs.google.com/document/d/19UoIh_SVrMy_b2Sx5ZaeOJttm6P0Qmyss2rdBuyfoic/edit</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="c1"># length = # user inputs</span>
    <span class="c1"># This gives us info about every input, and what sort of mutation happened to it (if any)</span>
    <span class="n">input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]</span>

    <span class="c1"># length = # user outputs</span>
    <span class="c1"># This gives us info about every output (mostly around whether it aliases other tensors)</span>
    <span class="n">output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span>

    <span class="c1"># length = # mutated inps + # user outputs</span>
    <span class="c1"># For every output *and* mutated input returned from the forward,</span>
    <span class="c1"># tells us whether or not the output should require gradients or not</span>
    <span class="n">requires_grad_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>

    <span class="c1"># length = the number of intermediate bases appended as outputs to the end of the forward graph.</span>
    <span class="c1"># Note: this is not necessarily the same thing as:</span>
    <span class="c1">#   len([x for x in output_info if x.output_type == OutputType.alias_of_intermediate])</span>
    <span class="c1"># Because outputs might share a ._base, or an output&#39;s ._base might itself be</span>
    <span class="c1"># another user output (in both cases, we won&#39;t redundantly append bases to the end of the graph)</span>
    <span class="n">num_intermediate_bases</span><span class="p">:</span> <span class="nb">int</span>

    <span class="c1"># For inference only: instructs us to keep data-only input mutations directly in the graph</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">int</span>

    <span class="c1"># length = (# inputs w data mutations) + (# user outputs that are non_aliasing tensors)</span>
    <span class="c1">#        + (# intermediate bases)</span>
    <span class="c1"># These are the FakeTensor (or potential SymInt) outputs that we traced from our</span>
    <span class="c1"># metadata pass of the user&#39;s forward function.</span>
    <span class="c1"># Their only use today is to pass them as a best-guess for tangents when tracing the joint.</span>
    <span class="c1"># Stashing them as part of our &quot;metadata&quot; makes it simpler if we want to run our analysis</span>
    <span class="c1"># pass once, and re-use the output throughout AOTAutograd</span>
    <span class="n">traced_tangents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="p">]</span>
        <span class="c1"># pre-compute the indices of the inputs that are mutated.</span>
        <span class="c1"># When keep_input_mutations is set, we don&#39;t need to worry about our epilogue</span>
        <span class="c1"># handling data-only mutations, because we keep them directly in the graph.</span>
        <span class="n">mutated_inp_runtime_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_input_mutations</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">aliased_out_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="n">mutated_inp_indices</span>
        <span class="c1"># This is pre-computed in post_init for perf.</span>
        <span class="c1"># It contains the index of every element</span>
        <span class="c1"># of input_info that corresponds to a mutation (data or metadata or both)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span> <span class="o">=</span> <span class="n">mutated_inp_runtime_indices</span>
        <span class="c1"># This is pre-computed for perf.</span>
        <span class="c1"># It contains the index of every element</span>
        <span class="c1"># of output_info that corresponds to an alias (either of an input or intermediate)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aliased_out_indices</span> <span class="o">=</span> <span class="n">aliased_out_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_non_aliased</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">,</span>
                    <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynamic_outputs</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span>
        <span class="c1"># All of the above metadata is collected by tracing the fw function.</span>
        <span class="c1"># However, extra outputs for rng offsets behave differently. Both fwd</span>
        <span class="c1"># and bwd graphs have their own outputs for the total consumed offsets.</span>
        <span class="c1"># Unlike mutated inputs, we don&#39;t have to worry about sending the right</span>
        <span class="c1"># set of tensors between fwd and bwd. Fwd and bwd offsets are</span>
        <span class="c1"># independent and simpler to handle. Therefore, we track them</span>
        <span class="c1"># separately.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ViewAndMutationMeta</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_info</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">input_info</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_info</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">output_info</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad_info</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">requires_grad_info</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">keep_input_mutations</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">keep_input_mutations</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="ow">and</span>
                <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">)</span> <span class="ow">and</span>
                <span class="nb">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">)))</span>


<span class="c1"># This class exists because:</span>
<span class="c1"># - the autograd.Function.forward() in aot autograd returns outputs that might alias inputs</span>
<span class="c1"># - we only care about the metadata on those aliases, so we can regenerate them.</span>
<span class="c1">#   We do not want them to participate in the autograd.Function.</span>
<span class="c1"># We do that by wrapping them in an opaque class, so the autograd.Function</span>
<span class="c1"># does not know to treat them as tensors.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorAlias</span><span class="p">:</span>
    <span class="n">alias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>


<span class="k">def</span> <span class="nf">has_same_metadata</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">t1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">t1</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">==</span> <span class="n">t2</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">target_meta_tensor</span><span class="p">,</span> <span class="n">target_requires_grad</span><span class="p">):</span>
    <span class="c1"># Try to do view-replay if possible.</span>
    <span class="c1"># fall back to .as_strided() if we can&#39;t.</span>
    <span class="k">if</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># The base that we want to replay our view off of might have a different shape than the view&#39;s original base.</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_base</span>
        <span class="n">abt</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
        <span class="c1"># Don&#39;t unnecessarily call as_strided if nothing changed; as_strided&#39;s</span>
        <span class="c1"># backward is poorly implemented and slow</span>
        <span class="k">if</span> <span class="n">abt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">b</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="ow">or</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="ow">or</span>
            <span class="n">abt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">reshaped_base_tensor</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
                <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reshaped_base_tensor</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">_view_func</span><span class="p">(</span><span class="n">reshaped_base_tensor</span><span class="p">)</span>
        <span class="c1"># This shape mismatch can happen due to a bug in inplace/view handling in autograd.</span>
        <span class="c1"># Try putting a breakpoint here and running</span>
        <span class="c1"># `test/functorch/test_aotdispatch TestAOTAutograd.test_output_all_alias_types`</span>
        <span class="c1"># Also, https://github.com/pytorch/pytorch/issues/49825</span>
        <span class="c1">#</span>
        <span class="c1"># As a stopgap, we&#39;ll fall back to as_strided.</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_requires_grad</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">target_requires_grad</span><span class="p">:</span>
                <span class="n">out</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
    <span class="n">storage_offset</span> <span class="o">=</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">)</span>
    <span class="c1"># For outputs aliasing inputs, we need to check if the requires-gradness has changed.</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">target_requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">aliased_out</span>

<span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_to_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mirror_autograd_meta</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">t</span>

<span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">t</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>


<span class="c1"># This is a version of functionalization that is specifically designed</span>
<span class="c1"># for the AOTAutograd use case.</span>
<span class="c1">#</span>
<span class="c1"># Unlike functorch&#39;s variant, this doesn&#39;t use the functorch level system,</span>
<span class="c1"># instead it directly uses PyTorch&#39;s conventional dispatcher to hit the</span>
<span class="c1"># functionalization key.  In particular, this means that FunctionalTensorWrapper</span>
<span class="c1"># can have autograd data stored directly on it.</span>
<span class="c1">#</span>
<span class="c1"># In typical AOTAutograd usage, the dispatch key order will look like:</span>
<span class="c1">#</span>
<span class="c1">#   Autograd - Functionalization ~~~~&gt; Proxy Mode - Fake Tensor</span>
<span class="c1">#       outer tensor                        inner tensor</span>
<span class="c1">#</span>
<span class="c1"># Returns:</span>
<span class="c1"># - ViewAndMutationMeta, telling us metadata about the inputs and outputs, and</span>
<span class="c1">#   The list of outputs from the forward, but **only** the outputs that we need</span>
<span class="c1">#   to pass in as tangents into the backward.</span>
<span class="c1">#   Specifically, aliased outputs from the forward get regenerated, and don&#39;t participate</span>
<span class="c1">#   in the compiled backward function.</span>
<span class="k">def</span> <span class="nf">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
    <span class="n">f</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_to_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mirror_autograd_meta</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
            <span class="k">return</span> <span class="n">r</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="c1"># This function is meant to be run with the forward, which expects a flat list of tensor/symint/other args.</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">KNOWN_TYPES</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">)</span>

        <span class="n">input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">input_requires_grad_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">output_requires_grad_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">flat_f_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_enable_functionalization</span><span class="p">(</span><span class="n">reapply_views</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># precondition: The passed in function already handles unflattening inputs + flattening outputs</span>
            <span class="n">flat_f_outs</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">flat_f_args</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_disable_functionalization</span><span class="p">()</span>

        <span class="c1"># Inspect the state of the input tensor functional wrapper to detect input mutation info</span>
        <span class="c1"># If inp[i] has a metadata-only mutation, then maybe_inputs_with_mutated_metadata[i] contains the updated version</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">f_arg</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_f_args</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">arg</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_arg</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span> <span class="o">==</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">new_arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()):</span>
                    <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">has_same_metadata</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">new_arg</span><span class="p">)</span>
                <span class="c1"># Only track requires_grad info on *mutated* inputs,</span>
                <span class="c1"># because they show up in the autograd.Function.forward as outputs</span>
                <span class="n">input_requires_grad_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">f_arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">f_arg</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mutates_data</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">mutates_metadata</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="n">input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">InputAliasInfo</span><span class="p">(</span>
                <span class="n">is_leaf</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">,</span>
                <span class="n">mutates_data</span><span class="o">=</span><span class="n">mutates_data</span><span class="p">,</span>
                <span class="n">mutates_metadata</span><span class="o">=</span><span class="n">mutates_metadata</span>
            <span class="p">))</span>

        <span class="c1"># If a function involves creating a tensor, and returning a view of it, such that its _base is the intermediiate,</span>
        <span class="c1"># We need to make sure our graph returns the _base as a graph output, and we manually recreate the view</span>
        <span class="c1"># to return to the user. Why? The backend compiler is free to (incorrectly) not set requires_grad</span>
        <span class="c1"># on the base tensor, but we are obligated to properly set requires-gradness on the real output.</span>

        <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">inp_storage_refs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()):</span> <span class="n">idx</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="c1"># We need inp tensor id&#39;s to be able to tell if an outputs **are** inputs.</span>
        <span class="n">inp_tensor_ids</span> <span class="o">=</span> <span class="p">{</span>
            <span class="nb">id</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span> <span class="k">for</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="n">flat_f_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="c1"># We need output tensor id&#39;s to tell if any output._base` attributes **are** other outputs.</span>
        <span class="c1"># (This is also a dict because we need to know that output&#39;s index, so we can regenerate</span>
        <span class="c1"># the alias from it).</span>
        <span class="n">out_tensor_ids</span> <span class="o">=</span> <span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="p">):</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_f_outs</span><span class="p">)}</span>

        <span class="c1"># Keep track of which outputs alias other outputs</span>
        <span class="n">out_tensor_alias_counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">flat_f_outs</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># maps the id of an intermediate base to its index in the output of the compiled forward</span>
        <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">intermediate_bases</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">flat_f_outs</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span> <span class="ow">in</span> <span class="n">inp_storage_refs</span>
            <span class="p">):</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="n">inp_storage_refs</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())]</span>
                <span class="n">is_input_tensor</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="ow">in</span> <span class="n">inp_tensor_ids</span>
                <span class="k">if</span> <span class="n">is_input_tensor</span><span class="p">:</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>

            <span class="c1"># We only need to handle the intermediate base case when both</span>
            <span class="c1"># the intermediate base and the output require gradients.</span>
            <span class="c1"># See Note [AOT Autograd: outputs aliasing inputs or intermediates!]</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">out_tensor_alias_counts</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Note [Intermediate Bases Optimization]</span>
                    <span class="c1"># Normally if we have an output that aliases an intermediate,</span>
                    <span class="c1"># we need to add the extra &quot;intermediate base&quot; logic further down</span>
                    <span class="c1"># to prevent autograd from yelling at us if the user later tries to</span>
                    <span class="c1"># mutate that output.</span>
                    <span class="c1"># However, the common case here is if we have an output that aliases an intermediate,</span>
                    <span class="c1"># but doesn&#39;t alias any other outputs.</span>
                    <span class="c1"># In that case, autograd shouldn&#39;t have to worry about the aliasing at all</span>
                    <span class="c1"># (if that output is mutated, there are no other live aliases for autograd to worry about).</span>
                    <span class="c1"># The &quot;intermediate bases&quot; can hurt inductor perf by forcing more variables to become outputs.</span>
                    <span class="c1"># So as an optimization, we won&#39;t do intermediate base handling in this case.</span>
                    <span class="c1"># Instead, we&#39;ll hide the aliasing from autograd using aten._unsafe_view().</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span>
                    <span class="n">base_idx</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># First, check if o&#39;s ._base is an existing output</span>
                    <span class="n">maybe_existing_out_idx</span> <span class="o">=</span> <span class="n">out_tensor_ids</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_existing_out_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># Special case where the output is an alias of a graph intermediate, but that intermediate</span>
                        <span class="c1"># is itself also a user output.</span>
                        <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span>
                        <span class="n">base_idx</span> <span class="o">=</span> <span class="n">maybe_existing_out_idx</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Next, check if o&#39;s ._base is an intermediate base that we already returned</span>
                        <span class="n">maybe_existing_base_output_idx</span> <span class="o">=</span> <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                            <span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">),</span> <span class="kc">None</span>
                        <span class="p">)</span>
                        <span class="k">if</span> <span class="n">maybe_existing_base_output_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span>
                            <span class="n">base_idx</span> <span class="o">=</span> <span class="n">maybe_existing_base_output_idx</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Otherwise, take o._base and explicitly return it as an output in the compiled graph</span>
                            <span class="n">new_out_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">)</span>
                            <span class="n">base_idx</span> <span class="o">=</span> <span class="n">new_out_idx</span>
                            <span class="c1"># Indicate to the logic later on (when we trace the joint)</span>
                            <span class="c1"># that this particular output should get it&#39;s ._base appended to the forward graph outputs</span>
                            <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span>
                            <span class="n">intermediate_base_tensor_id_to_output_idx</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_out_idx</span>
                            <span class="n">intermediate_bases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">dynamic_dims</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_concrete_int</span><span class="p">(</span><span class="n">s</span><span class="p">)}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dynamic_dims</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">out_info</span> <span class="o">=</span> <span class="n">OutputAliasInfo</span><span class="p">(</span>
                <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">,</span>
                <span class="n">raw_type</span><span class="o">=</span><span class="nb">type</span><span class="p">(</span><span class="n">o</span><span class="p">),</span>
                <span class="n">base_idx</span><span class="o">=</span><span class="n">base_idx</span><span class="p">,</span>
                <span class="n">dynamic_dims</span><span class="o">=</span><span class="n">dynamic_dims</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">output_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_info</span><span class="p">)</span>
            <span class="n">output_requires_grad_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">)</span>

        <span class="c1"># Our autograd.Function.forward returns both mutated inputs and outputs,</span>
        <span class="c1"># so we need grad info on all of them.</span>
        <span class="n">requires_grad_info</span> <span class="o">=</span> <span class="n">input_requires_grad_info</span> <span class="o">+</span> <span class="n">output_requires_grad_info</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">requires_grad_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_info</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># This analysis function returns *only* the outputs that are meant to be tangents to the backwards.</span>
        <span class="c1"># Anything that aliases (inputs returned in the fw due to metadata mutations, or outputs that alias inputs/intermediates)</span>
        <span class="c1"># are *regenerated* later, and not used directly in the autograd graph</span>
        <span class="n">f_input_tangents</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">inp</span>
            <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">,</span> <span class="n">input_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="p">]</span>
        <span class="n">f_output_tangents</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">o</span>
            <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_f_outs</span><span class="p">,</span> <span class="n">output_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="c1"># intermediate bases are also included in the backward graph</span>
        <span class="n">f_tangents</span> <span class="o">=</span> <span class="n">f_input_tangents</span> <span class="o">+</span> <span class="n">f_output_tangents</span> <span class="o">+</span> <span class="n">intermediate_bases</span>
        <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_tangents</span><span class="p">)</span>

        <span class="n">metadata</span> <span class="o">=</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
            <span class="n">input_info</span><span class="o">=</span><span class="n">input_info</span><span class="p">,</span>
            <span class="n">requires_grad_info</span><span class="o">=</span><span class="n">requires_grad_info</span><span class="p">,</span>
            <span class="n">output_info</span><span class="o">=</span><span class="n">output_info</span><span class="p">,</span>
            <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">),</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">keep_input_mutations</span><span class="p">,</span>
            <span class="n">traced_tangents</span><span class="o">=</span><span class="n">traced_tangents</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">metadata</span>

    <span class="k">return</span> <span class="n">inner</span>

<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">AOTConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for AOTDispatcher</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">aot_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">keep_inference_input_mutations</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">aot_autograd_arg_pos_to_source</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Source</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">enable_log</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># This function takes in a tensor t, and returns one of t, t.view(), or t.clone().</span>
<span class="c1"># When tracing the joint forward + backward, for any inputs in the graph that are mutated,</span>
<span class="c1"># we need to clone them first (and similarly for metadata-only mutations, we need to view them first).</span>
<span class="c1"># The idea is that when we trace the backward, we need to pass in the *original* primals</span>
<span class="c1"># to autograd.grad(), before they were mutated.</span>
<span class="c1"># Note: when we have synthetic base inputs, we need to clone them *before* creating views off of them.</span>
<span class="c1"># This means that &quot;idx&quot; here represents the index of the (potentially) synthetic base.</span>
<span class="c1"># What we need to do is:</span>
<span class="c1"># (1) map the current (post-synthetic-base calling convention) input argument index</span>
<span class="c1">#     to int index pre-synthetic-base-calling-convention.</span>
<span class="c1"># (2) There could be multiple, if this index corresponds to a synthetic base</span>
<span class="c1">#     that has multiple input aliases.</span>
<span class="c1"># (3) If any of those corresponding inputs get metadata mutations, then we clone the base.</span>
<span class="k">def</span> <span class="nf">maybe_to_fresh_input</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">meta</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">t</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">:</span>
        <span class="c1"># We only need to bother cloning mutated inputs that participate in autograd.</span>
        <span class="n">mutated_inp_idx</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">mutated_inp_idx</span><span class="p">]</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
            <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
            <span class="c1"># sees the tensor before the mutation</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">mutated_inp_idx</span><span class="p">]</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
            <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
            <span class="c1"># sees the tensor before the metadata mutation</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">t</span>

<span class="c1"># This function returns a new function that returns mutated inputs as outputs.</span>
<span class="c1"># if keep_data_input_mutations is set, then we assume that data-only mutations</span>
<span class="c1"># will be left in the graph, and we only return metadata-mutated inputs as outputs.</span>
<span class="k">def</span> <span class="nf">fn_input_mutations_to_outputs</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">keep_data_input_mutations</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="c1"># The compiled fw will return mutated input tensors, *including* metadata-only mutation.</span>
        <span class="c1"># However, if keep_data_input_mutations is set, the compiled fw only needs to return metadata-mutated inputs.</span>
        <span class="c1"># (because data-only input mutations are handled directly in the compiled graph)</span>
        <span class="n">mutated_inputs_to_return</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">keep_data_input_mutations</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="o">*</span><span class="n">mutated_inputs_to_return</span><span class="p">,</span> <span class="o">*</span><span class="n">outs</span>
    <span class="k">return</span> <span class="n">inner_fn</span>

<span class="c1"># This function takes in a fn with external aliasing and mutation,</span>
<span class="c1"># and returns a new fn with no external aliasing and mutation,</span>
<span class="c1"># as needed for autograd.</span>
<span class="c1"># The main transformations are:</span>
<span class="c1"># - Return mutated inputs as extra outputs</span>
<span class="c1"># - Clone mutated inputs that require gradients,</span>
<span class="c1">#   because autograd will require us to pass the pre-mutated inputs into autograd.grad</span>
<span class="c1"># - Return intermediate bases of outputs as additional outputs,</span>
<span class="c1">#   needed to appease autograd.Function</span>
<span class="c1"># The new function returns:</span>
<span class="c1"># (1) The updated outputs</span>
<span class="c1"># (2) A boolean mask of len(new_fn_outputs),</span>
<span class="c1">#     that can be used to tell autograd.grad which outputs should get tangents</span>
<span class="c1">#     if we trace the backward.</span>
<span class="k">def</span> <span class="nf">fn_prepped_for_autograd</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">args_maybe_cloned</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">maybe_to_fresh_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args_maybe_cloned</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>

        <span class="n">mutated_inputs_to_return</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args_maybe_cloned</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">or</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
        <span class="p">]</span>

        <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">:</span>
                <span class="n">intermediate_bases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_base</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">:</span>
                <span class="c1"># See Note [Intermediate Bases Optimization]</span>
                <span class="n">outs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_unsafe_view</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">meta</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">)</span>

        <span class="c1"># the compiled forward should return (mutated_inputs, user_outs, intermediate_bases)</span>
        <span class="n">fw_outs_to_return</span> <span class="o">=</span> <span class="o">*</span><span class="n">mutated_inputs_to_return</span><span class="p">,</span> <span class="o">*</span><span class="n">outs</span><span class="p">,</span> <span class="o">*</span><span class="n">intermediate_bases</span>

        <span class="c1"># Also return a boolean mask specifying which outputs to this function will be used as tangents</span>
        <span class="n">mutated_inputs_grad_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">meta</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">mutates_data</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mutated_inputs_to_return</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Pass any (non-aliased) outputs in as tangents, since they&#39;ll be returned as outputs in the fw</span>
        <span class="c1"># For outputs that are aliases of intermediates, we will have returned the output&#39;s _base as an output in the graph instead,</span>
        <span class="c1"># which we *should* send to grad()</span>
        <span class="n">output_grad_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">output_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">]</span>
            <span class="c1"># Also, only tensor outputs should participate in the backward</span>
            <span class="c1"># (in particular, Symint outputs in the forward graph shouldn&#39;t get tangents)</span>
            <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">intermediate_base_grad_mask</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">intermediate_bases</span><span class="p">))]</span>

        <span class="n">out_grad_mask</span> <span class="o">=</span> <span class="n">mutated_inputs_grad_mask</span> <span class="o">+</span> <span class="n">output_grad_mask</span> <span class="o">+</span> <span class="n">intermediate_base_grad_mask</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_grad_mask</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs_to_return</span><span class="p">)</span>

        <span class="c1"># Take care to grab and sync the updated inputs from primals_after_cloning (the inputs we actually mutate!)</span>
        <span class="c1"># and not primals (the preserved inputs, pre-mutation, that we pass to grad())</span>
        <span class="c1"># This is annoying: our joint function needs to be aware of functionalization</span>
        <span class="c1"># (syncing mutated inputs before calling autograd.grad())</span>
        <span class="c1"># In theory, we could make the autograd engine do this automatically, although that probably isn&#39;t any cleaner.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args_maybe_cloned</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fw_outs_to_return</span><span class="p">,</span> <span class="n">out_grad_mask</span>
    <span class="k">return</span> <span class="n">inner_fn</span>

<span class="c1"># Given a fn, computes the joint.</span>
<span class="c1"># NOTE: fn is expects the following behavior:</span>
<span class="c1"># (1) fn() needs to return a tuple of (outs, mask),</span>
<span class="c1">#     where `mask` tells us which outputs are meant to have tangents.</span>
<span class="c1">#     we don&#39;t know this info automatically, because we don&#39;t actually want to blindly</span>
<span class="c1">#     compute tangents for every output that requires grad.</span>
<span class="c1">#     Specifically, outputs that alias inputs won&#39;t participate in the backward and get tangents.</span>
<span class="c1"># (2) fn() cannot mutate any inputs that require gradient.</span>
<span class="c1">#     otherwise, when we compute autograd.grad(), we will not take those input mutations into account</span>
<span class="c1">#     (the way this is handled is that we ensure any inputs that normally get mutated are cloned first)</span>
<span class="k">def</span> <span class="nf">create_joint</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">tangents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]):</span>
        <span class="n">outs</span><span class="p">,</span> <span class="n">tangent_mask</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">primals</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tangent_mask</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="n">outs_to_grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span> <span class="k">for</span> <span class="n">needs_tangent</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tangent_mask</span><span class="p">,</span> <span class="n">outs</span><span class="p">)</span> <span class="k">if</span> <span class="n">needs_tangent</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs_to_grad</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span>

        <span class="c1"># Get the inputs that need gradients</span>
        <span class="n">grad_primals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">inputs_needs_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Note that we&#39;re not using primals here,</span>
        <span class="c1"># being carefully not to pass any mutated inputs into autograd.grad()</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">primals</span><span class="p">:</span>
            <span class="n">is_grad_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">inputs_needs_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">is_grad_tensor</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_grad_tensor</span><span class="p">:</span>
                <span class="n">grad_primals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="c1"># Get the outputs that need gradients</span>
        <span class="n">needed_outs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">needed_tangents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outs_to_grad</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="c1"># A bit sketchy, but fixes e.g. test_aot_autograd_exhaustive_matmul_cpu_float32</span>
                <span class="c1"># The issue is that we are sensitive to decomps that don&#39;t accurately maintain</span>
                <span class="c1"># their output&#39;s _base.shape compared to eager mode, and this helps mitigate a bit.</span>
                <span class="n">needed_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">out</span> <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">tangent</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tangent</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">needed_tangents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tangent</span><span class="p">)</span>

        <span class="n">setup_stacktrace_preservation_hooks</span><span class="p">([</span><span class="n">out</span><span class="o">.</span><span class="n">grad_fn</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">needed_outs</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
            <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">mark_beginning_of_backward</span><span class="p">()</span>
        <span class="n">backward_out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Call the backwards pass</span>
        <span class="k">if</span> <span class="n">grad_primals</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">():</span>
                <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                    <span class="n">needed_outs</span><span class="p">,</span>
                    <span class="n">grad_primals</span><span class="p">,</span>
                    <span class="n">grad_outputs</span><span class="o">=</span><span class="n">needed_tangents</span><span class="p">,</span>
                    <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="n">backward_out_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backward_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outs</span><span class="p">,</span> <span class="p">[</span>
            <span class="nb">next</span><span class="p">(</span><span class="n">backward_out_iter</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs_needs_grads</span>
        <span class="p">]</span>
    <span class="k">return</span> <span class="n">inner_fn</span>

<span class="c1"># This creates the final function that we want to trace using make_fx(),</span>
<span class="c1"># in both aot_dispatch_autograd and aot_dispatch_base.</span>
<span class="c1"># Preconditions:</span>
<span class="c1"># - fn corresponds to the user&#39;s fw function</span>
<span class="c1"># - fn arguments have been flattened, duplicate arguments have been handled</span>
<span class="c1"># - In the returned function, the &quot;primals&quot; arguments *includes* synthetic bases.</span>
<span class="c1"># This function does the work of functionalizing the input function,</span>
<span class="c1"># and performing copy_() calls at the end of the function if `keep_input_mutations` is set.</span>
<span class="c1"># The function returned has signature that is either:</span>
<span class="c1"># (1) &quot;traced_fn(primals: List[Any])&quot; if trace_joint is False</span>
<span class="c1"># (2) &quot;traced_fn(primals: List[Any], tangents: List[Any])&quot; if trace_joint is True</span>
<span class="k">def</span> <span class="nf">create_functionalized_graph</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">def</span> <span class="nf">functionalized_f_helper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Wrap inputs into functional wrappers</span>
        <span class="n">f_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_enable_functionalization</span><span class="p">(</span><span class="n">reapply_views</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Run the joint</span>
            <span class="n">f_outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">f_args</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_disable_functionalization</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">trace_joint</span><span class="p">:</span>
            <span class="c1"># Note: This is a bit annoying. There&#39;s a layering issue here, where:</span>
            <span class="c1"># (1) functionalization needs to operate on **synthetic base** inputs, before unpacking them into the &quot;real&quot; inputs.</span>
            <span class="c1"># (2) For keep_input_mutations, we support tracing a call to copy_() directly on mutated inputs.</span>
            <span class="c1">#     However, we **only** want to support this for inputs that have data-only (and no metadata) mutations,</span>
            <span class="c1">#     because inductor (and backends in generally) would prefer not to see these (e.g. as_strided_(), resize_()).</span>
            <span class="c1">#     This makes it pretty difficult for this logic to operate on synthetic bases.</span>
            <span class="c1"># (3) In addition, there are cases where it&#39;s significantly cheaper to perform the copy on the individual</span>
            <span class="c1">#     (unpacked) input aliases, instead of the synthetic base.</span>
            <span class="c1"># Example case where (3) could be important:</span>
            <span class="c1">#</span>
            <span class="c1">#     def f(x, y):</span>
            <span class="c1">#         x.mul_(2)</span>
            <span class="c1">#         y.mul_(3)</span>
            <span class="c1">#         return x, y</span>
            <span class="c1">#    a = torch.ones(1&#39;000&#39;000)</span>
            <span class="c1">#    x, y = out(a[0:9], a[1:10])</span>
            <span class="c1">#</span>
            <span class="c1"># It would be much better to add copy_() calls into the graph for the two tiny slices, instead of materializing</span>
            <span class="c1"># a giant &quot;updated synthetic base&quot; and copying into a&#39;s entire storage.</span>
            <span class="c1">#</span>
            <span class="c1"># For now, we are pessimistically not performing the optimization from (3);</span>
            <span class="c1"># we will materialize an &quot;updated&quot; synthetic base, and copy it back to the synthetic input base.</span>
            <span class="c1"># This allows us to factor aot autograd much more nicely, since only one area of the code needs to worry</span>
            <span class="c1"># about synthetic bases.</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inpt_old</span><span class="p">,</span> <span class="n">inpt_f</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">f_args</span><span class="p">)):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">)</span>
                <span class="n">inpt_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">inpt_f</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                    <span class="c1"># We found an input that had a (data-only) mutation.</span>
                    <span class="c1"># Since keep_input_mutations is set, we need to faithfully apply a copy_()</span>
                    <span class="c1"># so the compiler will see the input mutation in the graph.</span>
                    <span class="k">assert</span> <span class="n">inpt_new</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">inpt_old</span>
                    <span class="k">assert</span> <span class="n">has_same_metadata</span><span class="p">(</span><span class="n">inpt_new</span><span class="p">,</span> <span class="n">inpt_old</span><span class="p">)</span>
                    <span class="n">inpt_old</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">inpt_new</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">f_outs</span><span class="p">)</span>

    <span class="c1"># Kinda annoying, but needed to make sure that the fx graph we trace out has &quot;primals&quot;</span>
    <span class="c1"># and &quot;tangents&quot; as its input names (which are special-cased by the partitioner)</span>
    <span class="k">def</span> <span class="nf">joint_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">functionalized_f_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fwd_helper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">functionalized_f_helper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">helper</span> <span class="o">=</span> <span class="n">joint_helper</span> <span class="k">if</span> <span class="n">trace_joint</span> <span class="k">else</span> <span class="n">fwd_helper</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
        <span class="c1"># Setup the wrapper for functionalization of rng ops</span>
        <span class="n">helper</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">create_functionalized_rng_ops_wrapper</span><span class="p">(</span><span class="n">helper</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">trace_joint</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">helper</span><span class="p">,</span> <span class="n">decomposition_table</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fx_g</span>


<span class="k">def</span> <span class="nf">normalize_as_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>


<span class="n">aot_autograd_decompositions</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># This is a list since looking forward, we can have this arbitrarily nested.</span>
<span class="n">graph_being_compiled</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># TODO: It would be nice to reset the numbering every time aot_id goes</span>
<span class="c1"># up, but this is annoying to do right now (because we don&#39;t know if</span>
<span class="c1"># an aot_id will come back from the dead), so right now this also happens</span>
<span class="c1"># to be a globally unique number too (at the cost of wobbling if you change</span>
<span class="c1"># how the graphs compile)</span>
<span class="n">nth_graph</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>


<span class="k">def</span> <span class="nf">set_model_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">model_name</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">get_aot_compilation_context</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">),</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">nth_graph</span>


<span class="k">def</span> <span class="nf">get_aot_graph_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the name of the graph being compiled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">graph_being_compiled</span><span class="p">,</span> <span class="n">nth_graph</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">__</span><span class="si">{</span><span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">)</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">nth_graph</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="n">get_graph_being_compiled</span> <span class="o">=</span> <span class="n">get_aot_graph_name</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">graph_being_compiled</span>
    <span class="c1"># TODO: Don&#39;t shove the aot_id in here; set it in the context</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">global</span> <span class="n">nth_graph</span>
        <span class="n">nth_graph</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">make_boxed_func</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">g</span>


<span class="k">def</span> <span class="nf">make_boxed_compiler</span><span class="p">(</span><span class="n">compiler</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiler</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">):</span>
        <span class="n">out_f</span> <span class="o">=</span> <span class="n">compiler</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">)</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">out_f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fx_g</span>

    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">call_func_with_args</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">steal_args</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">steal_args</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
        <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Please remove soon</span>
            <span class="c1"># https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Your compiler for AOTAutograd is returning a function that doesn&#39;t take boxed arguments. &quot;</span>
                <span class="s2">&quot;Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. &quot;</span>
                <span class="s2">&quot;See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.&quot;</span>
            <span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">guard</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">aot_dispatch_base</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">):</span>
    <span class="c1"># aot_dispatch_base requires functionalization, but doesn&#39;t need to handle as many cases as the autograd case.</span>
    <span class="c1"># The cases that aot_dispatch_base doesn&#39;t need to handle include:</span>
    <span class="c1"># - outputs that are aliases of graph intermediates</span>
    <span class="c1"># - outputs that are aliases of graph inputs</span>
    <span class="c1"># While cases that it does need to handle include:</span>
    <span class="c1"># - input mutations (including when inputs are aliases of each other)</span>
    <span class="c1"># - input metadata mutations</span>
    <span class="n">keep_mutations</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span>
    <span class="n">fn_to_trace</span> <span class="o">=</span> <span class="n">fn_input_mutations_to_outputs</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">keep_data_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">fw_module</span> <span class="o">=</span> <span class="n">create_functionalized_graph</span><span class="p">(</span>
        <span class="n">fn_to_trace</span><span class="p">,</span>
        <span class="n">flat_args</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># As long as we opted to remove input mutations, then</span>
    <span class="c1"># there should be *NO* mutating ops in the graph at this point.</span>
    <span class="n">copy_count</span> <span class="o">=</span> <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">allow_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">)</span>

    <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
    <span class="n">fw_module</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

    <span class="n">copy_count2</span> <span class="o">=</span> <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">allow_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">copy_count</span> <span class="o">==</span> <span class="n">copy_count2</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">enable_log</span><span class="p">:</span>
        <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Forward graph&quot;</span><span class="p">,</span> <span class="n">fw_module</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>

    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>

    <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;inference&quot;</span><span class="p">):</span>
        <span class="n">compiler</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">inference_compiler</span> <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">inference_compiler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span>
        <span class="n">adjusted_flat_args</span> <span class="o">=</span> <span class="n">flat_args</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
            <span class="c1"># Add the seed and offset as example inputs to pass to the compiler</span>
            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">()</span>
            <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
            <span class="n">adjusted_flat_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="o">*</span><span class="n">flat_args</span><span class="p">]</span>
            <span class="n">flat_args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>  <span class="c1"># Don&#39;t hold extra reference</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">compiler</span><span class="p">(</span><span class="n">fw_module</span><span class="p">,</span> <span class="n">adjusted_flat_args</span><span class="p">)</span>

    <span class="c1"># This boxed_call handling happens inside create_runtime_wrapper as well.</span>
    <span class="c1"># However, create_runtime_wrapper does not expect the rng offsets in the</span>
    <span class="c1"># output. So, we have to create another wrapper and take out the offset. As</span>
    <span class="c1"># a result, we have to account for not boxed_call compilers as well.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">)</span>

    <span class="c1"># Create a wrapper to set up the rng functionalize bits</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">rng_functionalization_wrapper</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># args is a list because compiled_fw is boxed_call</span>
        <span class="k">if</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
            <span class="c1"># Add the seed and offset to args</span>
            <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">()</span>
            <span class="n">new_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">]</span>
            <span class="n">args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>  <span class="c1"># Remove the reference of inputs from the args itself</span>

            <span class="n">out</span> <span class="o">=</span> <span class="n">compiled_fw</span><span class="p">(</span><span class="n">new_args</span><span class="p">)</span>

            <span class="n">out</span> <span class="o">=</span> <span class="n">functionalized_rng_runtime_epilogue</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">compiled_fw</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_runtime_wrapper</span><span class="p">(</span>
        <span class="n">rng_functionalization_wrapper</span><span class="p">,</span>
        <span class="n">runtime_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">indices_of_inps_to_detach</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">compiled_fn</span>


<span class="c1"># Returns the number of detected copy_</span>
<span class="k">def</span> <span class="nf">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">allow_input_mutations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">placeholders</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">copy_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># NB: It would also be nice to verify that the mutations all happen at the</span>
    <span class="c1"># end, but we also do some administrative views after mutations so this</span>
    <span class="c1"># isn&#39;t actually true.  (TODO: Could this cause problems for Inductor?)</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fx_g</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
            <span class="n">placeholders</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">target</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">copy_</span><span class="o">.</span><span class="n">default</span> <span class="ow">and</span> <span class="n">allow_input_mutations</span><span class="p">:</span>
                <span class="n">suffix</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># Can only copy_ into an input, and can only do so once</span>
                <span class="k">assert</span> <span class="n">n</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">placeholders</span>
                <span class="n">placeholders</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">copy_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">,</span> \
                    <span class="sa">f</span><span class="s1">&#39;aot_autograd expected to have an entirely functional graph, but found </span><span class="si">{</span><span class="n">n</span><span class="o">.</span><span class="n">format_node</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="k">return</span> <span class="n">copy_count</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_autocast_manager</span><span class="p">():</span>
    <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">guard</span>


<span class="k">def</span> <span class="nf">are_differentiable_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">or</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span> <span class="ow">or</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">same_dtype_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="c1"># Note [Handling mutations on an input that aliases other inputs]</span>
<span class="c1"># The easiest example to show-case this edge case is here:</span>
<span class="c1">#</span>
<span class="c1"># def f(a, b):</span>
<span class="c1">#     a.mul_(2)</span>
<span class="c1">#     out = a + b</span>
<span class="c1">#     return out</span>
<span class="c1"># b = torch.ones(...)</span>
<span class="c1"># a = b.view(-1)</span>
<span class="c1"># f(a, b)</span>
<span class="c1">#</span>
<span class="c1"># In this situation, if a and b happened to be aliased, we need to trace something different!</span>
<span class="c1"># Suppose we had b = a.view(-1)</span>
<span class="c1"># (In this case, that means that `a._base is b`)</span>
<span class="c1">#</span>
<span class="c1"># We need to ensure that the aliasing relationship between a and b is preserved.</span>
<span class="c1"># We do that detecting the specific situation above (mutate an input that aliases another input),</span>
<span class="c1"># and when we do that, we create a synthetic base argument. Then inside of the traced forward,</span>
<span class="c1"># we regenerate a and b off of that base.</span>
<span class="c1"># The complete example of the transformed function looks like this:</span>
<span class="c1">#</span>
<span class="c1"># // The traced forward takes in a synthetic base, and regenerates the aliased inputs as views</span>
<span class="c1"># // We could consider getting view-replay support here to minimize as_strided_scatter ops in the graph</span>
<span class="c1"># def traced_forward(base):</span>
<span class="c1">#     a = base.as_strided(...)</span>
<span class="c1">#     b = base.as_strided(...)</span>
<span class="c1">#     a_updated = a.mul(2)</span>
<span class="c1">#     base_updated = torch.as_strided_scatter(base, a_updated, ...)</span>
<span class="c1">#     b_updated = base_updated.as_strided(...)</span>
<span class="c1">#     out = a_updated + b_updated</span>
<span class="c1">#     return a_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_fn(a, b):</span>
<span class="c1">#     // we detect that a is the &quot;differentiable base&quot; here</span>
<span class="c1">#     base = a</span>
<span class="c1">#     // In other situations, we might do either:</span>
<span class="c1">#     // (1) a and b are both views off of some larger differentiable base</span>
<span class="c1">#     //     assert a._base is b._base and a._base is not None</span>
<span class="c1">#     //     base = a._base</span>
<span class="c1">#     // (2) a and b both don&#39;t require gradients. Create a base from the storage</span>
<span class="c1">#     //     assert a._base is None and b._base is None</span>
<span class="c1">#     //     base = torch.Tensor(a.storage())</span>
<span class="c1">#     a_updated, out = traced_forward(base)</span>
<span class="c1">#     a.copy_(a_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># This function:</span>
<span class="c1"># (1) Merges input views into a synthetic base argument, when any of those input views are mutated</span>
<span class="c1"># (2) Returns metadata telling the autograd.Function how to modify their arguments properly,</span>
<span class="c1">#     to respect the new calling convention.</span>
<span class="c1">#</span>
<span class="c1"># The calling convention is as follows.</span>
<span class="c1"># Any inputs that were originally views of one another get yanked, and replaced with a synthetic base.</span>
<span class="c1"># The argument list ordering goes [base1, ..., baseN], [arg1, ..., argN],</span>
<span class="c1"># Where the ordering of the bases is determined from the ordering of the original view args.</span>
<span class="c1"># baseA will come before baseB if the earliest original argument coming from baseA</span>
<span class="c1"># showed up earlier in the argument list than the earliest original argument coming from baseB.</span>
<span class="c1">#</span>
<span class="c1"># Example, given some tensors a, b, c, d</span>
<span class="c1"># call site:</span>
<span class="c1">#   f(a, c.view(-1), b.view(-1), b, c, d)</span>
<span class="c1"># Modified argument list:</span>
<span class="c1">#   c_base comes first because the first c view came earlier in arg list than the first b view</span>
<span class="c1">#   a and d still show up in the modified arg list, but b and c don&#39;t- they&#39;re regenerated from their bases</span>
<span class="c1">#   b_base = torch.Tensor(b.storage())</span>
<span class="c1">#   c_base = torch.Tensor(c.storage())</span>
<span class="c1">#   f(c_base, b_base, a, d)</span>
<span class="k">def</span> <span class="nf">merge_view_inputs</span><span class="p">(</span>
    <span class="n">fwd_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">mutated_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># The autograd case currently has more restrictions than the inference case.</span>
    <span class="n">is_inference</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]]:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_input_info</span><span class="p">)</span>
    <span class="n">storage_ref_to_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">base_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">other_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">storage_ref</span> <span class="o">=</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
            <span class="n">storage_ref_to_idx</span><span class="p">[</span><span class="n">storage_ref</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span>
    <span class="c1"># Note [Synthetic Base Info Metadata]</span>
    <span class="c1"># This list contains metadata that tells you what the i&#39;th argument in the inner calling convention should be.</span>
    <span class="c1"># It&#39;s either:</span>
    <span class="c1"># - another int (corresponding to the index in the argument list of the element from the outer calling convention)</span>
    <span class="c1"># - idx, view_tensor, where we can generate the new output with view_tensor._view_func(old_args[idx])</span>
    <span class="c1">#   idx corresponds to which synthetic base from the outer calling context to view</span>
    <span class="n">inner_calling_convention_meta</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">aliased_input_indices</span> <span class="ow">in</span> <span class="n">storage_ref_to_idx</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
            <span class="c1"># We only care about mutations that affect all aliases,</span>
            <span class="c1"># so metadata mutations on an input doesn&#39;t require us to do synthetic base handling.</span>
            <span class="n">mutated_input_info</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
            <span class="k">for</span> <span class="n">inpt_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
        <span class="p">):</span>
            <span class="k">for</span> <span class="n">curr_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
                <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">])</span>
            <span class="k">continue</span>
        <span class="c1"># We detected an input that was mutated, AND aliases with another input.</span>
        <span class="c1"># we need to replace this set of aliased inputs with a single synthetic base.</span>
        <span class="c1"># For now, I&#39;m banning a bunch of cases. We expect dynamo to properly detect these cases</span>
        <span class="c1"># and error out. We can fix them later.</span>
        <span class="c1"># These checks are transitive, so we don&#39;t need to check every pair.</span>
        <span class="k">for</span> <span class="n">idx1</span><span class="p">,</span> <span class="n">idx2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">,</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">view1</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx1</span><span class="p">]</span>
            <span class="n">view2</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span>
            <span class="c1"># The &quot;inputs that are aliased but have different differentiable bases&quot; case</span>
            <span class="c1"># is more complicated and hopefully pretty rare. Not currently handled.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_inference</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">are_differentiable_views</span><span class="p">(</span>
                    <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
            <span class="c1"># Regenerating views when reinterpreting complex / real tensors seems non-trivial,</span>
            <span class="c1"># not handling for now</span>
            <span class="k">assert</span> <span class="n">same_dtype_views</span><span class="p">(</span>
                <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
            <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle input mutations on views with different dtypes.&quot;</span>
        <span class="n">non_none_bases</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
            <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="n">aliases_with_none_bases</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span> <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_none_bases</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Case where none of the aliases have a ._base</span>
            <span class="c1"># we generate a synthetic base without gradients, and generate views off of it</span>
            <span class="c1"># We hit this case when we have input tensors to the graph that share a storage,</span>
            <span class="c1"># but do not have a ._base field.</span>
            <span class="c1"># Wondering when we hit this case?</span>
            <span class="c1"># The _base field simply says that autograd knows about the aliasing relationship,</span>
            <span class="c1"># but sometimes we create tensors which are aliased out of the same storage but guaranteed</span>
            <span class="c1"># to be disjoint. In these cases, we will skip setting up the _base relationship</span>
            <span class="c1"># for performance reasons (because the fact that the tensors share the same storage</span>
            <span class="c1"># is unobservable unless you (1) do naughty things with resize_/as_strided</span>
            <span class="c1"># or (2) look at the storage--as we are doing here.)</span>
            <span class="c1"># One particular example of this is optimizer steps on the LSTM module:</span>
            <span class="c1"># LSTM parameters are packed into a contiguous storage for efficiency reasons when</span>
            <span class="c1"># calling cuDNN kernels, so when these parameters get passed to the optimizer we will</span>
            <span class="c1"># find they share the same storage, but do not have _base set since they are all disjoint.</span>
            <span class="c1">#</span>
            <span class="c1"># NOTE: There is one case where this is unsafe:</span>
            <span class="c1"># torch.Tensor(storage) will ALWAYS create a 1D tensor, which is not necessarily</span>
            <span class="c1"># the same shape as the &quot;actual&quot; base that the tensor came from.</span>
            <span class="c1"># For the most part this is fine, because we always use as_strided()</span>
            <span class="c1"># to generate the original aliased inputs again.</span>
            <span class="c1"># If we were to use view-replay though, this could cause the aliased views</span>
            <span class="c1"># to have incorrect sizes.</span>
            <span class="n">example_idx</span> <span class="o">=</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">example_alias</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span>
            <span class="c1"># Note that this function is re-used at both trace time and rutnime.</span>
            <span class="c1"># At trace time, we&#39;re under a FakeMode so synthetic_base becomes a FakeTensor.</span>
            <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">example_alias</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">example_alias</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># We don&#39;t actually have a convenient way of going from storage -&gt; tensor,</span>
            <span class="c1"># So using set_() here (we suffer some minor overhead, but this case is rare).</span>
            <span class="n">synthetic_base</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">example_alias</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Case where all of the aliases require gradients, and have the same _base.</span>
            <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">other_base</span> <span class="ow">in</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">other_base</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
            <span class="k">for</span> <span class="n">alias</span> <span class="ow">in</span> <span class="n">aliases_with_none_bases</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">alias</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
        <span class="n">base_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">synthetic_base</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">curr_view_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
            <span class="n">curr_view</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span>
            <span class="n">base_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="c1"># We store just enough info here so that we can regenerate the view later.</span>
            <span class="c1"># Regeneration: curr_view._view_func(args[base_idx])</span>
            <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">base_idx</span><span class="p">,</span> <span class="n">curr_view</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">other_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span>
        <span class="c1"># If no synthetic bases are necessary, just return the original inputs.</span>
        <span class="k">return</span> <span class="n">fwd_inputs</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Otherwise, return:</span>
        <span class="c1"># (1) The new args according to the updated calling convention: (synthetic_bases, other_args)</span>
        <span class="c1"># (2) Metadata telling functionalization how to generate the inner argument list given the outer calling convention.</span>
        <span class="c1">#     We post-process it into a list, where meta[i] tells you info about the i&#39;th argument in the inner calling convention.</span>
        <span class="n">args_to_functionalization</span> <span class="o">=</span> <span class="n">base_args</span> <span class="o">+</span> <span class="n">other_args</span>
        <span class="n">arg_to_old_idx_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">arg</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">other_arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">other_args</span><span class="p">):</span>
            <span class="n">new_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span>
            <span class="n">old_idx</span> <span class="o">=</span> <span class="n">arg_to_old_idx_map</span><span class="p">[</span><span class="n">other_arg</span><span class="p">]</span>
            <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">old_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_idx</span>
        <span class="c1"># post process into a list</span>
        <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inner_calling_convention_meta</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inner_calling_convention_meta</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">post_processed_calling_convention_meta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># Quick assert: every argument in the inner calling convention should be accounted for.</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">args_to_functionalization</span><span class="p">,</span> <span class="n">post_processed_calling_convention_meta</span>


<span class="k">def</span> <span class="nf">format_guard_bug_msg</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">expected</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;At compilation time, graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> was compiled under the &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;assumption that </span><span class="si">{</span><span class="n">expected</span><span class="si">}</span><span class="s2">, but at runtime this was not the case.  &quot;</span>
        <span class="s2">&quot;This indicates a guard bug in AOTAutograd or Dynamo, please file a bug to PyTorch.&quot;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">remove_dupe_metadata</span><span class="p">(</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">keep_arg_mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_arg_mask</span><span class="p">)</span>
    <span class="c1"># Easy invariant: the first argument should never be a dupe (it will be kept)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_arg_mask</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">keep_arg_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dupe_to_dedup_idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">keep_arg_mask</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="k">if</span> <span class="n">b</span><span class="p">:</span>
            <span class="n">dupe_to_dedup_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dupe_to_dedup_idx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dupe_to_dedup_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dupe_to_dedup_idx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Filter dupe&#39;d mutated inputs out of traced_tangents</span>
    <span class="n">num_data_mutations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">])</span>
    <span class="n">other_traced_tangents</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">[</span><span class="n">num_data_mutations</span><span class="p">:]</span>
    <span class="n">inp_traced_tangents</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">[:</span><span class="n">num_data_mutations</span><span class="p">]</span>
    <span class="n">filtered_inp_traced_tangents</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inp_traced_tangents</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep_arg_mask</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]]</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">filtered_inp_traced_tangents</span> <span class="o">+</span> <span class="n">other_traced_tangents</span>

    <span class="k">return</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
        <span class="n">input_info</span><span class="o">=</span><span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep_arg_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span>
        <span class="c1"># requires_grad_info consists of (mutated_inputs, forward_outputs).</span>
        <span class="c1"># Need to remove only the duplicate entries that correspond to the mutated inputs.</span>
        <span class="n">requires_grad_info</span><span class="o">=</span><span class="p">[</span>
            <span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">)</span> <span class="ow">or</span> <span class="n">keep_arg_mask</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">mutated_inp_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]],</span>
        <span class="c1"># For outputs that are views of inputs, we store the index of the input that the output</span>
        <span class="c1"># was generated from. Need to update that index to account for removed dupes.</span>
        <span class="n">output_info</span><span class="o">=</span><span class="p">[</span>
            <span class="n">OutputAliasInfo</span><span class="p">(</span>
                <span class="n">output_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">output_type</span><span class="p">,</span>
                <span class="n">raw_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span>
                <span class="n">dynamic_dims</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span><span class="p">,</span>
                <span class="n">base_idx</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">base_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dupe_to_dedup_idx</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">output_info</span>
        <span class="p">],</span>
        <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="n">traced_tangents</span><span class="o">=</span><span class="n">traced_tangents</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Given our ViewAndMutation metadata, this fn constructs a new set of metadata,</span>
<span class="c1"># after adding synthetic base arguments to the function.</span>
<span class="c1"># Most of the work in this fn is slogging through all of the metadata corresponding to inputs,</span>
<span class="c1"># and updating it with our synthetic base calling convention.</span>
<span class="c1">#</span>
<span class="c1"># When config.debug_assert is set, we automatically regenerate the metadata</span>
<span class="c1"># and compare it to this output for sanity.</span>
<span class="c1">#</span>
<span class="c1"># In addition to the updated metadata, also return the list of input indices</span>
<span class="c1"># that will need to be updated in the synthetic base epilogue</span>
<span class="k">def</span> <span class="nf">create_synthetic_base_metadata</span><span class="p">(</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="c1"># Maps each outer argument idx to its inner idx (or, if this outer arg is generated from a</span>
    <span class="c1"># synthetic base, you get a tuple of (i, TensorMeta), telling you the base tensor idx, and view metadata)</span>
    <span class="n">synthetic_base_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]],</span>
    <span class="n">outer_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">inner_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ViewAndMutationMeta</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>

    <span class="n">S_Outer</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s1">&#39;S_Outer&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="n">S_Inner</span> <span class="o">=</span> <span class="n">NewType</span><span class="p">(</span><span class="s1">&#39;S_Inner&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="n">synthetic_base_to_indices</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">S_Inner</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">S_Outer</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">inner_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inner_args</span><span class="p">)):</span>
        <span class="n">outer_aliased_indices_of_current_base_arg</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">outer_idx</span> <span class="k">for</span> <span class="n">outer_idx</span><span class="p">,</span> <span class="n">inner_idx_or_tuple</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_idx_or_tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inner_idx_or_tuple</span> <span class="o">==</span> <span class="n">inner_idx</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_idx_or_tuple</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inner_idx_or_tuple</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">inner_idx</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">synthetic_base_to_indices</span><span class="p">[</span><span class="n">inner_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">outer_aliased_indices_of_current_base_arg</span>

    <span class="c1"># given the requires_grad info on mutated inputs,</span>
    <span class="c1"># generate the requires_grad info on those same mutated inputs, but after constructing synthetic bases.</span>
    <span class="n">input_infos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">mutated_inp_require_grad_info</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">outer_indices</span> <span class="ow">in</span> <span class="n">synthetic_base_to_indices</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># leaf-ness should be all-or-nothing for aliased tensor.</span>
        <span class="c1"># (aka if &quot;a&quot; and &quot;b&quot; are views, then a.is_leaf == b.is_leaf)</span>
        <span class="n">any_leaf</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">is_leaf</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outer_indices</span><span class="p">)</span>
        <span class="n">all_leaf</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">is_leaf</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outer_indices</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">any_leaf</span> <span class="o">==</span> <span class="n">all_leaf</span>
        <span class="n">inpt_info</span> <span class="o">=</span> <span class="n">InputAliasInfo</span><span class="p">(</span>
            <span class="c1"># If len(outer_indices) &gt; 1, then this input is a synthetic base.</span>
            <span class="c1"># The invariant is that to the rest of aot autograd, synthetic bases only show up if</span>
            <span class="c1"># one of their aliases gets a data mutation. And if any of their aliases get metadata</span>
            <span class="c1"># mutations, they will be hidden from the rest of aot autograd.</span>
            <span class="n">mutates_data</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outer_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">outer_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">,</span>
            <span class="n">mutates_metadata</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outer_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">outer_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">,</span>
            <span class="n">is_leaf</span><span class="o">=</span><span class="n">any_leaf</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">input_infos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inpt_info</span><span class="p">)</span>
        <span class="c1"># requires_grad_info consists of (mutated_inputs, forward_outputs).</span>
        <span class="c1"># For any mutated inputs that correspond to aliased inputs,</span>
        <span class="c1"># Need to replace them with their mutated synthetic base</span>
        <span class="k">if</span> <span class="n">inpt_info</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">inpt_info</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
            <span class="n">mutated_inp_require_grad_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">any</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outer_indices</span><span class="p">))</span>

    <span class="c1"># Find any inputs that fulfill the following criteria:</span>
    <span class="c1"># (1) They are part of a synthetic base (because they alias another input,</span>
    <span class="c1">#      and at least one input experiences a data mutation)</span>
    <span class="c1"># (2) They experience a metadata mutation</span>
    <span class="n">outer_aliased_arg_idx_with_metadata_mutations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">outer_idx</span> <span class="k">for</span> <span class="n">outer_idx</span><span class="p">,</span> <span class="n">inpt_info</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inpt_info</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># grab the original requires grad info on the outputs, except the ones from the mutated inputs</span>
    <span class="n">num_original_input_data_mutations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">])</span>
    <span class="n">output_grad_info</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">num_original_input_data_mutations</span><span class="p">:]</span>
    <span class="n">input_metadata_mutation_grad_info</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">outer_args</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">outer_idx</span> <span class="ow">in</span> <span class="n">outer_aliased_arg_idx_with_metadata_mutations</span><span class="p">]</span>
    <span class="n">input_metadata_output_info</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">OutputAliasInfo</span><span class="p">(</span>
            <span class="n">output_type</span><span class="o">=</span><span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">,</span>
            <span class="n">raw_type</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">dynamic_dims</span><span class="o">=</span><span class="p">{</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outer_args</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_concrete_int</span><span class="p">(</span><span class="n">s</span><span class="p">)},</span>
            <span class="n">base_idx</span><span class="o">=</span><span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">outer_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">)</span> <span class="k">for</span> <span class="n">outer_idx</span> <span class="ow">in</span> <span class="n">outer_aliased_arg_idx_with_metadata_mutations</span><span class="p">]</span>
    <span class="n">existing_output_infos</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">OutputAliasInfo</span><span class="p">(</span>
            <span class="n">output_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">output_type</span><span class="p">,</span>
            <span class="n">raw_type</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span>
            <span class="n">dynamic_dims</span><span class="o">=</span><span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span><span class="p">,</span>
            <span class="c1"># Map the input idx pre-synthetic-bases to the new idx post-synthetic-bases</span>
            <span class="n">base_idx</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">base_idx</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">synthetic_base_info</span><span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">base_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">output_info</span><span class="p">]</span>

    <span class="n">num_outer_mutated_data_inps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">])</span>
    <span class="n">inner_mutated_data_inps</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">inner_idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inner_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">input_infos</span><span class="p">[</span><span class="n">inner_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">]</span>

    <span class="n">requires_grad_info</span> <span class="o">=</span> <span class="n">mutated_inp_require_grad_info</span> <span class="o">+</span> <span class="n">output_grad_info</span> <span class="o">+</span> <span class="n">input_metadata_mutation_grad_info</span>
    <span class="n">output_info</span> <span class="o">=</span> <span class="n">existing_output_infos</span> <span class="o">+</span> <span class="n">input_metadata_output_info</span>
    <span class="c1"># Regenerate traced tangents to include mutated inputs including synthetic bases</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">inner_mutated_data_inps</span> <span class="o">+</span> <span class="n">m</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">[</span><span class="n">num_outer_mutated_data_inps</span><span class="p">:]</span>

    <span class="k">return</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
        <span class="n">input_info</span><span class="o">=</span><span class="n">input_infos</span><span class="p">,</span>
        <span class="n">requires_grad_info</span><span class="o">=</span><span class="n">requires_grad_info</span><span class="p">,</span>
        <span class="n">output_info</span><span class="o">=</span><span class="n">output_info</span><span class="p">,</span>
        <span class="n">num_intermediate_bases</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="n">traced_tangents</span><span class="o">=</span><span class="n">traced_tangents</span><span class="p">,</span>
    <span class="p">),</span> <span class="n">outer_aliased_arg_idx_with_metadata_mutations</span>

<span class="c1"># MOTIVATION:</span>
<span class="c1">#</span>
<span class="c1"># When tracing functions for future execution, one must be careful not to pass</span>
<span class="c1"># in the same input tensor multiple times (e.g., f(x, x), as this can result</span>
<span class="c1"># in graphs that are ONLY valid if you later pass a new tensor in exactly the</span>
<span class="c1"># same way (e.g., f(y, y)).  (NB: we really mean duplicate; two distinct</span>
<span class="c1"># tensors that alias each other is a different situation that is covered by</span>
<span class="c1"># aot_dispatch_deduplicated_autograd). Here are two examples:</span>
<span class="c1">#</span>
<span class="c1"># (1) Suppose you have a function:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return x + y</span>
<span class="c1">#</span>
<span class="c1"># If you make_fx(f)(x, x), you will trace out:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return y + y</span>
<span class="c1">#</span>
<span class="c1"># Oops!</span>
<span class="c1">#</span>
<span class="c1"># (2) For most tensors x and y, you can compute f&#39;s gradient with respect to</span>
<span class="c1"># these to inputs by saying torch.autograd.grad(f(x, y), (x, y)).  However,</span>
<span class="c1"># if x is y, you will trace out a program that gets incorrect gradients:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; x = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + x, (x, x))</span>
<span class="c1">#   (tensor([2.]), tensor([2.]))</span>
<span class="c1">#</span>
<span class="c1"># In other words, the gradient is double-counted.  Deduplicating the arguments</span>
<span class="c1"># gives you an appropriate gradient:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; y = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + y, (x, y))</span>
<span class="c1">#   (tensor([1.]), tensor([1.]))</span>
<span class="c1">#</span>
<span class="c1"># HOW TO DEDUPLICATE:</span>
<span class="c1">#</span>
<span class="c1"># There are a few strategies, in order of preference:</span>
<span class="c1">#</span>
<span class="c1"># 1. For every duplicate argument to the function, detach it into</span>
<span class="c1">#    a separate leaf tensor, so that it is no longer duplicated.</span>
<span class="c1">#</span>
<span class="c1">#       PRO: The resulting compiled graph works for any configuration</span>
<span class="c1">#       of duplicated arguments.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the metadata of inputs:</span>
<span class="c1">#</span>
<span class="c1">#           def f(x, y):</span>
<span class="c1">#               x.transpose_(0, 1)</span>
<span class="c1">#               y.transpose_(0, 2)</span>
<span class="c1">#</span>
<span class="c1">#           x = torch.randn(2, 3, 4)</span>
<span class="c1">#           f(x, x)</span>
<span class="c1">#</span>
<span class="c1">#       The ordering of the transposes inside f dictates whether or not</span>
<span class="c1">#       you get [4, 2, 3] or [3, 4, 2].  This means that you cannot precompute</span>
<span class="c1">#       what metadata mutations should get applied to each input; you need to</span>
<span class="c1">#       assume they aren&#39;t duplicates (what we do today) or preserve</span>
<span class="c1">#       the original metadata mutations exactly in order, so that they work</span>
<span class="c1">#       for any duplicate configuration.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the data of inputs.</span>
<span class="c1">#       In particular, leaf tensors that require grad cannot be mutated,</span>
<span class="c1">#       this makes it impossible to differentiate with respect to the original</span>
<span class="c1">#       base.</span>
<span class="c1">#</span>
<span class="c1"># 2. For every duplicate argument to the function, remove it, so it is</span>
<span class="c1">#    no longer part of the &quot;true&quot; signature:</span>
<span class="c1">#</span>
<span class="c1">#       PRO: Implemented naively, it still works for metadata/data mutation.</span>
<span class="c1">#</span>
<span class="c1">#       CON: The resulting compiled graph is duplicate-specialized: it only</span>
<span class="c1">#       works if future calls duplicate arguments in exactly the same way.</span>
<span class="c1">#       Horribly, Dynamo doesn&#39;t guard on this at the moment.  But even if</span>
<span class="c1">#       it did, you could still end up recompiling a bunch of each duplicate.</span>
<span class="c1">#</span>
<span class="c1"># Our strategy is to do (1) if we can, and do (2) otherwise, erroring if</span>
<span class="c1"># Dynamo&#39;s guards are not enough.  In practice, this seems to cover</span>
<span class="c1"># everything.</span>
<span class="c1">#</span>
<span class="k">def</span> <span class="nf">aot_wrapper_dedupe</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">compiler_fn</span><span class="p">,</span>
    <span class="n">fw_metadata</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Use information about whether or not flat_fn mutates its arguments</span>
    <span class="c1"># or not to handle dupe args</span>

    <span class="c1"># Strategy 1: For any input that is not mutated, we can leafify it if we</span>
    <span class="c1"># need to remove a duplicate.</span>
    <span class="n">leaf_flat_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">args_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">ok</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">a</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args_set</span><span class="p">:</span>
            <span class="n">args_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
            <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ok</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>

    <span class="k">if</span> <span class="n">ok</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">leaf_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>

    <span class="c1"># Strategy 2: Duplicate specialize.</span>
    <span class="c1">#</span>
    <span class="c1"># In Haskell types, suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   add_dupe_args :: DedupedArgs -&gt; Args</span>
    <span class="c1">#   remove_dupe_args :: Args -&gt; DedupedArgs</span>
    <span class="c1">#</span>
    <span class="c1">#   compiler_fn</span>
    <span class="c1">#       :: (DedupedArgs -&gt; R) -&gt; DedupedArgs -&gt; AOTConfig -&gt; (DedupedArgs -&gt; R)</span>
    <span class="c1">#   deped_compiler_fn</span>
    <span class="c1">#       :: (Args -&gt; R) -&gt; Args -&gt; AOTConfig -&gt; (Args -&gt; R)</span>
    <span class="c1">#</span>
    <span class="c1"># Then the code below can be written in point-free style as:</span>
    <span class="c1">#</span>
    <span class="c1">#   deduped_compiler_fn f a c =</span>
    <span class="c1">#       compiler_fn (f . add_dupe_args) (remove_dupe_args a) c . remove_dupe_args</span>
    <span class="c1">#</span>
    <span class="c1"># Suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># We want:</span>
    <span class="c1">#</span>
    <span class="c1">#   remove_dupe_args([a, b, a, c]) == [a, b, c]</span>
    <span class="c1">#   add_dupe_args([a, b, c]) == [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># This is done via (respectively):</span>
    <span class="c1">#</span>
    <span class="c1">#   seen_args = {a: 0, b: 1, c: 2}</span>
    <span class="c1">#   enumerate(add_dupe_map) = [  # how to get args from the deduped list</span>
    <span class="c1">#       (0, 0),</span>
    <span class="c1">#       (1, 1),</span>
    <span class="c1">#       (2, 0),</span>
    <span class="c1">#       (3, 2),</span>
    <span class="c1">#   ]</span>
    <span class="c1">#   keep_arg_mask = [True, True, False, True]</span>

    <span class="n">seen_args</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">keep_arg_mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Implicitly map duped arg position (list index) to de-duped arg position</span>
    <span class="n">add_dupe_map</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">duped_arg_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># index into deduped_flat_args</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">seen_args</span><span class="p">:</span>
                <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">add_dupe_map</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
                <span class="k">continue</span>
            <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>

        <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">add_dupe_map</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">)</span> <span class="o">==</span> <span class="n">duped_arg_len</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Expects add_dupe_map to have length </span><span class="si">{</span><span class="n">duped_arg_len</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># NB: Hot path, avoid set lookups here</span>
    <span class="c1"># TODO: Can avoid the zip here too, probably</span>
    <span class="k">def</span> <span class="nf">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">keep</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duped_arg_len</span><span class="p">)]</span>

    <span class="n">deduped_flat_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="c1"># Update our input metadata to remove duped input metadata.</span>
    <span class="n">updated_fw_metadata</span> <span class="o">=</span> <span class="n">remove_dupe_metadata</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)</span>

    <span class="n">tracing_context</span> <span class="o">=</span> <span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">tracing_context</span> <span class="ow">and</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">:</span>
        <span class="c1"># TODO(voz): This structure is 1:1, we could consider an alternate structure like</span>
        <span class="c1"># kept_pos:[dupe_arg_pos], however, add_dupe_map is 1:1 so we would need a new structure there,</span>
        <span class="c1"># which feels like needless complexity for a tiny bit of efficiency at this point.</span>
        <span class="k">for</span> <span class="n">dupe_arg_pos</span><span class="p">,</span> <span class="p">(</span><span class="n">kept_pos</span><span class="p">,</span> <span class="n">keep_arg</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">keep_arg</span><span class="p">:</span>
                <span class="n">dupe_arg_source</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">[</span><span class="n">dupe_arg_pos</span><span class="p">]</span>
                <span class="n">kept_arg_source</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">[</span><span class="n">kept_pos</span><span class="p">]</span>
                <span class="n">tracing_context</span><span class="o">.</span><span class="n">guards_context</span><span class="o">.</span><span class="n">aotautograd_guards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">DuplicateInputs</span><span class="p">(</span><span class="n">kept_arg_source</span><span class="p">,</span> <span class="n">dupe_arg_source</span><span class="p">))</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="n">ref_fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
            <span class="n">wrapped_flat_fn</span><span class="p">,</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="p">)(</span><span class="o">*</span><span class="n">deduped_flat_args</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">ref_fw_metadata</span> <span class="o">==</span> <span class="n">updated_fw_metadata</span><span class="p">,</span> \
            <span class="sa">f</span><span class="s1">&#39;ref_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ref_fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s1">, actual_metadata=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">updated_fw_metadata</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">wrapped_flat_fn</span><span class="p">,</span> <span class="n">deduped_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">updated_fw_metadata</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">deduped_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">deduped_args</span><span class="p">)</span>

    <span class="n">wrapped_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># This can be uncommented when we properly guard for duplicates,</span>
    <span class="c1"># but right now we must not do it.</span>
    <span class="c1"># if not config.debug_assert:</span>
    <span class="c1">#     return wrapped_compiled_fn</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">wrapped_compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debugged_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Test that the computed remove/add arg functions are an inverse</span>
        <span class="n">new_args</span> <span class="o">=</span> <span class="n">add_dupe_args</span><span class="p">(</span><span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_args</span><span class="p">,</span> <span class="n">args</span><span class="p">)):</span>
            <span class="n">seen</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="ow">is</span> <span class="n">y</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                <span class="n">aot_config</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would be a duplicate of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># This is only an error if there is metadata mutation on both of</span>
        <span class="c1"># the duped arguments; in this case, we need to know what order</span>
        <span class="c1"># the metadata mutation applies in.  You&#39;ll get the correct result</span>
        <span class="c1"># otherwise, because a graph that assumes distinct inputs works if</span>
        <span class="c1"># you dupe the inputs (the gradient contributions from each input</span>
        <span class="c1"># will get summed up appropriately.)</span>
        <span class="c1">#</span>
        <span class="c1"># TODO: work out how to setup this assert correctly</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        assert len(seen) == unique_args, format_guard_bug_msg(aot_config,</span>
<span class="sd">            f&quot;there would be {unique_args} distinct arguments&quot;</span>
<span class="sd">        )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">debugged_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">debugged_compiled_fn</span>

<span class="c1"># This layer handles the situation where you have two inputs that alias each other,</span>
<span class="c1"># and one of the inputs is mutated.</span>
<span class="c1"># We need to take special care to ensure that the mutation is applied to the other aliases in the graph.</span>
<span class="c1">#</span>
<span class="c1"># pre-condition: aot_wrapper_dedup has already run.</span>
<span class="c1"># (This function will in theory work if there are duplicate args.</span>
<span class="c1"># However, the synthetic base code path is a bit sub-optimal, and running with dupe&#39;d inputs</span>
<span class="c1"># would cause us to hit that path more frequently).</span>
<span class="k">def</span> <span class="nf">aot_wrapper_synthetic_base</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="c1"># Currently, the only reason we need to plumb this bool is because</span>
    <span class="c1"># the synthetic base code prohibits more cases in the autograd case than the inference case.</span>
    <span class="n">needs_autograd</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">compiler_fn</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">is_inference</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">needs_autograd</span>
    <span class="n">flat_args_with_synthetic_bases</span><span class="p">,</span> <span class="n">synthetic_base_info</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span> <span class="n">is_inference</span><span class="o">=</span><span class="n">is_inference</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Happy path: we don&#39;t need synthetic bases</span>
    <span class="k">if</span> <span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">)</span>

    <span class="c1"># Update our forward metadata to take synthetic bases into account</span>
    <span class="n">fw_metadata_updated</span><span class="p">,</span> <span class="n">aliased_arg_idx_with_metadata_mutations</span> <span class="o">=</span> \
        <span class="n">create_synthetic_base_metadata</span><span class="p">(</span><span class="n">fw_metadata</span><span class="p">,</span> <span class="n">synthetic_base_info</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_args_with_synthetic_bases</span><span class="p">)</span>

    <span class="n">num_aliased_args_with_metadata_mutations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_arg_idx_with_metadata_mutations</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">unpack_synthetic_bases</span><span class="p">(</span><span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="n">f_args_inner</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">inner_idx_or_tuple</span> <span class="ow">in</span> <span class="n">synthetic_base_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inner_idx_or_tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primals</span><span class="p">[</span><span class="n">inner_idx_or_tuple</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">inner_base_idx</span><span class="p">,</span> <span class="n">view_tensor</span> <span class="o">=</span> <span class="n">inner_idx_or_tuple</span>
                <span class="n">base</span> <span class="o">=</span> <span class="n">primals</span><span class="p">[</span><span class="n">inner_base_idx</span><span class="p">]</span>
                <span class="n">view_arg</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span>
                    <span class="n">base</span><span class="p">,</span> <span class="n">view_tensor</span><span class="p">,</span> <span class="n">view_tensor</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="p">)</span>
                <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">view_arg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">f_args_inner</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">unpacked_args</span> <span class="o">=</span> <span class="n">unpack_synthetic_bases</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="c1"># This is a bit subtle. The goal of this entire function (aot_dispatch_synthetic_bases)</span>
        <span class="c1"># is to relieve the downstream logic from having to reason about mutations on inputs that alias</span>
        <span class="c1"># each other, by replacing aliased inputs with a synthetic base.</span>
        <span class="c1"># One area where this breaks down a bit however is if one of those aliased inputs</span>
        <span class="c1"># experienced a metadata mutation.</span>
        <span class="c1"># We are now obligated to reapply the metadata mutation directly to the user&#39;s input;</span>
        <span class="c1"># it isn&#39;t enough to apply mutations back to the synthetic base in the downstream logic.</span>
        <span class="c1">#</span>
        <span class="c1"># The way we handle this is by pretending that those aliased inputs that experience metadata mutations</span>
        <span class="c1"># are additional outputs in the user&#39;s forward function.</span>
        <span class="c1"># The downstream logic will just treat these as &quot;user outputs that alias inputs&quot;.</span>
        <span class="c1"># However, we will manually grab them at runtime here, use them to reapply the metadata mutation</span>
        <span class="c1"># to the user inputs, and not return them to the user.</span>
        <span class="n">aliased_args_with_metadata_mutations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">unpacked_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_arg_idx_with_metadata_mutations</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_args_with_metadata_mutations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">*</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">unpacked_args</span><span class="p">)),</span> <span class="o">*</span><span class="n">aliased_args_with_metadata_mutations</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">unpacked_args</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="n">ref_fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
            <span class="n">wrapped_flat_fn</span><span class="p">,</span>
            <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">keep_input_mutations</span><span class="p">,</span>
        <span class="p">)(</span><span class="o">*</span><span class="n">flat_args_with_synthetic_bases</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">ref_fw_metadata</span> <span class="o">==</span> <span class="n">fw_metadata_updated</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;ref_metadata=</span><span class="si">{</span><span class="n">pprint</span><span class="o">.</span><span class="n">pformat</span><span class="p">(</span><span class="n">partial_asdict</span><span class="p">(</span><span class="n">ref_fw_metadata</span><span class="p">))</span><span class="si">}</span><span class="s1">, &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;actual_metadata=</span><span class="si">{</span><span class="n">pprint</span><span class="o">.</span><span class="n">pformat</span><span class="p">(</span><span class="n">partial_asdict</span><span class="p">(</span><span class="n">fw_metadata_updated</span><span class="p">))</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">wrapped_flat_fn</span><span class="p">,</span> <span class="n">flat_args_with_synthetic_bases</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata_updated</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">args_with_synthetic_bases</span><span class="p">,</span> <span class="n">synthetic_base_info</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">,</span> <span class="n">is_inference</span><span class="o">=</span><span class="n">is_inference</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">aliased_args_w_metadata_mutations</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_arg_idx_with_metadata_mutations</span><span class="p">]</span>
        <span class="n">args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">args_with_synthetic_bases</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_aliased_args_with_metadata_mutations</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># This code does not handle **all** input metadata mutations.</span>
            <span class="c1"># Instead, it only handles metadata mutations on inputs that were converted into synthetic bases</span>
            <span class="c1"># (which only happens if at least one aliased input experienced a data mutation).</span>
            <span class="c1"># e.g:</span>
            <span class="c1"># def f(a, b):</span>
            <span class="c1">#     a.mul_(2)</span>
            <span class="c1">#     b.t_(1, 0)</span>
            <span class="c1"># f(x.view(2, 2), x.view(2, 2))</span>
            <span class="n">mutated_metadata_inps</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="o">-</span><span class="n">num_aliased_args_with_metadata_mutations</span><span class="p">:]</span>
            <span class="n">user_outs</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[:</span><span class="o">-</span><span class="n">num_aliased_args_with_metadata_mutations</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">inp</span><span class="p">,</span> <span class="n">mutated_inp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aliased_args_w_metadata_mutations</span><span class="p">,</span> <span class="n">mutated_metadata_inps</span><span class="p">):</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span><span class="n">mutated_inp</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">mutated_inp</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">mutated_inp</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">user_outs</span>
        <span class="k">return</span> <span class="n">outs</span>

    <span class="k">return</span> <span class="n">wrapped_compiled_fn</span>


<span class="k">def</span> <span class="nf">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;parameter/buffer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;input </span><span class="si">{</span><span class="n">i</span> <span class="o">-</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># The wrapper created by this function handles all of the runtime aliasing and mutation &quot;epilogue&quot; logic</span>
<span class="c1"># that needs to run after the compiled function.</span>
<span class="c1">#</span>
<span class="c1"># This function accepts a trace_joint flag, indicating whether or not we&#39;re generating the runtime</span>
<span class="c1"># epilogue for a forward-only inference graph, or for an autograd.Function.apply function.</span>
<span class="c1"># This is because there are some minor differences in how we treat these cases at runtime:</span>
<span class="c1"># - resize_() is currently handled in the inference case, but not fully handled in the autograd case.</span>
<span class="c1"># - the autograd cases inserts TensorAlias wrapper objects for outputs that alias inputs</span>
<span class="k">def</span> <span class="nf">create_runtime_wrapper</span><span class="p">(</span>
    <span class="n">compiled_fn</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">runtime_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">indices_of_inps_to_detach</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">trace_joint</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">keep_input_mutations</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">disable_amp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">runtime_wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
            <span class="n">args_</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="c1"># See Note [Detaching inputs that never need gradients]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices_of_inps_to_detach</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args_</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">args_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">args_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">_force_original_view_tracking</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
                <span class="n">all_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                    <span class="n">compiled_fn</span><span class="p">,</span>
                    <span class="n">args_</span><span class="p">,</span>
                    <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">all_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">compiled_fn</span><span class="p">,</span>
                <span class="n">args</span><span class="p">,</span>
                <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
        <span class="n">num_metadata_mutated_inps</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_inputs</span>
        <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>

        <span class="k">if</span> <span class="n">keep_input_mutations</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
                <span class="o">==</span> <span class="n">num_metadata_mutated_inps</span> <span class="o">+</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_metadata_mutated_inps</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
                <span class="o">==</span> <span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_mutated_inps</span>
            <span class="p">)</span>
        <span class="c1"># Step 3: After running the compiled fw, apply updates to mutated inputs</span>
        <span class="n">num_mutations_to_apply</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_mutations_to_apply</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">updated_inputs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[:</span> <span class="n">num_mutations_to_apply</span><span class="p">]</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[</span><span class="n">num_mutations_to_apply</span> <span class="p">:]</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inpt_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">mutated_inp_runtime_indices</span>
            <span class="p">):</span>
                <span class="n">meta</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">original_inpt</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span>
                <span class="n">updated_inpt</span> <span class="o">=</span> <span class="n">updated_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="c1"># TODO: add better resize_() support for autograd case.</span>
                <span class="c1"># Check for the case when an input has been resized.</span>
                <span class="c1"># Note: One important thing to check for is user code that calls inpt.storage().resize_().</span>
                <span class="c1"># We can&#39;t trace operations on storage into the graph, so we should get dynamo to graph break.</span>
                <span class="c1"># TODO: handle resize_() on inputs to a larger size.</span>
                <span class="c1"># This is actually non-trivial to detect, so we should probably just handle it</span>
                <span class="c1"># (or make dynamo detect).</span>
                <span class="c1"># We can&#39;t just check of original_inpt.storage_size != updated_inpt.storage_size,</span>
                <span class="c1"># Because the original_inpt might be a view of some larger tensor,</span>
                <span class="c1"># and updated_inpt is always densely packed.</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">trace_joint</span> <span class="ow">and</span> <span class="n">original_inpt</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span>
                        <span class="n">updated_inpt</span> <span class="o">=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">alias</span>
                    <span class="c1"># We need to grab the size/stride/storage_offset from the compiled forward,</span>
                    <span class="c1"># and use that to mutate the metadata of the input</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_metadata</span><span class="p">:</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutates_data</span>
                    <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">is_leaf</span> <span class="ow">and</span> <span class="n">original_inpt</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                        <span class="c1"># We can hit this situation in this case:</span>
                        <span class="c1">#   def f(x):</span>
                        <span class="c1">#       x.detach().mul_(2)</span>
                        <span class="c1">#       return x + 1</span>
                        <span class="c1"># AOTAutograd will see a mutation in the above case, and try to</span>
                        <span class="c1"># apply a copy_() here, in the epilogue.</span>
                        <span class="c1"># But if x required gradients, and is a leaf, then autograd</span>
                        <span class="c1"># will yell at us for trying to mutate it.</span>
                        <span class="c1"># However, it&#39;s only possible to end up in this scenario (like the above)</span>
                        <span class="c1"># if all of the mutations to the leaf input were non-autograd-tracking mutations</span>
                        <span class="c1"># (aka mutations under no_grad(), or on detached views).</span>
                        <span class="c1"># In that case, we fully want to hide the mutation from autograd, so detaching is ok.</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">all_outs</span>

        <span class="c1"># Step 4: Manually regenerate any outputs that are aliased to inputs, instead of</span>
        <span class="c1"># compiling them.</span>
        <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># The compiled forward also returned intermediate bases. We don&#39;t want to return them to the user.</span>
            <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">fw_outs_no_intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span>
                    <span class="p">:</span> <span class="o">-</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
                <span class="p">]</span>
                <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="o">-</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span><span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fw_outs_no_intermediate_bases</span> <span class="o">=</span> <span class="n">fw_outs</span>
                <span class="n">intermediate_bases</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs_no_intermediate_bases</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">)</span>

            <span class="n">fw_outs_including_aliases</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
                <span class="n">fw_outs_no_intermediate_bases</span><span class="p">,</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">output_info</span>
            <span class="p">)):</span>
                <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span> <span class="ow">or</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">:</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span>
                    <span class="n">o_</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">alias</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">o_</span> <span class="o">=</span> <span class="n">o</span>
                <span class="n">o_grad</span> <span class="o">=</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">runtime_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">:</span>
                    <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">o_grad</span><span class="p">)</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">is_input</span><span class="p">:</span>
                    <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span><span class="p">:</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">intermediate_bases</span>
                <span class="k">elif</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_save_as_output</span><span class="p">:</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">intermediate_bases</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate_base_is_user_output</span>
                    <span class="n">base_tensor_list</span> <span class="o">=</span> <span class="n">fw_outs_no_intermediate_bases</span>
                <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">base_tensor_list</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                <span class="c1"># TODO: handle the custom autograd function case here.</span>
                <span class="c1"># We need a way to check whether a tensor came from a custom autograd fn from python,</span>
                <span class="c1"># AND a way to replay that custom view fn.</span>
                <span class="n">regenerated_out</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">o_grad</span><span class="p">)</span>
                <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regenerated_out</span><span class="p">)</span>
            <span class="n">ret_outs</span> <span class="o">=</span> <span class="n">fw_outs_including_aliases</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret_outs</span> <span class="o">=</span> <span class="n">fw_outs</span>

        <span class="k">if</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">dynamic_outputs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ret_outs</span><span class="p">,</span> <span class="n">runtime_metadata</span><span class="o">.</span><span class="n">output_info</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;_dynamo_weak_dynamic_indices&#39;</span><span class="p">):</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">_dynamo_weak_dynamic_indices</span> <span class="o">|=</span> <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">_dynamo_weak_dynamic_indices</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">dynamic_dims</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ret_outs</span>
    <span class="k">return</span> <span class="n">runtime_wrapper</span>

<span class="c1"># Calling convention: If we are running functionalized RNG, then outs consists</span>
<span class="c1"># of (user_outs, rng_offset)</span>
<span class="k">def</span> <span class="nf">functionalized_rng_runtime_epilogue</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">outs</span><span class="p">,</span> <span class="n">return_new_outs</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">new_rng_offset</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">set_new_offset</span><span class="p">(</span><span class="n">new_rng_offset</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_new_outs</span><span class="p">:</span>
            <span class="n">user_outs</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">user_outs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">outs</span>


<span class="k">def</span> <span class="nf">create_functionalized_rng_ops_wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># Functionalization of rng ops changes the calling convention of the joint graph.</span>
    <span class="c1"># It goes from (primals, tangents) to (seed, offset, primals, tangents)</span>
    <span class="c1"># At runtime, we pass on the current seed and offset. This is hidden from</span>
    <span class="c1"># the user.</span>
    <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">nullcontext</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">override_get_rng_state</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_state_as_tensor</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">override_set_rng_state</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">set_state_from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">append_rng_offsets</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
            <span class="c1"># args signature before: Tuple(fwd_outputs), Tuple(bwd_outputs)</span>
            <span class="c1"># args signature after: Tuple(fwd_outputs, new_fwd_rng_offset), Tuple(bwd_offset, new_bwd_rng_offset)</span>
            <span class="k">return</span> <span class="p">((</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_updated_fwd_offset</span><span class="p">()),</span>
                    <span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_updated_bwd_offset</span><span class="p">()))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># args signature before: Tuple(fwd_outputs)</span>
            <span class="c1"># args signature after: Tuple(fwd_outputs, new_fwd_rng_offset)</span>
            <span class="k">return</span> <span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">get_updated_fwd_offset</span><span class="p">())</span>


    <span class="k">def</span> <span class="nf">traced_joint</span><span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.get_rng_state&quot;</span><span class="p">,</span> <span class="n">override_get_rng_state</span><span class="p">),</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.set_rng_state&quot;</span><span class="p">,</span> <span class="n">override_set_rng_state</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">append_rng_offsets</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">traced_forward</span><span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.get_rng_state&quot;</span><span class="p">,</span> <span class="n">override_get_rng_state</span><span class="p">),</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.set_rng_state&quot;</span><span class="p">,</span> <span class="n">override_set_rng_state</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">append_rng_offsets</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">primals</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">trace_joint</span><span class="p">:</span>
        <span class="c1"># Get the current seed and offset to setup tracing.</span>
        <span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
        <span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">record_state</span><span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">)</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">record_state</span><span class="p">(</span><span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">traced_joint</span><span class="p">,</span> <span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="n">bwd_seed</span><span class="p">,</span> <span class="n">bwd_base_offset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Get the current seed and offset to setup tracing.</span>
        <span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
        <span class="n">PhiloxStateTracker</span><span class="o">.</span><span class="n">record_state</span><span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">traced_forward</span><span class="p">,</span> <span class="p">(</span><span class="n">fwd_seed</span><span class="p">,</span> <span class="n">fwd_base_offset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

<span class="c1"># Has the precondition that there</span>
<span class="c1"># are no duplicate arguments in flat_args (e.g., the same Tensor</span>
<span class="c1"># object never shows up twice.  However, two tensor inputs MAY alias</span>
<span class="c1"># the same storage, so long as they have separate TensorImpls.)</span>
<span class="k">def</span> <span class="nf">aot_dispatch_autograd</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">):</span>
    <span class="c1"># traced_tangents corresponds to the set of outputs in the traced forward that should get grad_outputs in the traced backward.</span>
    <span class="c1"># It includes outputs of the original forward, *and* any updated inputs due to input mutations.</span>
    <span class="c1"># However, it does *not* include any outputs that are aliases of inputs or intermediates, or any metadata-only input mutations.</span>
    <span class="n">traced_tangents</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="o">.</span><span class="n">traced_tangents</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">)</span> <span class="o">==</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_outputs</span>
    <span class="n">joint_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">traced_tangents</span><span class="p">)</span>
    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>

    <span class="n">fn_prepared_for_autograd</span> <span class="o">=</span> <span class="n">fn_prepped_for_autograd</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">fw_metadata</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">joint_fn_to_trace</span> <span class="o">=</span> <span class="n">create_joint</span><span class="p">(</span><span class="n">fn_prepared_for_autograd</span><span class="p">)</span>

    <span class="n">fx_g</span> <span class="o">=</span> <span class="n">create_functionalized_graph</span><span class="p">(</span>
        <span class="n">joint_fn_to_trace</span><span class="p">,</span>
        <span class="n">joint_inputs</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># There should be *NO* mutating ops in the graph at this point.</span>
    <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>

    <span class="c1"># Redudant with the check above, but worth having in case tracing introduced</span>
    <span class="c1"># a fake tensor. Unlikely.</span>
    <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>
    <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
    <span class="n">fx_g</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">enable_log</span><span class="p">:</span>
        <span class="n">aot_joint_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Joint graph&quot;</span><span class="p">,</span> <span class="n">fx_g</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;joint&quot;</span><span class="p">):</span>
            <span class="n">num_inner_fwd_outputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
                <span class="o">+</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_outputs</span>
                <span class="o">+</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
                <span class="o">+</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span>
            <span class="p">)</span>
            <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">partition_fn</span><span class="p">(</span>
                <span class="n">fx_g</span><span class="p">,</span> <span class="n">joint_inputs</span><span class="p">,</span> <span class="n">num_fwd_outputs</span><span class="o">=</span><span class="n">num_inner_fwd_outputs</span>
            <span class="p">)</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># we only need to bookkeep the symints that are saved for bw, not any symints</span>
            <span class="c1"># the user forward might have returned in its own output</span>
            <span class="n">fw_outs_saved_for_bw</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_inner_fwd_outputs</span><span class="p">:]</span>
            <span class="n">symint_outs_saved_for_bw</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_outs_saved_for_bw</span> <span class="k">if</span> <span class="n">is_sym_node</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">_num_symints_saved_for_bw</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">)</span>

        <span class="c1"># Note [Detaching inputs that never need gradients]</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/97745</span>
        <span class="c1"># Suppose we have a function like this that we want to compile:</span>
        <span class="c1">#</span>
        <span class="c1"># def f(x, y):</span>
        <span class="c1">#     return torch.mul(x, y.detach())</span>
        <span class="c1">#</span>
        <span class="c1"># What gradients should we compute for x and y?</span>
        <span class="c1"># By default, AOTAutograd will compute a gradient for **every** input that requires gradients,</span>
        <span class="c1"># and so we&#39;ll compute:</span>
        <span class="c1">#    x_grad_input = y</span>
        <span class="c1">#    y_grad_input = None</span>
        <span class="c1"># Does this preserve the semantics of eager mode?</span>
        <span class="c1"># Unfortunately, no.</span>
        <span class="c1"># Doing the above will cause autograd to **continue** to backprop the autograd tape</span>
        <span class="c1"># that was generated from constructing y.</span>
        <span class="c1">#</span>
        <span class="c1"># This is **different** from what would have happened in eager mode.</span>
        <span class="c1"># In eager mode, if we backprop through the output of this function, autograd will only traverse</span>
        <span class="c1"># the bit of the autograd tape corresponding to &quot;x&quot;.</span>
        <span class="c1"># In particular, if a user had previously backpropped through y&#39;s autograd tape,</span>
        <span class="c1"># And then they try to backprop through the output of the above function,</span>
        <span class="c1"># then we&#39;ll hit the dreaded &quot;Trying to backward through the graph a second time&quot; error.</span>
        <span class="c1">#</span>
        <span class="c1"># You might think: If autograd sees that a gradient is None, shouldn&#39;t it stop early,</span>
        <span class="c1"># instead of continuing the backprop through the ancestors of that node in the graph?</span>
        <span class="c1">#</span>
        <span class="c1"># Autograd has two passes:</span>
        <span class="c1"># (1) a first pass that traverses the autograd graph and figures out which nodes need to be executed</span>
        <span class="c1"># (2) a second pass that actually goes ahead and executes each node when it becomes ready,</span>
        <span class="c1">#     propagating gradients</span>
        <span class="c1"># By the time we&#39;re executing a node and we see that it produces a None, the set of nodes to execute</span>
        <span class="c1"># is already locked-in.</span>
        <span class="c1">#</span>
        <span class="c1"># The fix: instead, we can recognize statically that the graph we&#39;re compiling will never contribute</span>
        <span class="c1"># gradients to y, and prevent autograd from trying to traverse y&#39;s autograd tape at all.</span>
        <span class="c1"># We can do this by manually detach&#39;ing y before sending it through the `CompiledFunction`.</span>
        <span class="c1">#</span>
        <span class="c1"># Note that this solution is not bulletproof.</span>
        <span class="c1"># It&#39;s possible to construct a case where eager may or may not have have tried to autograd through y,</span>
        <span class="c1"># depending on the actual grad_outputs that were passed in during the backward.</span>
        <span class="c1"># There is no easy fix for this: the simplest fix would be to run with `retain_graph=True`,</span>
        <span class="c1"># allowing autograd to re-use the graph.</span>
        <span class="c1">#</span>
        <span class="c1"># An example of this case is:</span>
        <span class="c1"># def f(x):</span>
        <span class="c1">#     return x.detach() * 2, x * 3</span>
        <span class="c1"># If we were to only backprop through outs[0], in eager, we would stop</span>
        <span class="c1"># If we backward only on the first output, we shouldn&#39;t send a grad through x.</span>
        <span class="c1"># But the custom autograd function doesn&#39;t know that: it will materialize zero grads for x * 3</span>
        <span class="c1"># and we will end up with a zero grad at x.</span>
        <span class="c1"># If we later backprop through the second output, this will also require backprop&#39;ing through x.</span>
        <span class="c1"># Meaning we&#39;ll need to use `retain_graph=True` to be able to backprop through x the second time.</span>
        <span class="n">_indices_of_inps_to_detach</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">bw_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">bw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">bw_outs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">+</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">bw_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bw_outs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">bw_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_indices_of_inps_to_detach</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">enable_log</span><span class="p">:</span>
            <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Forward graph&quot;</span><span class="p">,</span> <span class="n">fw_module</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>
            <span class="n">aot_graphs_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">lazy_format_graph_code</span><span class="p">(</span><span class="s2">&quot;Backward graph&quot;</span><span class="p">,</span> <span class="n">bw_module</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">):</span>
            <span class="n">adjusted_flat_args</span> <span class="o">=</span> <span class="n">flat_args</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
                <span class="c1"># Update example inputs for the fw_compiler</span>
                <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">()</span>
                <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">)</span>
                <span class="n">adjusted_flat_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="o">*</span><span class="n">flat_args</span><span class="p">]</span>
                <span class="c1"># We are not clearing flat_args here because</span>
                <span class="c1"># 1) There is a check in the the debug compiler at the end</span>
                <span class="c1"># 2) It does not matter as these are fake tensors</span>
            <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span>
                <span class="n">fw_module</span><span class="p">,</span> <span class="n">adjusted_flat_args</span>
            <span class="p">)</span>


    <span class="n">saved_context</span> <span class="o">=</span> <span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">CompiledFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">compiled_fw_func</span>
        <span class="n">compiled_bw</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">metadata</span> <span class="o">=</span> <span class="n">fw_metadata</span>
        <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">_num_symints_saved_for_bw</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">deduped_flat_tensor_args</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="n">deduped_flat_tensor_args</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
                <span class="c1"># Add the seed and offset to args</span>
                <span class="n">seed</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">()</span>
                <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="c1"># There is a pretty complicated calling convention around what the compiled fw returns.</span>
            <span class="c1"># The full list of outputs and their relative order is:</span>
            <span class="c1"># (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)</span>
            <span class="c1"># - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version</span>
            <span class="c1">#   of the original view, and not the synthetic base</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_fw</span><span class="p">,</span>
                <span class="n">args</span><span class="p">,</span>
                <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span>
            <span class="n">num_outputs_aliased_to_inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span>
            <span class="p">)</span>
            <span class="n">num_outputs_aliased_to_intermediates</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span>
            <span class="p">)</span>
            <span class="n">num_outputs_aliased</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span>
            <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
            <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span>
            <span class="n">num_mutated_inputs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
            <span class="n">num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span>
            <span class="p">)</span>
            <span class="n">num_outputs_rng_offset</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_rng_offset</span>
            <span class="c1"># Our forward() returns both (mutated_inputs, outputs, output_intermediate_bases, saved_tensors, saved_symints)</span>
            <span class="n">num_forward_returns</span> <span class="o">=</span> <span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="c1"># In case of functionalization of rng ops, the fw_module returns one</span>
            <span class="c1"># additinal output for rng offset. This rng offset is used right</span>
            <span class="c1"># away to advance the rng state, and is not passed on to the raw</span>
            <span class="c1"># outputs. However, we need to know the exact boundary to identify</span>
            <span class="c1"># which tensors to be saved for the bwd graph.  num_forward captures</span>
            <span class="c1"># this information.</span>
            <span class="n">num_forward</span> <span class="o">=</span> <span class="n">num_forward_returns</span> <span class="o">+</span> <span class="n">num_outputs_rng_offset</span>

            <span class="k">assert</span> <span class="n">num_forward_returns</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">requires_grad_info</span>
            <span class="p">)</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>

            <span class="c1"># Partitioners must put symint arguments at the end separate from tensor arguments</span>
            <span class="k">if</span> <span class="n">num_symints_saved_for_bw</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">tensors_saved_for_backwards</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span>
                    <span class="n">num_forward</span><span class="p">:</span><span class="o">-</span><span class="n">num_symints_saved_for_bw</span>
                <span class="p">]</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_saved_for_backwards</span>
                <span class="p">)</span>
                <span class="c1"># See Note [Detaching saved tensors in AOTAutograd]</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">_is_view</span><span class="p">()</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_saved_for_backwards</span><span class="p">))</span>
                <span class="n">symint_outs</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="o">-</span><span class="n">num_symints_saved_for_bw</span><span class="p">:]</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symint_outs</span>
                <span class="p">)</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="n">symint_outs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tensors_saved_for_backwards</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_forward</span><span class="p">:]</span>
                <span class="c1"># See Note [Detaching saved tensors in AOTAutograd]</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">_is_view</span><span class="p">()</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_saved_for_backwards</span><span class="p">))</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">raw_returns</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_forward_returns</span><span class="p">]</span>

            <span class="c1"># Wrap all autograd.Function.forward() outputs that are aliases</span>
            <span class="c1"># so that autograd.Function doesn&#39;t treat them as tensors</span>
            <span class="k">if</span> <span class="n">num_mutated_metadata_only_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
                <span class="p">):</span>
                    <span class="c1"># We could make this faster by only looping over inputs with metadata-only mutations</span>
                    <span class="c1"># (instead of looping over inputs with either data or metadata mutations), but there shouldn&#39;t be many.</span>
                    <span class="n">info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">info</span><span class="o">.</span><span class="n">mutates_data</span><span class="p">:</span>
                        <span class="n">raw_returns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorAlias</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
                    <span class="n">user_mutated_inputs_raw</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_mutated_inputs</span><span class="p">]</span>
                    <span class="n">mut_inp_infos</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_data</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">mutates_metadata</span>
                    <span class="p">]</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_mutated_inputs_raw</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mut_inp_infos</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">aliased_out_indices</span><span class="p">:</span>
                    <span class="n">raw_return_idx</span> <span class="o">=</span> <span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">idx</span>
                    <span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorAlias</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">[</span><span class="n">raw_return_idx</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
                    <span class="n">intermediates_raw</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span><span class="p">:]</span>
                    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">TensorAlias</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">intermediates_raw</span><span class="p">)</span>

            <span class="c1"># invariant: intermediate bases always require gradients, so we don&#39;t have to</span>
            <span class="c1"># consider marking them as non-differentiable.</span>
            <span class="n">raw_returns_not_including_intermediate_bases</span> <span class="o">=</span> <span class="n">raw_returns</span><span class="p">[:</span><span class="n">num_mutated_inputs</span> <span class="o">+</span> <span class="n">num_outputs</span><span class="p">]</span>
            <span class="n">fw_outs_not_requiring_grad</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw_returns_not_including_intermediate_bases</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">requires_grad_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs_not_requiring_grad</span><span class="p">)</span>

            <span class="n">functionalized_rng_runtime_epilogue</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_forward_returns</span><span class="p">:</span><span class="n">num_forward</span><span class="p">],</span>
                <span class="n">return_new_outs</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">raw_returns</span><span class="p">)</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="c1"># Calling convention: we expect a grad_out passed to the backward:</span>
            <span class="c1"># - for every output of the fw that does *not* alias an input or graph intermediate</span>
            <span class="c1"># - for every updated_input generated by the fw that does *not* alias an input (aka only data-mutations)</span>
            <span class="c1"># - for every graph intermediate that we need to use to generate an output later.</span>
            <span class="c1"># The other outputs in the autograd.Function.forward that do *not* show up in the backward include:</span>
            <span class="c1"># - outputs that alias inputs or graph intermediates</span>
            <span class="c1"># - updated inputs due to metadata-only mutations.</span>
            <span class="c1"># We need to return them in the forward, but ensure that they all do not get gradients in the backward,</span>
            <span class="c1"># and we filter them out here before passing the remaining grad_outputs into the compiled backward.</span>
            <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
            <span class="n">num_intermediate_bases</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_intermediate_bases</span>
            <span class="n">expected_grad_outs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">+</span> <span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">num_intermediate_bases</span>
            <span class="p">)</span>

            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected_grad_outs</span>
            <span class="n">out_info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">output_info</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_metadata_only_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">or</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs_aliased</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">inp_tangents</span><span class="p">,</span> <span class="n">out_tangents</span><span class="p">,</span> <span class="n">intermediate_base_tangents</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">flat_args</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_mutated_inps</span><span class="p">],</span>
                    <span class="n">flat_args</span><span class="p">[</span><span class="n">num_mutated_inps</span><span class="p">:</span><span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">],</span>
                    <span class="n">flat_args</span><span class="p">[</span><span class="n">num_mutated_inps</span> <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">:],</span>
                <span class="p">)</span>
                <span class="c1"># input_info contains info on *every* input,</span>
                <span class="c1"># But in the backward(), we are only given grad outputs for every mutated input.</span>
                <span class="c1"># We then need to filter out the grad outputs that correspond to metadata-only mutations.</span>
                <span class="n">mutated_inp_indices</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">mutated_inp_indices</span>
                <span class="n">input_info</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">input_info</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inp_tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_inp_indices</span><span class="p">)</span>
                <span class="n">inp_tangents_filtered</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">x</span>
                    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">info_idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inp_tangents</span><span class="p">,</span> <span class="n">mutated_inp_indices</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">input_info</span><span class="p">[</span><span class="n">info_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mutates_data</span>
                <span class="p">]</span>
                <span class="c1"># We also need to filter out grad outputs that correspond to outputs aliasing inputs/intermediates</span>
                <span class="n">out_tangents_filtered</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">x</span>
                    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out_tangents</span><span class="p">,</span> <span class="n">out_info</span><span class="p">)</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span> <span class="ow">or</span> <span class="n">info</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">unsafe_view_alias</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="p">]</span>
                <span class="c1"># intermediate bases always require gradients, and always participate in the backward graph.</span>
                <span class="n">flat_bw_args</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">inp_tangents_filtered</span><span class="p">,</span> <span class="n">out_tangents_filtered</span><span class="p">,</span> <span class="n">intermediate_base_tangents</span><span class="p">)</span>

                <span class="c1"># sanity asserts</span>
                <span class="c1"># metadata_only_inps = [</span>
                <span class="c1">#     x for x, info_idx in zip(inp_tangents, mutated_inp_indices)</span>
                <span class="c1">#     if not input_info[info_idx].mutates_data</span>
                <span class="c1"># ]</span>
                <span class="c1"># aliased_outputs = [</span>
                <span class="c1">#     x for x, info in zip(out_tangents, out_info) if info.output_type != OutputType.non_alias]</span>
                <span class="c1"># assert all(x is None for x in metadata_only_inps)</span>
                <span class="c1"># assert all(x is None for x in aliased_outputs)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># filter out non-tensor grad_outputs (aka due to ints being returned as outputs in the forward)</span>
                <span class="n">num_mutated_inps</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">num_mutated_inputs</span>
                <span class="n">mutated_inp_args</span> <span class="o">=</span> <span class="n">flat_args</span><span class="p">[:</span><span class="n">num_mutated_inps</span><span class="p">]</span> <span class="k">if</span> <span class="n">num_mutated_inps</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">[]</span>
                <span class="n">user_tangents</span> <span class="o">=</span> <span class="n">flat_args</span><span class="p">[</span><span class="n">num_mutated_inps</span><span class="p">:]</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">user_tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_info</span><span class="p">)</span>
                <span class="n">filtered_user_tangents</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">user_tangents</span><span class="p">,</span> <span class="n">out_info</span><span class="p">)</span> <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">raw_type</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)]</span>
                <span class="n">flat_bw_args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">mutated_inp_args</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">filtered_user_tangents</span><span class="p">)</span>

            <span class="n">contiguous_args</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">t</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">flat_bw_args</span>
            <span class="p">]</span>

            <span class="n">rng_args</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">is_rng_op_functionalized</span><span class="p">:</span>
                <span class="c1"># Add the seed and offset to args</span>
                <span class="n">rng_args</span> <span class="o">=</span> <span class="n">CUDARngStateHelper</span><span class="o">.</span><span class="n">get_torch_state_as_tuple</span><span class="p">()</span>

            <span class="n">all_args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="n">rng_args</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">symints</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">contiguous_args</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="k">del</span> <span class="n">contiguous_args</span>

            <span class="k">def</span> <span class="nf">call_compiled_backward</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">all_args</span><span class="p">)</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
                    <span class="k">with</span> <span class="n">tracing</span><span class="p">(</span><span class="n">saved_context</span><span class="p">),</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">):</span>
                        <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">bw_compiler</span><span class="p">(</span>
                            <span class="n">bw_module</span><span class="p">,</span> <span class="n">fx_placeholder_vals</span><span class="p">(</span><span class="n">bw_module</span><span class="p">)</span>
                        <span class="p">)</span>

                <span class="n">ctx</span><span class="o">.</span><span class="n">maybe_clear_saved_tensors</span><span class="p">()</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span><span class="p">,</span>
                    <span class="n">all_args</span><span class="p">,</span>
                    <span class="n">steal_args</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">out</span> <span class="o">=</span> <span class="n">functionalized_rng_runtime_epilogue</span><span class="p">(</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">all_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
                <span class="c1"># Ensure that the graph is connected, and error if double backward is performed.</span>
                <span class="c1"># See comment for why once_differentiable is not sufficient:</span>
                <span class="c1"># https://github.com/pytorch/pytorch/pull/92348/files#r1072962107</span>
                <span class="k">class</span> <span class="nc">CompiledFunctionBackward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
                    <span class="nd">@staticmethod</span>
                    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">unused_args</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">call_compiled_backward</span><span class="p">()</span>

                    <span class="nd">@staticmethod</span>
                    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;torch.compile with aot_autograd does not currently support double backward&quot;</span><span class="p">)</span>
                <span class="c1"># Pass args even though they&#39;re unused, so that the graph is built</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">CompiledFunctionBackward</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="n">all_args</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">call_compiled_backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">out</span>

    <span class="n">compiled_function</span> <span class="o">=</span> <span class="n">create_runtime_wrapper</span><span class="p">(</span>
        <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span>
        <span class="n">runtime_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
        <span class="n">indices_of_inps_to_detach</span><span class="o">=</span><span class="n">_indices_of_inps_to_detach</span><span class="p">,</span>
        <span class="n">trace_joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">keep_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiled_function</span>

    <span class="n">flat_requires_grad</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">flat_args</span>
    <span class="p">]</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_function</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debug_compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># TODO: Check aliasing relationships</span>
        <span class="c1"># TODO: Check strides for metadata mutation</span>
        <span class="c1"># (NB: ideally, this logic is factored out of this function and</span>
        <span class="c1"># you move these debug checks there)</span>

        <span class="c1"># Check requires grad.  Bad case is when we compiled with</span>
        <span class="c1"># requires_grad = False, but input requires_grad = True</span>
        <span class="c1"># (vice versa is OK; we compute a gradient and then throw</span>
        <span class="c1"># it away when it hits the input.)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">can_require_grad</span> <span class="o">=</span> <span class="n">flat_requires_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">can_require_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">can_require_grad</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                    <span class="n">aot_config</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would not require grad&quot;</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">debug_compiled_function</span>


<span class="nd">@dynamo_timed</span>
<span class="k">def</span> <span class="nf">create_aot_dispatcher_function</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graphs of the attr:`flat_fn` to generate a</span>
<span class="sd">    joint graph. The joint graph is an Fx graph with Aten ops. Please refer to</span>
<span class="sd">    the tracing mechanism to understand the graph capturing details.</span>

<span class="sd">    The joint graph is then passed through attr:`partition_fn` to isolate the</span>
<span class="sd">    forward and backward portions, which are then respectively compiled via the</span>
<span class="sd">    provided attr:`fw_compiler` and attr:`bw_compiler`.</span>

<span class="sd">    The resulting compiled forward and backward graphs are then wrapped up in a</span>
<span class="sd">    ``torch.autograd.Function`` object.</span>

<span class="sd">    The calling convention here is that the first aot_config.num_params_buffers</span>
<span class="sd">    inputs in flat_args are parameters and buffers, and the rest are inputs.</span>

<span class="sd">    We use this to assume that parameters/buffer&#39;s shapes don&#39;t change.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This is the main entry point.</span>
    <span class="c1"># TODO: Chillee argues that dynamo itself should pass in fake tensors to</span>
    <span class="c1"># the list of arguments when compiling; at the moment we do not do this</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{}</span>


    <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">aot_autograd_decompositions</span><span class="p">,</span>
        <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">functionalize_rng_ops</span><span class="p">:</span>
        <span class="c1"># Update the decompositions with functionalized random decompositions</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="o">**</span><span class="n">rng_decompositions</span><span class="p">,</span>
            <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="c1"># Check flat_args to see if they&#39;re already fake.  If so, use that fake</span>
    <span class="c1"># mode instead.</span>

    <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">ShapeEnv</span><span class="p">()</span> <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">dynamic_shapes</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">shape_env</span><span class="o">=</span><span class="n">shape_env</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span>

    <span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">enable_python_dispatcher</span><span class="p">()</span> <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_multithreading_enabled</span><span class="p">(</span>
        <span class="kc">False</span>
    <span class="p">),</span> <span class="n">preserve_rng_state</span><span class="p">(),</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">python_dispatcher_mode</span><span class="p">,</span> <span class="n">PhiloxStateTracker</span><span class="p">():</span>

        <span class="k">def</span> <span class="nf">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="kn">from</span> <span class="nn">torch._dynamo.source</span> <span class="kn">import</span> <span class="n">ConstantSource</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">shape_env</span><span class="o">.</span><span class="n">create_symintnode</span><span class="p">(</span>
                            <span class="n">shape_env</span><span class="o">.</span><span class="n">create_symbol</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ConstantSource</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sym_</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)),</span>
                            <span class="n">hint</span><span class="o">=</span><span class="n">x</span>
                        <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">x</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span> <span class="ow">is</span> <span class="n">fake_mode</span>
                    <span class="k">return</span> <span class="n">x</span>
                <span class="c1"># TODO: Ensure that this codepath is never exercised from</span>
                <span class="c1"># Dynamo</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span>
                    <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">static_weight_shapes</span>
                <span class="p">):</span>
                    <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">return</span> <span class="p">[</span><span class="n">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)]</span>

        <span class="n">fake_flat_args</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

        <span class="n">needs_autograd</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">any</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fake_flat_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="c1"># Patch set_rng_state as set_rng_state with fake tensors is</span>
            <span class="c1"># nonsensical. This does not affect the collection of metadata.</span>
            <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s2">&quot;torch.cuda.set_rng_state&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="kc">None</span><span class="p">):</span>
                <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
                    <span class="n">flat_fn</span><span class="p">,</span>
                    <span class="n">keep_input_mutations</span><span class="o">=</span><span class="n">aot_config</span><span class="o">.</span><span class="n">keep_inference_input_mutations</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">needs_autograd</span><span class="p">,</span>
                <span class="p">)(</span><span class="o">*</span><span class="n">fake_flat_args</span><span class="p">)</span>

        <span class="c1"># crappy version of dispatcher</span>
        <span class="c1"># TODO: Do this properly</span>
        <span class="k">if</span> <span class="n">needs_autograd</span><span class="p">:</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_autograd</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_base</span>

        <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">aot_wrapper_synthetic_base</span><span class="p">,</span> <span class="n">compiler_fn</span><span class="o">=</span><span class="n">compiler_fn</span><span class="p">,</span> <span class="n">needs_autograd</span><span class="o">=</span><span class="n">needs_autograd</span><span class="p">)</span>
        <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">aot_wrapper_dedupe</span><span class="p">,</span> <span class="n">compiler_fn</span><span class="o">=</span><span class="n">compiler_fn</span><span class="p">)</span>
        <span class="c1"># You can put more passes here</span>

        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_fn</span>


<span class="c1"># Inspired by autodidax (thanks!)</span>
<span class="k">class</span> <span class="nc">PytreeThunk</span><span class="p">:</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># These are some kinda dumb microoptimizations that save about 3-4 us of overhead.</span>
    <span class="n">is_simple</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>  <span class="c1"># if the output spec is a tuple/list, we won&#39;t bother unflattening it.</span>
    <span class="p">)</span>
    <span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># if the output spec is a LeafSpec</span>

    <span class="k">def</span> <span class="nf">set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spec</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">==</span> <span class="n">spec</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">=</span> <span class="n">spec</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">children_specs</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span>


<div class="viewcode-block" id="aot_function"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_function.html#functorch.compile.aot_function">[docs]</a><span class="k">def</span> <span class="nf">aot_function</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">keep_inference_input_mutations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Whether or not to trace with dynamic shapes</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">enable_log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`fn` using torch dispatch</span>
<span class="sd">    mechanism, and then compiles the generated forward and backward graphs</span>
<span class="sd">    through :attr:`fw_compiler` and :attr:`bw_compiler`.</span>

<span class="sd">    :func:`aot_function` traces the forward and backward graph ahead of time,</span>
<span class="sd">    and generates a joint forward and backward graph.  :attr:`partition_fn` is</span>
<span class="sd">    then used to separate out forward and backward graphs. The partitioner</span>
<span class="sd">    function can be used to perform optimizations such as recomputation. One can</span>
<span class="sd">    set `decompositions` dictionary to decompose the operators into a sequence</span>
<span class="sd">    of core or simpler operators supported by the backend compilers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Callable): A Python function that takes one ore more arguments. Must</span>
<span class="sd">            return one or more Tensors.</span>
<span class="sd">        fw_compiler (Callable): A Python function that accepts an Fx graph with</span>
<span class="sd">            Aten ops and input args, and returns a Callable that semantically is</span>
<span class="sd">            equivalent to the input Fx graph.</span>
<span class="sd">        bw_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph.  Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">        partition_fn (Callable): A Python function that takes a joint forward</span>
<span class="sd">            and backward graph, and partitions it into separate forward and</span>
<span class="sd">            backward graphs.</span>
<span class="sd">        decompositions (Dict): A dictionary to define the decomposition of</span>
<span class="sd">            larger Aten ops into simpler or core Aten ops.</span>
<span class="sd">        inference_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph. inference_compiler is invoked</span>
<span class="sd">            if no autograd is needed. Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``Callable`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`fn`, but with forward and backward graph compiled via</span>
<span class="sd">        :attr:`fw_compile` and :attr:`bw_compile`.</span>

<span class="sd">    A simple example usage of :func:`aot_function` is as follows. This example</span>
<span class="sd">    will print the forward and backward graphs of the function ``fn``</span>

<span class="sd">        &gt;&gt;&gt; fn = lambda x : x.sin().cos()</span>
<span class="sd">        &gt;&gt;&gt; def print_compile_fn(fx_module, args):</span>
<span class="sd">        &gt;&gt;&gt;     print(fx_module)</span>
<span class="sd">        &gt;&gt;&gt;     return fx_module</span>
<span class="sd">        &gt;&gt;&gt; aot_fn = aot_function(fn, print_compile_fn)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(4, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; aot_fn(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="k">if</span> <span class="n">inference_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inference_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_log</span><span class="o">=</span><span class="n">enable_log</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">cached_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">returned_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">cached_res</span>
        <span class="c1"># Now flatten the tensor args</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="c1"># Compile the function and save it in the cache</span>
        <span class="k">if</span> <span class="n">cached_res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Save the args_spec for flat_tensor_args to unflatten while tracing</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">tensor_args_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
            <span class="n">out_spec</span> <span class="o">=</span> <span class="n">PytreeThunk</span><span class="p">()</span>

            <span class="k">def</span> <span class="nf">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
                <span class="c1"># The input are flattened tensor args. Prepare the args in the</span>
                <span class="c1"># order that original function expects. Add static args as well.</span>
                <span class="c1"># They will appear as tensor constants in the traced graph.</span>
                <span class="k">nonlocal</span> <span class="n">out_spec</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">tensor_args_spec</span><span class="p">)</span>
                <span class="n">tree_out</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="n">flat_out</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">tree_out</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">flat_out</span><span class="p">:</span>
                    <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">KNOWN_TYPES</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
                            <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="k">break</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_known_type</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2"> in output, which is not a known type. &quot;</span>
                            <span class="s2">&quot;If this type holds tensors, you need to register a pytree for it. &quot;</span>
                            <span class="s2">&quot;See https://github.com/pytorch/functorch/issues/475 for a brief &quot;</span>
                            <span class="s2">&quot;explanation why. If you don&#39;t need to register a pytree, please &quot;</span>
                            <span class="s2">&quot;leave a comment explaining your use case and we&#39;ll make this more &quot;</span>
                            <span class="s2">&quot;ergonomic to deal with&quot;</span>
                        <span class="p">)</span>
                <span class="n">out_spec</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">flat_out</span>

            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                <span class="n">flat_fn</span><span class="p">,</span>
                <span class="n">flat_args</span><span class="p">,</span>
                <span class="n">aot_config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">cached_res</span> <span class="o">=</span> <span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="n">out_spec</span><span class="p">)</span>

        <span class="n">cached_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">cached_res</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">cached_fn</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">returned_function</span></div>


<div class="viewcode-block" id="aot_module"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_module.html#functorch.compile.aot_module">[docs]</a><span class="k">def</span> <span class="nf">aot_module</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`mod` using torch dispatch</span>
<span class="sd">    tracing mechanism. It is wrapper function, that underneath uses</span>
<span class="sd">    :func:`aot_function` to perform tracing and compilation.</span>

<span class="sd">    :func:`aot_module` lifts the parameters and buffers of ``nn.Module`` as inputs</span>
<span class="sd">    to a new callable which is then compiled through :func:`aot_function`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod (Callable): A ``nn.Module`` module.</span>
<span class="sd">        args : args to be passed to :func:`aot_function`</span>
<span class="sd">        kwargs : kwargs to be passed to :func:`aot_function`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``nn.Module`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`mod`, but with forward and backward graph compiled.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="n">named_params</span><span class="p">,</span> <span class="n">named_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">named_params</span><span class="p">,</span> <span class="o">**</span><span class="n">named_buffers</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_and_buffers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">named_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">num_params_buffers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_params</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">)</span>
    <span class="n">compiled_f</span> <span class="o">=</span> <span class="n">aot_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span> <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="k">class</span> <span class="nc">AOTModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">orig_module</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="n">named_params</span><span class="p">,</span>
                <span class="n">named_buffers</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">AOTModule</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">aot_module_simplified</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">inference_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the simplified or low overhead version of aot_module. For frontends</span>
<span class="sd">    like TorchDynamo, the input functions/modules to AOT are static and have</span>
<span class="sd">    unpacked inputs/outputs. This gives us an opportunity to remove the</span>
<span class="sd">        (1) pytree overhead to parse inputs/outputs,</span>
<span class="sd">        (2) AOT Autograd cache,</span>
<span class="sd">        (3) Reading of params/buffers in every forward call</span>

<span class="sd">    :func:`aot_module_simplified` removes these overheads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#########################################################</span>

    <span class="c1"># Redudant with dynamo, but worth having in case this gets invoked elsewhere.</span>

    <span class="c1"># Note [Fake Modules and AOTAutograd]</span>
    <span class="c1">#</span>
    <span class="c1"># A simple heuristic for when to use fake versus real tensors is that fake tensors are for compile time</span>
    <span class="c1"># (when we don&#39;t want to actually run the compute, but we do want to know about metadata),</span>
    <span class="c1"># and real tensors are for runtime (when we actually want to do the compute.) However, in AOTAutograd,</span>
    <span class="c1"># modules are the exception: we always pass AOTAutograd modules with real tensors.</span>
    <span class="c1"># This is because AOTAutograd will produce a compiled function which needs to directly access any</span>
    <span class="c1"># parameters the compiled function may need, but these parameters will NOT be passed in by the caller (aka Dynamo).</span>
    <span class="c1"># So at compile time, the compiled function we produce must close over any parameters, and those parameters must be</span>
    <span class="c1"># real parameters, and we cannot do this unless at compile time we get a module with real tensors.</span>

    <span class="c1"># Even if Dynamo did pass all parameters explicitly at runtime, which would eliminate the need to close over</span>
    <span class="c1"># the parameters, it would still be profitable to pass real tensor parameters to the compiler at compile time,</span>
    <span class="c1"># because some compilation strategies like CUDA graphs want to burn in the pointer addresses where the parameter data live,</span>
    <span class="c1"># and of course we can&#39;t do that unless we give the backend a real tensor.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="n">params_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">stateless</span><span class="o">.</span><span class="n">_reparametrize_module</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">args</span><span class="p">[:</span><span class="n">params_len</span><span class="p">],</span> <span class="n">params_spec</span><span class="p">)</span>
        <span class="p">):</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">(),</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
                        <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="s2">&quot;Anomaly Detection has been enabled.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">detect_anomaly</span><span class="p">(</span><span class="n">check_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">Interpreter</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Graph output must be a tuple(). This is so that we can avoid &quot;</span>
                <span class="s2">&quot;pytree processing of the ouputs. Please change the module to &quot;</span>
                <span class="s2">&quot;have tuple outputs or use aot_module instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="k">if</span> <span class="n">inference_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inference_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>

    <span class="n">seen_sources</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># First, the params</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Then, the params 1:1 mapped sources, if relevant.</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;_param_name_to_source&quot;</span><span class="p">):</span>
        <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># We now know this came from dynamo, and (1) we care about guards,</span>
        <span class="c1"># so setting up aot_autograd_arg_pos_to_source for downstream dedup guards</span>
        <span class="c1"># can now be done safely. (2) Dynamo logic protects the 1:1 sizing below.</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_param_name_to_source</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> not found.&quot;</span>
            <span class="n">source</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">_param_name_to_source</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">source</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_sources</span><span class="p">,</span> <span class="n">source</span>
            <span class="n">seen_sources</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
            <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

    <span class="c1"># Next, the input args</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">):</span>
        <span class="c1"># Non dynamo entrypoints can get to here...</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="s2">&quot;_dynamo_source&quot;</span><span class="p">):</span>
                    <span class="c1"># ... but not here!</span>
                    <span class="k">if</span> <span class="n">aot_autograd_arg_pos_to_source</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">aot_autograd_arg_pos_to_source</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">source</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">_dynamo_source</span>
                    <span class="k">assert</span> <span class="n">source</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_sources</span><span class="p">,</span> <span class="n">source</span>
                    <span class="n">seen_sources</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
                    <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">aot_autograd_arg_pos_to_source</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">aot_autograd_arg_pos_to_source</span><span class="p">)</span>

    <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">full_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
            <span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">break</span>

    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">inference_compiler</span><span class="o">=</span><span class="n">inference_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">params_len</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
        <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="n">keep_inference_input_mutations</span><span class="p">,</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">,</span>
        <span class="n">aot_autograd_arg_pos_to_source</span><span class="o">=</span><span class="n">aot_autograd_arg_pos_to_source</span>
    <span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span>
        <span class="n">full_args</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># TODO: There is something deeply wrong here; compiled_fn running with</span>
    <span class="c1"># the boxed calling convention, but aot_module_simplified somehow</span>
    <span class="c1"># historically returned a function that was not the boxed calling</span>
    <span class="c1"># convention.  This should get fixed...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">runtime_args</span><span class="p">):</span>
        <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">runtime_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span>

    <span class="c1"># Just for convenience</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">zero_grad</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">zero_grad</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_buffers</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span>

    <span class="k">return</span> <span class="n">forward</span>


<span class="n">compiled_function</span> <span class="o">=</span> <span class="n">aot_function</span>
<span class="n">compiled_module</span> <span class="o">=</span> <span class="n">aot_module</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>